
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Sparse Autoencoder for Mechanistic Interpretability">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.14">
    
    
      
        <title>Sparse Autoencoder Library - Sparse Autoencoder</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.fad675c6.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../css/material_extra.css">
    
      <link rel="stylesheet" href="../css/custom_formatting.css">
    
      <link rel="stylesheet" href="../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#sparse-autoencoder-library" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Sparse Autoencoder" class="md-header__button md-logo" aria-label="Sparse Autoencoder" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sparse Autoencoder
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sparse Autoencoder Library
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="amber"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ai-safety-foundation/sparse_autoencoder" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ai-safety-foundation/sparse_autoencoder
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Sparse Autoencoder" class="md-nav__button md-logo" aria-label="Sparse Autoencoder" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sparse Autoencoder
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ai-safety-foundation/sparse_autoencoder" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ai-safety-foundation/sparse_autoencoder
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../demo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Demo
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="activation_resampler/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    activation_resampler
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            activation_resampler
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_resampler/abstract_activation_resampler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_activation_resampler
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_resampler/activation_resampler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    activation_resampler
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="activation_store/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    activation_store
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            activation_store
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_store/base_store/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    base_store
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_store/disk_store/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    disk_store
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_store/list_store/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    list_store
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_store/tensor_store/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tensor_store
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="activation_store/utils/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_2_5" id="__nav_3_2_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_5">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="activation_store/utils/extend_resize/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    extend_resize
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="autoencoder/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    autoencoder
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            autoencoder
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/abstract_autoencoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_autoencoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="autoencoder/components/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    components
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_3_2" id="__nav_3_3_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_2">
            <span class="md-nav__icon md-icon"></span>
            components
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/abstract_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/abstract_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/abstract_outer_bias/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_outer_bias
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/linear_encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linear_encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/tied_bias/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tied_bias
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/components/unit_norm_decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    unit_norm_decoder
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="autoencoder/model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    model
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="loss/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    loss
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            loss
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/abstract_loss/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_loss
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/decoded_activations_l2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decoded_activations_l2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/learned_activations_l1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    learned_activations_l1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="loss/reducer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reducer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    metrics
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            metrics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/generate/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    generate
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_5_1" id="__nav_3_5_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_1">
            <span class="md-nav__icon md-icon"></span>
            generate
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/generate/abstract_generate_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_generate_metric
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/metrics_container/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    metrics_container
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/resample/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    resample
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_5_3" id="__nav_3_5_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_3">
            <span class="md-nav__icon md-icon"></span>
            resample
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/resample/abstract_resample_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_resample_metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/resample/neuron_activity_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    neuron_activity_metric
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/train/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    train
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_5_4" id="__nav_3_5_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_4">
            <span class="md-nav__icon md-icon"></span>
            train
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/abstract_train_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_train_metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/capacity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    capacity
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/feature_density/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    feature_density
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/train/l0_norm_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    l0_norm_metric
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="metrics/validate/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    validate
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_5_5" id="__nav_3_5_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5_5">
            <span class="md-nav__icon md-icon"></span>
            validate
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/validate/abstract_validate_metric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_validate_metric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="metrics/validate/model_reconstruction_score/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    model_reconstruction_score
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="optimizer/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    optimizer
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            optimizer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizer/abstract_optimizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_optimizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="optimizer/adam_with_reset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    adam_with_reset
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="source_data/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    source_data
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            source_data
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/abstract_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abstract_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/mock_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mock_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/pretokenized_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pretokenized_dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_data/text_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_dataset
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="source_model/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    source_model
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            source_model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_model/replace_activations_hook/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    replace_activations_hook
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_model/store_activations_hook/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    store_activations_hook
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="source_model/zero_ablate_hook/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    zero_ablate_hook
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="tensor_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tensor_types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="train/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    train
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_10" id="__nav_3_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_10">
            <span class="md-nav__icon md-icon"></span>
            train
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pipeline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/sweep_config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sweep_config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_10_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="train/utils/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_10_3" id="__nav_3_10_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_10_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_10_3">
            <span class="md-nav__icon md-icon"></span>
            utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/utils/get_model_device/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    get_model_device
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="train/utils/wandb_sweep_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wandb_sweep_types
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../citation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Citation
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      sparse_autoencoder
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossLogType" class="md-nav__link">
    <span class="md-ellipsis">
      LossLogType
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler" class="md-nav__link">
    <span class="md-ellipsis">
      ActivationResampler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ActivationResampler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.assign_sampling_probabilities" class="md-nav__link">
    <span class="md-ellipsis">
      assign_sampling_probabilities()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations" class="md-nav__link">
    <span class="md-ellipsis">
      compute_loss_and_get_activations()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.get_dead_neuron_indices" class="md-nav__link">
    <span class="md-ellipsis">
      get_dead_neuron_indices()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.renormalize_and_scale" class="md-nav__link">
    <span class="md-ellipsis">
      renormalize_and_scale()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.resample_dead_neurons" class="md-nav__link">
    <span class="md-ellipsis">
      resample_dead_neurons()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ActivationResampler.sample_input" class="md-nav__link">
    <span class="md-ellipsis">
      sample_input()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset" class="md-nav__link">
    <span class="md-ellipsis">
      AdamWithReset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdamWithReset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.parameter_names" class="md-nav__link">
    <span class="md-ellipsis">
      parameter_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state" class="md-nav__link">
    <span class="md-ellipsis">
      reset_neurons_state()
    </span>
  </a>
  
    <nav class="md-nav" aria-label="reset_neurons_state()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this" class="md-nav__link">
    <span class="md-ellipsis">
      ... train the model and then resample some dead neurons, then do this ...
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated" class="md-nav__link">
    <span class="md-ellipsis">
      Reset the optimizer state for parameters that have been updated
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.AdamWithReset.reset_state_all_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_state_all_parameters()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      CapacityMetric
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CapacityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric.calculate" class="md-nav__link">
    <span class="md-ellipsis">
      calculate()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric.capacities" class="md-nav__link">
    <span class="md-ellipsis">
      capacities()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.CapacityMetric.wandb_capacities_histogram" class="md-nav__link">
    <span class="md-ellipsis">
      wandb_capacities_histogram()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore" class="md-nav__link">
    <span class="md-ellipsis">
      DiskActivationStore
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DiskActivationStore">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.__del__" class="md-nav__link">
    <span class="md-ellipsis">
      __del__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.__getitem__" class="md-nav__link">
    <span class="md-ellipsis">
      __getitem__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.append" class="md-nav__link">
    <span class="md-ellipsis">
      append()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.empty" class="md-nav__link">
    <span class="md-ellipsis">
      empty()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.extend" class="md-nav__link">
    <span class="md-ellipsis">
      extend()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.DiskActivationStore.wait_for_writes_to_complete" class="md-nav__link">
    <span class="md-ellipsis">
      wait_for_writes_to_complete()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss" class="md-nav__link">
    <span class="md-ellipsis">
      L2ReconstructionLoss
    </span>
  </a>
  
    <nav class="md-nav" aria-label="L2ReconstructionLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log" class="md-nav__link">
    <span class="md-ellipsis">
      Outputs both loss and metrics to log
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.L2ReconstructionLoss.log_name" class="md-nav__link">
    <span class="md-ellipsis">
      log_name()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss" class="md-nav__link">
    <span class="md-ellipsis">
      LearnedActivationsL1Loss
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LearnedActivationsL1Loss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log" class="md-nav__link">
    <span class="md-ellipsis">
      Returns loss and metrics to log
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient" class="md-nav__link">
    <span class="md-ellipsis">
      l1_coefficient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.batch_scalar_loss_with_log" class="md-nav__link">
    <span class="md-ellipsis">
      batch_scalar_loss_with_log()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.extra_repr" class="md-nav__link">
    <span class="md-ellipsis">
      extra_repr()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LearnedActivationsL1Loss.log_name" class="md-nav__link">
    <span class="md-ellipsis">
      log_name()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore" class="md-nav__link">
    <span class="md-ellipsis">
      ListActivationStore
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ListActivationStore">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.__del__" class="md-nav__link">
    <span class="md-ellipsis">
      __del__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.__getitem__" class="md-nav__link">
    <span class="md-ellipsis">
      __getitem__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.__sizeof__" class="md-nav__link">
    <span class="md-ellipsis">
      __sizeof__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.append" class="md-nav__link">
    <span class="md-ellipsis">
      append()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.empty" class="md-nav__link">
    <span class="md-ellipsis">
      empty()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.extend" class="md-nav__link">
    <span class="md-ellipsis">
      extend()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.shuffle" class="md-nav__link">
    <span class="md-ellipsis">
      shuffle()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.ListActivationStore.wait_for_writes_to_complete" class="md-nav__link">
    <span class="md-ellipsis">
      wait_for_writes_to_complete()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer" class="md-nav__link">
    <span class="md-ellipsis">
      LossReducer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LossReducer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__dir__" class="md-nav__link">
    <span class="md-ellipsis">
      __dir__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__getitem__" class="md-nav__link">
    <span class="md-ellipsis">
      __getitem__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__iter__" class="md-nav__link">
    <span class="md-ellipsis">
      __iter__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReducer.log_name" class="md-nav__link">
    <span class="md-ellipsis">
      log_name()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReductionType" class="md-nav__link">
    <span class="md-ellipsis">
      LossReductionType
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LossReductionType">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReductionType.MEAN" class="md-nav__link">
    <span class="md-ellipsis">
      MEAN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.LossReductionType.SUM" class="md-nav__link">
    <span class="md-ellipsis">
      SUM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.NeuronActivityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      NeuronActivityMetric
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NeuronActivityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.NeuronActivityMetric.calculate" class="md-nav__link">
    <span class="md-ellipsis">
      calculate()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Pipeline
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.activation_resampler" class="md-nav__link">
    <span class="md-ellipsis">
      activation_resampler
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      autoencoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.cache_name" class="md-nav__link">
    <span class="md-ellipsis">
      cache_name
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.layer" class="md-nav__link">
    <span class="md-ellipsis">
      layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.log_frequency" class="md-nav__link">
    <span class="md-ellipsis">
      log_frequency
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.loss" class="md-nav__link">
    <span class="md-ellipsis">
      loss
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.metrics" class="md-nav__link">
    <span class="md-ellipsis">
      metrics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      optimizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.progress_bar" class="md-nav__link">
    <span class="md-ellipsis">
      progress_bar
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.source_data" class="md-nav__link">
    <span class="md-ellipsis">
      source_data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.source_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      source_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.source_model" class="md-nav__link">
    <span class="md-ellipsis">
      source_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.total_training_steps" class="md-nav__link">
    <span class="md-ellipsis">
      total_training_steps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.generate_activations" class="md-nav__link">
    <span class="md-ellipsis">
      generate_activations()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.resample_neurons" class="md-nav__link">
    <span class="md-ellipsis">
      resample_neurons()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.run_pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      run_pipeline()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.save_checkpoint" class="md-nav__link">
    <span class="md-ellipsis">
      save_checkpoint()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.stateful_dataloader_iterable" class="md-nav__link">
    <span class="md-ellipsis">
      stateful_dataloader_iterable()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.train_autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      train_autoencoder()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.Pipeline.validate_sae" class="md-nav__link">
    <span class="md-ellipsis">
      validate_sae()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.PreTokenizedDataset" class="md-nav__link">
    <span class="md-ellipsis">
      PreTokenizedDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PreTokenizedDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PreTokenizedDataset.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.PreTokenizedDataset.preprocess" class="md-nav__link">
    <span class="md-ellipsis">
      preprocess()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      SparseAutoencoder
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SparseAutoencoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.decoder" class="md-nav__link">
    <span class="md-ellipsis">
      decoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.encoder" class="md-nav__link">
    <span class="md-ellipsis">
      encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset" class="md-nav__link">
    <span class="md-ellipsis">
      geometric_median_dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.n_input_features" class="md-nav__link">
    <span class="md-ellipsis">
      n_input_features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.n_learned_features" class="md-nav__link">
    <span class="md-ellipsis">
      n_learned_features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.post_decoder_bias" class="md-nav__link">
    <span class="md-ellipsis">
      post_decoder_bias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.pre_encoder_bias" class="md-nav__link">
    <span class="md-ellipsis">
      pre_encoder_bias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.tied_bias" class="md-nav__link">
    <span class="md-ellipsis">
      tied_bias
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      initialize_tied_parameters()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.SparseAutoencoder.reset_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_parameters()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore" class="md-nav__link">
    <span class="md-ellipsis">
      TensorActivationStore
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorActivationStore">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.items_stored" class="md-nav__link">
    <span class="md-ellipsis">
      items_stored
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.max_items" class="md-nav__link">
    <span class="md-ellipsis">
      max_items
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__getitem__" class="md-nav__link">
    <span class="md-ellipsis">
      __getitem__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__len__" class="md-nav__link">
    <span class="md-ellipsis">
      __len__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.__sizeof__" class="md-nav__link">
    <span class="md-ellipsis">
      __sizeof__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.append" class="md-nav__link">
    <span class="md-ellipsis">
      append()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.empty" class="md-nav__link">
    <span class="md-ellipsis">
      empty()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.extend" class="md-nav__link">
    <span class="md-ellipsis">
      extend()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TensorActivationStore.shuffle" class="md-nav__link">
    <span class="md-ellipsis">
      shuffle()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.TextDataset" class="md-nav__link">
    <span class="md-ellipsis">
      TextDataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TextDataset.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TextDataset.preprocess" class="md-nav__link">
    <span class="md-ellipsis">
      preprocess()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      TrainBatchFeatureDensityMetric
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TrainBatchFeatureDensityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate" class="md-nav__link">
    <span class="md-ellipsis">
      calculate()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density" class="md-nav__link">
    <span class="md-ellipsis">
      feature_density()
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram" class="md-nav__link">
    <span class="md-ellipsis">
      wandb_feature_density_histogram()
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="sparse-autoencoder-library">Sparse Autoencoder Library<a class="headerlink" href="#sparse-autoencoder-library" title="Permanent link">¤</a></h1>


<div class="doc doc-object doc-module">



<a id="sparse_autoencoder"></a>
  <div class="doc doc-contents first">
  
      <p>Sparse Autoencoder Library.</p>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h2 id="sparse_autoencoder.LossLogType" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">LossLogType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">str</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-module-attribute"><code>module-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.LossLogType" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
  
      <p>Loss log dict.</p>
  </div>

</div>


<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.ActivationResampler" class="doc doc-heading">
          <code>ActivationResampler</code>


<a href="#sparse_autoencoder.ActivationResampler" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler" href="activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler">AbstractActivationResampler</a></code></p>

  
      <p>Activation resampler.</p>
<p>Over the course of training, a subset of autoencoder neurons will have zero activity across
a large number of datapoints. The authors of <em>Towards Monosemanticity: Decomposing Language
Models With Dictionary Learning</em> found that “resampling” these dead neurons during training
improves the number of likely-interpretable features (i.e., those in the high density cluster)
and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and
increase the number of chances the network has to find promising feature directions.</p>
<p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that
if we increase the number of training steps then networks will kill off more of these ultralow
density neurons. This reinforces the use of the high density cluster as a useful metric because
there can exist neurons that are de facto dead but will not appear to be when looking at the
number of dead neurons alone.</p>
<p>This approach is designed to seed new features to fit inputs where the current autoencoder
performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled
neuron will only fire weakly for inputs similar to the one used for its reinitialization. This
was done to minimize interference with the rest of the network.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>The optimizer should be reset after applying this function, as the Adam state will be
incorrect for the modified weights and biases.</p>
<p>Note this approach is also known to create sudden loss spikes, and resampling too frequently
causes training to diverge.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ActivationResampler</span><span class="p">(</span><span class="n">AbstractActivationResampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Activation resampler.</span>

<span class="sd">    Over the course of training, a subset of autoencoder neurons will have zero activity across</span>
<span class="sd">    a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language</span>
<span class="sd">    Models With Dictionary Learning* found that “resampling” these dead neurons during training</span>
<span class="sd">    improves the number of likely-interpretable features (i.e., those in the high density cluster)</span>
<span class="sd">    and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and</span>
<span class="sd">    increase the number of chances the network has to find promising feature directions.</span>

<span class="sd">    An interesting nuance around dead neurons involves the ultralow density cluster. They found that</span>
<span class="sd">    if we increase the number of training steps then networks will kill off more of these ultralow</span>
<span class="sd">    density neurons. This reinforces the use of the high density cluster as a useful metric because</span>
<span class="sd">    there can exist neurons that are de facto dead but will not appear to be when looking at the</span>
<span class="sd">    number of dead neurons alone.</span>

<span class="sd">    This approach is designed to seed new features to fit inputs where the current autoencoder</span>
<span class="sd">    performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled</span>
<span class="sd">    neuron will only fire weakly for inputs similar to the one used for its reinitialization. This</span>
<span class="sd">    was done to minimize interference with the rest of the network.</span>

<span class="sd">    Warning:</span>
<span class="sd">        The optimizer should be reset after applying this function, as the Adam state will be</span>
<span class="sd">        incorrect for the modified weights and biases.</span>

<span class="sd">        Note this approach is also known to create sudden loss spikes, and resampling too frequently</span>
<span class="sd">        causes training to diverge.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_dead_neuron_indices</span><span class="p">(</span>
        <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
        <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntNeuronIndices</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Identify the indices of neurons that have zero activity.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 1])</span>
<span class="sd">            &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(</span>
<span class="sd">            ...     neuron_activity_sample_size=10,</span>
<span class="sd">            ...     neuron_activity=neuron_activity,</span>
<span class="sd">            ...     threshold=0.2</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; dead_neuron_indices.tolist()</span>
<span class="sd">            [0, 1, 4]</span>

<span class="sd">        Args:</span>
<span class="sd">            neuron_activity_sample_size: Sample size for resampling.</span>
<span class="sd">            neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">            threshold: Threshold for determining if a neuron is dead (has fired less than this</span>
<span class="sd">                portion of the sample size).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor containing the indices of neurons that are &#39;dead&#39; (zero activity).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">threshold_activity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">neuron_activity_sample_size</span> <span class="o">*</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">neuron_activity</span> <span class="o">&lt;=</span> <span class="n">threshold_activity</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">compute_loss_and_get_activations</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">,</span> <span class="n">InputOutputActivationBatch</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the loss on a random subset of inputs.</span>

<span class="sd">        Computes the loss and also stores the input activations (for use in resampling neurons).</span>

<span class="sd">        Args:</span>
<span class="sd">            store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing the loss per item, and all input activations.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the number of items in the store is less than the number of inputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">loss_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">input_activations_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationBatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>
            <span class="n">num_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="p">)</span>
            <span class="n">batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="o">//</span> <span class="n">train_batch_size</span>
            <span class="n">model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)):</span>
                <span class="n">input_activations_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">source_activations</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
                <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source_activations</span><span class="p">)</span>
                <span class="n">loss_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">loss_fn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                        <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;=</span> <span class="n">batches</span><span class="p">:</span>
                    <span class="k">break</span>

            <span class="n">loss_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loss_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
            <span class="n">input_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_activations_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>

            <span class="c1"># Check we generated enough data</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_inputs</span><span class="p">:</span>
                <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot get </span><span class="si">{</span><span class="n">num_inputs</span><span class="si">}</span><span class="s2"> items from the store, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;as only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span><span class="si">}</span><span class="s2"> were available.&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">loss_result</span><span class="p">,</span> <span class="n">input_activations</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Assign the sampling probabilities for each input activations vector.</span>

<span class="sd">        Assign each input vector a probability of being picked that is proportional to the square of</span>
<span class="sd">        the autoencoder&#39;s loss on that input.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])</span>
<span class="sd">            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)</span>
<span class="sd">            tensor([0.1000, 0.3000, 0.6000])</span>

<span class="sd">        Args:</span>
<span class="sd">            loss: Loss per item.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensor of probabilities for each item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">square_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">square_loss</span> <span class="o">/</span> <span class="n">square_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sample_input</span><span class="p">(</span>
        <span class="n">probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">,</span>
        <span class="n">input_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample an input vector based on the provided probabilities.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])</span>
<span class="sd">            &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])</span>
<span class="sd">            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(</span>
<span class="sd">            ...     probabilities, input_activations, 2</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; sampled_input.tolist()</span>
<span class="sd">            [[5.0, 6.0], [3.0, 4.0]]</span>

<span class="sd">        Args:</span>
<span class="sd">            probabilities: Probabilities for each input.</span>
<span class="sd">            input_activations: Input activation vectors.</span>
<span class="sd">            num_samples: Number of samples to take (number of dead neurons).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Sampled input activation vector.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the number of samples is greater than the number of input activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_samples</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">):</span>
            <span class="n">exception_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot sample </span><span class="si">{</span><span class="n">num_samples</span><span class="si">}</span><span class="s2"> inputs from &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">)</span><span class="si">}</span><span class="s2"> input activations.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">exception_message</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">sample_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span>
            <span class="n">probabilities</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">input_activations</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">,</span> <span class="p">:]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">renormalize_and_scale</span><span class="p">(</span>
        <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">,</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
        <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeadEncoderNeuronWeightUpdates</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Renormalize and scale the resampled dictionary vectors.</span>

<span class="sd">        Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
<span class="sd">        neurons times 0.2.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])</span>
<span class="sd">            &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])</span>
<span class="sd">            &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))</span>
<span class="sd">            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(</span>
<span class="sd">            ...     sampled_input,</span>
<span class="sd">            ...     neuron_activity,</span>
<span class="sd">            ...     encoder_weight</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; rescaled_input.round(decimals=1)</span>
<span class="sd">            tensor([[0.2000, 0.2000]])</span>

<span class="sd">        Args:</span>
<span class="sd">            sampled_input: Tensor of the sampled input activation.</span>
<span class="sd">            neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">            encoder_weight: Tensor of encoder weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Rescaled sampled input.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If there are no alive neurons.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">alive_neuron_mask</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; learned_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="c1"># Check there is at least one alive neuron</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;No alive neurons found.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Handle all alive neurons</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampled_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>

        <span class="c1"># Calculate the average norm of the encoder weights for alive neurons.</span>
        <span class="n">alive_encoder_weights</span><span class="p">:</span> <span class="n">AliveEncoderWeights</span> <span class="o">=</span> <span class="n">encoder_weight</span><span class="p">[</span><span class="n">alive_neuron_mask</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">average_alive_norm</span><span class="p">:</span> <span class="n">ItemTensor</span> <span class="o">=</span> <span class="n">alive_encoder_weights</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
        <span class="c1"># neurons times 0.2.</span>
        <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
            <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">renormalized_input</span> <span class="o">*</span> <span class="p">(</span><span class="n">average_alive_norm</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParameterUpdateResults</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">        Args:</span>
<span class="sd">            activation_store: Activation store.</span>
<span class="sd">            autoencoder: Sparse autoencoder model.</span>
<span class="sd">            loss_fn: Loss function.</span>
<span class="sd">            neuron_activity_sample_size: Sample size for resampling.</span>
<span class="sd">            neuron_activity: Number of times each neuron fired.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">dead_neuron_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dead_neuron_indices</span><span class="p">(</span>
                <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
                <span class="n">neuron_activity_sample_size</span><span class="o">=</span><span class="n">neuron_activity_sample_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Compute the loss for the current model on a random subset of inputs and get the</span>
            <span class="c1"># activations.</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">input_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_and_get_activations</span><span class="p">(</span>
                <span class="n">store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
                <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Assign each input vector a probability of being picked that is proportional to the</span>
            <span class="c1"># square of the autoencoder&#39;s loss on that input.</span>
            <span class="n">sample_probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Get references to the encoder and decoder parameters</span>
            <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>

            <span class="c1"># For each dead neuron sample an input according to these probabilities.</span>
            <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">(</span>
                <span class="n">sample_probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dead_neuron_indices</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># Renormalize the input vector to have unit L2 norm and set this to be the dictionary</span>
            <span class="c1"># vector for the dead autoencoder neuron.</span>
            <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
                <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>

            <span class="n">dead_decoder_weight_updates</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
                <span class="n">renormalized_input</span><span class="p">,</span> <span class="s2">&quot;dead_neuron input_feature -&gt; input_feature dead_neuron&quot;</span>
            <span class="p">)</span>

            <span class="c1"># For the corresponding encoder vector, renormalize the input vector to equal the</span>
            <span class="c1"># average norm of the encoder weights for alive neurons times 0.2. Set the corresponding</span>
            <span class="c1"># encoder bias element to zero.</span>
            <span class="n">rescaled_sampled_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">renormalize_and_scale</span><span class="p">(</span>
                <span class="n">sampled_input</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">encoder_weight</span>
            <span class="p">)</span>
            <span class="n">dead_encoder_bias_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                <span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">ParameterUpdateResults</span><span class="p">(</span>
                <span class="n">dead_neuron_indices</span><span class="o">=</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">dead_encoder_weight_updates</span><span class="o">=</span><span class="n">rescaled_sampled_input</span><span class="p">,</span>
                <span class="n">dead_encoder_bias_updates</span><span class="o">=</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
                <span class="n">dead_decoder_weight_updates</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationResampler.assign_sampling_probabilities" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.assign_sampling_probabilities" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Assign the sampling probabilities for each input activations vector.</p>
<p>Assign each input vector a probability of being picked that is proportional to the square of
the autoencoder's loss on that input.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>loss = torch.tensor([1.0, 2.0, 3.0])
ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)
tensor([0.1000, 0.3000, 0.6000])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>loss</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss per item.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tensor of probabilities for each item.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Assign the sampling probabilities for each input activations vector.</span>

<span class="sd">    Assign each input vector a probability of being picked that is proportional to the square of</span>
<span class="sd">    the autoencoder&#39;s loss on that input.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])</span>
<span class="sd">        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)</span>
<span class="sd">        tensor([0.1000, 0.3000, 0.6000])</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: Loss per item.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor of probabilities for each item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">square_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">square_loss</span> <span class="o">/</span> <span class="n">square_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">compute_loss_and_get_activations</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Compute the loss on a random subset of inputs.</p>
<p>Computes the loss and also stores the input activations (for use in resampling neurons).</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>autoencoder</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse autoencoder model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>loss_fn</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss function.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size (also used for resampling).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tuple containing the loss per item, and all input activations.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the number of items in the store is less than the number of inputs</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_loss_and_get_activations</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">,</span> <span class="n">InputOutputActivationBatch</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the loss on a random subset of inputs.</span>

<span class="sd">    Computes the loss and also stores the input activations (for use in resampling neurons).</span>

<span class="sd">    Args:</span>
<span class="sd">        store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing the loss per item, and all input activations.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the number of items in the store is less than the number of inputs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">loss_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">input_activations_batches</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationBatch</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">)</span>
        <span class="n">num_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_resample_dataset_size</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="p">)</span>
        <span class="n">batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="o">//</span> <span class="n">train_batch_size</span>
        <span class="n">model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)):</span>
            <span class="n">input_activations_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">source_activations</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
            <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">(</span><span class="n">source_activations</span><span class="p">)</span>
            <span class="n">loss_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">loss_fn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                    <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;=</span> <span class="n">batches</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">loss_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">loss_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>
        <span class="n">input_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_activations_batches</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_device</span><span class="p">)</span>

        <span class="c1"># Check we generated enough data</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">num_inputs</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot get </span><span class="si">{</span><span class="n">num_inputs</span><span class="si">}</span><span class="s2"> items from the store, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;as only </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">loss_result</span><span class="p">)</span><span class="si">}</span><span class="s2"> were available.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss_result</span><span class="p">,</span> <span class="n">input_activations</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationResampler.get_dead_neuron_indices" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_dead_neuron_indices</span><span class="p">(</span><span class="n">neuron_activity_sample_size</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.get_dead_neuron_indices" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Identify the indices of neurons that have zero activity.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>neuron_activity = torch.tensor([0, 0, 3, 10, 1])
dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(
...     neuron_activity_sample_size=10,
...     neuron_activity=neuron_activity,
...     threshold=0.2
... )
dead_neuron_indices.tolist()
[0, 1, 4]</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>neuron_activity_sample_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sample size for resampling.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_activity</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tensor representing the number of times each neuron fired.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>threshold</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Threshold for determining if a neuron is dead (has fired less than this
portion of the sample size).</p>
            </div>
          </td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearntNeuronIndices" href="tensor_types/#sparse_autoencoder.tensor_types.LearntNeuronIndices">LearntNeuronIndices</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A tensor containing the indices of neurons that are 'dead' (zero activity).</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_dead_neuron_indices</span><span class="p">(</span>
    <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntNeuronIndices</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Identify the indices of neurons that have zero activity.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 1])</span>
<span class="sd">        &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(</span>
<span class="sd">        ...     neuron_activity_sample_size=10,</span>
<span class="sd">        ...     neuron_activity=neuron_activity,</span>
<span class="sd">        ...     threshold=0.2</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; dead_neuron_indices.tolist()</span>
<span class="sd">        [0, 1, 4]</span>

<span class="sd">    Args:</span>
<span class="sd">        neuron_activity_sample_size: Sample size for resampling.</span>
<span class="sd">        neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">        threshold: Threshold for determining if a neuron is dead (has fired less than this</span>
<span class="sd">            portion of the sample size).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor containing the indices of neurons that are &#39;dead&#39; (zero activity).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">threshold_activity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">neuron_activity_sample_size</span> <span class="o">*</span> <span class="n">threshold</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">neuron_activity</span> <span class="o">&lt;=</span> <span class="n">threshold_activity</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationResampler.renormalize_and_scale" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">renormalize_and_scale</span><span class="p">(</span><span class="n">sampled_input</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">encoder_weight</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.renormalize_and_scale" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Renormalize and scale the resampled dictionary vectors.</p>
<p>Renormalize the input vector to equal the average norm of the encoder weights for alive
neurons times 0.2.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>_seed = torch.manual_seed(0)  # For reproducibility in example
sampled_input = torch.tensor([[3.0, 4.0]])
neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])
encoder_weight = torch.ones((6, 2))
rescaled_input = ActivationResampler.renormalize_and_scale(
...     sampled_input,
...     neuron_activity,
...     encoder_weight
... )
rescaled_input.round(decimals=1)
tensor([[0.2000, 0.2000]])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sampled_input</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SampledDeadNeuronInputs" href="tensor_types/#sparse_autoencoder.tensor_types.SampledDeadNeuronInputs">SampledDeadNeuronInputs</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tensor of the sampled input activation.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_activity</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tensor representing the number of times each neuron fired.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>encoder_weight</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.EncoderWeights" href="tensor_types/#sparse_autoencoder.tensor_types.EncoderWeights">EncoderWeights</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tensor of encoder weights.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates" href="tensor_types/#sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates">DeadEncoderNeuronWeightUpdates</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Rescaled sampled input.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If there are no alive neurons.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">renormalize_and_scale</span><span class="p">(</span>
    <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">,</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
    <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DeadEncoderNeuronWeightUpdates</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Renormalize and scale the resampled dictionary vectors.</span>

<span class="sd">    Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
<span class="sd">    neurons times 0.2.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])</span>
<span class="sd">        &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])</span>
<span class="sd">        &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))</span>
<span class="sd">        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(</span>
<span class="sd">        ...     sampled_input,</span>
<span class="sd">        ...     neuron_activity,</span>
<span class="sd">        ...     encoder_weight</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rescaled_input.round(decimals=1)</span>
<span class="sd">        tensor([[0.2000, 0.2000]])</span>

<span class="sd">    Args:</span>
<span class="sd">        sampled_input: Tensor of the sampled input activation.</span>
<span class="sd">        neuron_activity: Tensor representing the number of times each neuron fired.</span>
<span class="sd">        encoder_weight: Tensor of encoder weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Rescaled sampled input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If there are no alive neurons.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alive_neuron_mask</span><span class="p">:</span> <span class="n">Bool</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot; learned_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="c1"># Check there is at least one alive neuron</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;No alive neurons found.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Handle all alive neurons</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">alive_neuron_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampled_input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">sampled_input</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

    <span class="c1"># Calculate the average norm of the encoder weights for alive neurons.</span>
    <span class="n">alive_encoder_weights</span><span class="p">:</span> <span class="n">AliveEncoderWeights</span> <span class="o">=</span> <span class="n">encoder_weight</span><span class="p">[</span><span class="n">alive_neuron_mask</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">average_alive_norm</span><span class="p">:</span> <span class="n">ItemTensor</span> <span class="o">=</span> <span class="n">alive_encoder_weights</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Renormalize the input vector to equal the average norm of the encoder weights for alive</span>
    <span class="c1"># neurons times 0.2.</span>
    <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
        <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">renormalized_input</span> <span class="o">*</span> <span class="p">(</span><span class="n">average_alive_norm</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationResampler.resample_dead_neurons" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">resample_dead_neurons</span><span class="p">(</span><span class="n">activation_store</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">neuron_activity_sample_size</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ActivationResampler.resample_dead_neurons" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Resample dead neurons.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation_store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>autoencoder</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse autoencoder model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>loss_fn</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss function.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_activity_sample_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sample size for resampling.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_activity</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each neuron fired.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size (also used for resampling).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults" href="activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults">ParameterUpdateResults</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">resample_dead_neurons</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">activation_store</span><span class="p">:</span> <span class="n">ActivationStore</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParameterUpdateResults</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_store: Activation store.</span>
<span class="sd">        autoencoder: Sparse autoencoder model.</span>
<span class="sd">        loss_fn: Loss function.</span>
<span class="sd">        neuron_activity_sample_size: Sample size for resampling.</span>
<span class="sd">        neuron_activity: Number of times each neuron fired.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">dead_neuron_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dead_neuron_indices</span><span class="p">(</span>
            <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
            <span class="n">neuron_activity_sample_size</span><span class="o">=</span><span class="n">neuron_activity_sample_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Compute the loss for the current model on a random subset of inputs and get the</span>
        <span class="c1"># activations.</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">input_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_and_get_activations</span><span class="p">(</span>
            <span class="n">store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
            <span class="n">autoencoder</span><span class="o">=</span><span class="n">autoencoder</span><span class="p">,</span>
            <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
            <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Assign each input vector a probability of being picked that is proportional to the</span>
        <span class="c1"># square of the autoencoder&#39;s loss on that input.</span>
        <span class="n">sample_probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign_sampling_probabilities</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="c1"># Get references to the encoder and decoder parameters</span>
        <span class="n">encoder_weight</span><span class="p">:</span> <span class="n">EncoderWeights</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>

        <span class="c1"># For each dead neuron sample an input according to these probabilities.</span>
        <span class="n">sampled_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">(</span>
            <span class="n">sample_probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dead_neuron_indices</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Renormalize the input vector to have unit L2 norm and set this to be the dictionary</span>
        <span class="c1"># vector for the dead autoencoder neuron.</span>
        <span class="n">renormalized_input</span><span class="p">:</span> <span class="n">SampledDeadNeuronInputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
            <span class="n">sampled_input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="n">dead_decoder_weight_updates</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
            <span class="n">renormalized_input</span><span class="p">,</span> <span class="s2">&quot;dead_neuron input_feature -&gt; input_feature dead_neuron&quot;</span>
        <span class="p">)</span>

        <span class="c1"># For the corresponding encoder vector, renormalize the input vector to equal the</span>
        <span class="c1"># average norm of the encoder weights for alive neurons times 0.2. Set the corresponding</span>
        <span class="c1"># encoder bias element to zero.</span>
        <span class="n">rescaled_sampled_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">renormalize_and_scale</span><span class="p">(</span>
            <span class="n">sampled_input</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">encoder_weight</span>
        <span class="p">)</span>
        <span class="n">dead_encoder_bias_updates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
            <span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">ParameterUpdateResults</span><span class="p">(</span>
            <span class="n">dead_neuron_indices</span><span class="o">=</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">dead_encoder_weight_updates</span><span class="o">=</span><span class="n">rescaled_sampled_input</span><span class="p">,</span>
            <span class="n">dead_encoder_bias_updates</span><span class="o">=</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
            <span class="n">dead_decoder_weight_updates</span><span class="o">=</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ActivationResampler.sample_input" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">sample_input</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">input_activations</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.ActivationResampler.sample_input" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Sample an input vector based on the provided probabilities.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>probabilities = torch.tensor([0.1, 0.2, 0.7])
input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
_seed = torch.manual_seed(0)  # For reproducibility in example
sampled_input = ActivationResampler.sample_input(
...     probabilities, input_activations, 2
... )
sampled_input.tolist()
[[5.0, 6.0], [3.0, 4.0]]</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>probabilities</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Probabilities for each input.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>input_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Input activation vectors.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_samples</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of samples to take (number of dead neurons).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SampledDeadNeuronInputs" href="tensor_types/#sparse_autoencoder.tensor_types.SampledDeadNeuronInputs">SampledDeadNeuronInputs</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sampled input activation vector.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the number of samples is greater than the number of input activations.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">sample_input</span><span class="p">(</span>
    <span class="n">probabilities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">,</span>
    <span class="n">input_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SampledDeadNeuronInputs</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample an input vector based on the provided probabilities.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])</span>
<span class="sd">        &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example</span>
<span class="sd">        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(</span>
<span class="sd">        ...     probabilities, input_activations, 2</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; sampled_input.tolist()</span>
<span class="sd">        [[5.0, 6.0], [3.0, 4.0]]</span>

<span class="sd">    Args:</span>
<span class="sd">        probabilities: Probabilities for each input.</span>
<span class="sd">        input_activations: Input activation vectors.</span>
<span class="sd">        num_samples: Number of samples to take (number of dead neurons).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sampled input activation vector.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the number of samples is greater than the number of input activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">num_samples</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">):</span>
        <span class="n">exception_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Cannot sample </span><span class="si">{</span><span class="n">num_samples</span><span class="si">}</span><span class="s2"> inputs from &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_activations</span><span class="p">)</span><span class="si">}</span><span class="s2"> input activations.&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">exception_message</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_activations</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">sample_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span>
        <span class="n">probabilities</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">input_activations</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.AdamWithReset" class="doc doc-heading">
          <code>AdamWithReset</code>


<a href="#sparse_autoencoder.AdamWithReset" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.optim.Adam">Adam</span></code>, <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset" href="optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset">AbstractOptimizerWithReset</a></code></p>

  
      <p>Adam Optimizer with a reset method.</p>
<p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when
manually editing the model parameters during training (e.g. when resampling dead neurons). This
is because Adam maintains running averages of the gradients and the squares of gradients, which
will be incorrect if the parameters are changed.</p>
<p>Otherwise this is the same as the standard Adam optimizer.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">AdamWithReset</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="n">AbstractOptimizerWithReset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adam Optimizer with a reset method.</span>

<span class="sd">    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when</span>
<span class="sd">    manually editing the model parameters during training (e.g. when resampling dead neurons). This</span>
<span class="sd">    is because Adam maintains running averages of the gradients and the squares of gradients, which</span>
<span class="sd">    will be incorrect if the parameters are changed.</span>

<span class="sd">    Otherwise this is the same as the standard Adam optimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">parameter_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parameter Names.</span>

<span class="sd">    The names of the parameters, so that we can find them later when resetting the state.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913 (extending existing implementation)</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">params_t</span><span class="p">,</span>
        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">betas</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">amsgrad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">foreach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">named_parameters</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the optimizer.</span>

<span class="sd">        Warning:</span>
<span class="sd">            Named parameters must be with default settings (remove duplicates and not recursive).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">            &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">            ...     model.parameters(),</span>
<span class="sd">            ...     named_parameters=model.named_parameters(),</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_state_all_parameters()</span>

<span class="sd">        Args:</span>
<span class="sd">            params: Iterable of parameters to optimize or dicts defining parameter groups.</span>
<span class="sd">            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a</span>
<span class="sd">                float LR unless specifying fused=True or capturable=True.</span>
<span class="sd">            betas: Coefficients used for computing running averages of gradient and its square.</span>
<span class="sd">            eps: Term added to the denominator to improve numerical stability.</span>
<span class="sd">            weight_decay: Weight decay (L2 penalty).</span>
<span class="sd">            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper &quot;On the</span>
<span class="sd">                Convergence of Adam and Beyond&quot;.</span>
<span class="sd">            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used</span>
<span class="sd">                over the for-loop implementation on CUDA if more performant. Note that foreach uses</span>
<span class="sd">                more peak memory.</span>
<span class="sd">            maximize: If True, maximizes the parameters based on the objective, instead of</span>
<span class="sd">                minimizing.</span>
<span class="sd">            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair</span>
<span class="sd">                ungraphed performance.</span>
<span class="sd">            differentiable: Whether autograd should occur through the optimizer step in training.</span>
<span class="sd">                Setting to True can impair performance.</span>
<span class="sd">            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,</span>
<span class="sd">                torch.float32, torch.float16, and torch.bfloat16.</span>
<span class="sd">            named_parameters: An iterator over the named parameters of the model. This is used to</span>
<span class="sd">                find the parameters when resetting their state. You should set this as</span>
<span class="sd">                `model.named_parameters()`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the number of parameter names does not match the number of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialise the parent class (note we repeat the parameter names so that type hints work).</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">,</span>
            <span class="n">foreach</span><span class="o">=</span><span class="n">foreach</span><span class="p">,</span>
            <span class="n">maximize</span><span class="o">=</span><span class="n">maximize</span><span class="p">,</span>
            <span class="n">capturable</span><span class="o">=</span><span class="n">capturable</span><span class="p">,</span>
            <span class="n">differentiable</span><span class="o">=</span><span class="n">differentiable</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="n">fused</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Store the names of the parameters, so that we can find them later when resetting the</span>
        <span class="c1"># state.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;The number of parameter names does not match the number of parameters. &quot;</span>
                <span class="s2">&quot;If using model.named_parameters() make sure remove_duplicates is True &quot;</span>
                <span class="s2">&quot;and recursive is False (the default settings).&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_state_all_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the state for all parameters.</span>

<span class="sd">        Iterates over all parameters and resets both the running averages of the gradients and the</span>
<span class="sd">        squares of gradients.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Iterate over every parameter</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="c1"># Get the state</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

                <span class="c1"># Check if state is initialized</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># Reset running averages</span>
                <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

                <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
                <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
                    <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_parameter_name_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the index of a parameter name.</span>

<span class="sd">        Args:</span>
<span class="sd">            parameter_name: The name of the parameter.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The index of the parameter name.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the parameter name is not found.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">parameter_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Parameter name </span><span class="si">{</span><span class="n">parameter_name</span><span class="si">}</span><span class="s2"> not found.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_neurons_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">neuron_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">parameter_group</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the state for specific neurons, on a specific parameter.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">            &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">            ...     model.parameters(),</span>
<span class="sd">            ...     named_parameters=model.named_parameters(),</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...</span>
<span class="sd">            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</span>
<span class="sd">            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;_encoder._weight&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;_encoder._bias&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">            &gt;&gt;&gt; optimizer.reset_neurons_state(</span>
<span class="sd">            ...     &quot;_decoder._weight&quot;,</span>
<span class="sd">            ...     dead_neurons_indices,</span>
<span class="sd">            ...     axis=1</span>
<span class="sd">            ... )</span>

<span class="sd">        Args:</span>
<span class="sd">            parameter_name: The name of the parameter. Examples from the standard sparse autoencoder</span>
<span class="sd">                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,</span>
<span class="sd">                `_decoder._weight`.</span>
<span class="sd">            neuron_indices: The indices of the neurons to reset.</span>
<span class="sd">            axis: The axis of the parameter to reset.</span>
<span class="sd">            parameter_group: The index of the parameter group to reset (typically this is just zero,</span>
<span class="sd">                unless you have setup multiple parameter groups for e.g. different learning rates</span>
<span class="sd">                for different parameters).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the state of the parameter</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="n">parameter_group</span><span class="p">]</span>
        <span class="n">parameter_name_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_parameter_name_idx</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">)</span>
        <span class="n">parameter</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="n">parameter_name_idx</span><span class="p">]</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

        <span class="c1"># Check if state is initialized</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Reset running averages for the specified neurons</span>
        <span class="k">if</span> <span class="s2">&quot;exp_avg&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
            <span class="n">exp_avg</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
            <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
        <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
            <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.AdamWithReset.parameter_names" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">parameter_names</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">_value</span><span class="p">)</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.AdamWithReset.parameter_names" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Parameter Names.</p>
<p>The names of the parameters, so that we can find them later when resetting the state.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.AdamWithReset.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">foreach</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">capturable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">differentiable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fused</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">named_parameters</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.AdamWithReset.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the optimizer.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>Named parameters must be with default settings (remove duplicates and not recursive).</p>
</details>
<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
from sparse_autoencoder.autoencoder.model import SparseAutoencoder
model = SparseAutoencoder(5, 10, torch.zeros(5))
optimizer = AdamWithReset(
...     model.parameters(),
...     named_parameters=model.named_parameters(),
... )
optimizer.reset_state_all_parameters()</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>params</code></td>
          <td>
                <code><span title="torch.optim.optimizer.params_t">params_t</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Iterable of parameters to optimize or dicts defining parameter groups.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>lr</code></td>
          <td>
                <code>float | <span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a
float LR unless specifying fused=True or capturable=True.</p>
            </div>
          </td>
          <td>
                <code>0.001</code>
          </td>
        </tr>
        <tr>
          <td><code>betas</code></td>
          <td>
                <code>tuple[float, float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Coefficients used for computing running averages of gradient and its square.</p>
            </div>
          </td>
          <td>
                <code>(0.9, 0.999)</code>
          </td>
        </tr>
        <tr>
          <td><code>eps</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Term added to the denominator to improve numerical stability.</p>
            </div>
          </td>
          <td>
                <code>1e-08</code>
          </td>
        </tr>
        <tr>
          <td><code>weight_decay</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Weight decay (L2 penalty).</p>
            </div>
          </td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>amsgrad</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to use the AMSGrad variant of this algorithm from the paper "On the
Convergence of Adam and Beyond".</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>foreach</code></td>
          <td>
                <code>bool | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether foreach implementation of optimizer is used. If None, foreach is used
over the for-loop implementation on CUDA if more performant. Note that foreach uses
more peak memory.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>maximize</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If True, maximizes the parameters based on the objective, instead of
minimizing.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>capturable</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether this instance is safe to capture in a CUDA graph. True can impair
ungraphed performance.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>differentiable</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether autograd should occur through the optimizer step in training.
Setting to True can impair performance.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>fused</code></td>
          <td>
                <code>bool | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64,
torch.float32, torch.float16, and torch.bfloat16.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>named_parameters</code></td>
          <td>
                <code><span title="collections.abc.Iterator">Iterator</span>[tuple[str, <span title="torch.nn.parameter.Parameter">Parameter</span>]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>An iterator over the named parameters of the model. This is used to
find the parameters when resetting their state. You should set this as
<code>model.named_parameters()</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the number of parameter names does not match the number of parameters.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913 (extending existing implementation)</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">params_t</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
    <span class="n">betas</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">amsgrad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">foreach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">maximize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">capturable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">differentiable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">named_parameters</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the optimizer.</span>

<span class="sd">    Warning:</span>
<span class="sd">        Named parameters must be with default settings (remove duplicates and not recursive).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">        ...     model.parameters(),</span>
<span class="sd">        ...     named_parameters=model.named_parameters(),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_state_all_parameters()</span>

<span class="sd">    Args:</span>
<span class="sd">        params: Iterable of parameters to optimize or dicts defining parameter groups.</span>
<span class="sd">        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a</span>
<span class="sd">            float LR unless specifying fused=True or capturable=True.</span>
<span class="sd">        betas: Coefficients used for computing running averages of gradient and its square.</span>
<span class="sd">        eps: Term added to the denominator to improve numerical stability.</span>
<span class="sd">        weight_decay: Weight decay (L2 penalty).</span>
<span class="sd">        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper &quot;On the</span>
<span class="sd">            Convergence of Adam and Beyond&quot;.</span>
<span class="sd">        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used</span>
<span class="sd">            over the for-loop implementation on CUDA if more performant. Note that foreach uses</span>
<span class="sd">            more peak memory.</span>
<span class="sd">        maximize: If True, maximizes the parameters based on the objective, instead of</span>
<span class="sd">            minimizing.</span>
<span class="sd">        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair</span>
<span class="sd">            ungraphed performance.</span>
<span class="sd">        differentiable: Whether autograd should occur through the optimizer step in training.</span>
<span class="sd">            Setting to True can impair performance.</span>
<span class="sd">        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,</span>
<span class="sd">            torch.float32, torch.float16, and torch.bfloat16.</span>
<span class="sd">        named_parameters: An iterator over the named parameters of the model. This is used to</span>
<span class="sd">            find the parameters when resetting their state. You should set this as</span>
<span class="sd">            `model.named_parameters()`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the number of parameter names does not match the number of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialise the parent class (note we repeat the parameter names so that type hints work).</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">,</span>
        <span class="n">foreach</span><span class="o">=</span><span class="n">foreach</span><span class="p">,</span>
        <span class="n">maximize</span><span class="o">=</span><span class="n">maximize</span><span class="p">,</span>
        <span class="n">capturable</span><span class="o">=</span><span class="n">capturable</span><span class="p">,</span>
        <span class="n">differentiable</span><span class="o">=</span><span class="n">differentiable</span><span class="p">,</span>
        <span class="n">fused</span><span class="o">=</span><span class="n">fused</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Store the names of the parameters, so that we can find them later when resetting the</span>
    <span class="c1"># state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_value</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;The number of parameter names does not match the number of parameters. &quot;</span>
            <span class="s2">&quot;If using model.named_parameters() make sure remove_duplicates is True &quot;</span>
            <span class="s2">&quot;and recursive is False (the default settings).&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.AdamWithReset.reset_neurons_state" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_neurons_state</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">parameter_group</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.AdamWithReset.reset_neurons_state" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Reset the state for specific neurons, on a specific parameter.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
from sparse_autoencoder.autoencoder.model import SparseAutoencoder
model = SparseAutoencoder(5, 10, torch.zeros(5))
optimizer = AdamWithReset(
...     model.parameters(),
...     named_parameters=model.named_parameters(),
... )</p>
<h4 id="sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this">... train the model and then resample some dead neurons, then do this ...<a class="headerlink" href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this" title="Permanent link">¤</a></h4>
<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>
<h4 id="sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated">Reset the optimizer state for parameters that have been updated<a class="headerlink" href="#sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated" title="Permanent link">¤</a></h4>
<p>optimizer.reset_neurons_state("_encoder._weight", dead_neurons_indices, axis=0)
optimizer.reset_neurons_state("_encoder._bias", dead_neurons_indices, axis=0)
optimizer.reset_neurons_state(
...     "_decoder._weight",
...     dead_neurons_indices,
...     axis=1
... )</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>parameter_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The name of the parameter. Examples from the standard sparse autoencoder
implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>,
<code>_decoder._weight</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_indices</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearntNeuronIndices" href="tensor_types/#sparse_autoencoder.tensor_types.LearntNeuronIndices">LearntNeuronIndices</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The indices of the neurons to reset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>axis</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The axis of the parameter to reset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>parameter_group</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The index of the parameter group to reset (typically this is just zero,
unless you have setup multiple parameter groups for e.g. different learning rates
for different parameters).</p>
            </div>
          </td>
          <td>
                <code>0</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_neurons_state</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">parameter_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">neuron_indices</span><span class="p">:</span> <span class="n">LearntNeuronIndices</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">parameter_group</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the state for specific neurons, on a specific parameter.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder</span>
<span class="sd">        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; optimizer = AdamWithReset(</span>
<span class="sd">        ...     model.parameters(),</span>
<span class="sd">        ...     named_parameters=model.named_parameters(),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...</span>
<span class="sd">        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</span>
<span class="sd">        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;_encoder._weight&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(&quot;_encoder._bias&quot;, dead_neurons_indices, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; optimizer.reset_neurons_state(</span>
<span class="sd">        ...     &quot;_decoder._weight&quot;,</span>
<span class="sd">        ...     dead_neurons_indices,</span>
<span class="sd">        ...     axis=1</span>
<span class="sd">        ... )</span>

<span class="sd">    Args:</span>
<span class="sd">        parameter_name: The name of the parameter. Examples from the standard sparse autoencoder</span>
<span class="sd">            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,</span>
<span class="sd">            `_decoder._weight`.</span>
<span class="sd">        neuron_indices: The indices of the neurons to reset.</span>
<span class="sd">        axis: The axis of the parameter to reset.</span>
<span class="sd">        parameter_group: The index of the parameter group to reset (typically this is just zero,</span>
<span class="sd">            unless you have setup multiple parameter groups for e.g. different learning rates</span>
<span class="sd">            for different parameters).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the state of the parameter</span>
    <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="n">parameter_group</span><span class="p">]</span>
    <span class="n">parameter_name_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_parameter_name_idx</span><span class="p">(</span><span class="n">parameter_name</span><span class="p">)</span>
    <span class="n">parameter</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="n">parameter_name_idx</span><span class="p">]</span>
    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

    <span class="c1"># Check if state is initialized</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Reset running averages for the specified neurons</span>
    <span class="k">if</span> <span class="s2">&quot;exp_avg&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
        <span class="n">exp_avg</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
        <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
    <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
        <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
        <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">neuron_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.AdamWithReset.reset_state_all_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_state_all_parameters</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.AdamWithReset.reset_state_all_parameters" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Reset the state for all parameters.</p>
<p>Iterates over all parameters and resets both the running averages of the gradients and the
squares of gradients.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_state_all_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the state for all parameters.</span>

<span class="sd">    Iterates over all parameters and resets both the running averages of the gradients and the</span>
<span class="sd">    squares of gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Iterate over every parameter</span>
    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="c1"># Get the state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">parameter</span><span class="p">]</span>

            <span class="c1"># Check if state is initialized</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="c1"># Reset running averages</span>
            <span class="n">exp_avg</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
            <span class="n">exp_avg</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
            <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="c1"># If AdamW is used (weight decay fix), also reset the max exp_avg_sq</span>
            <span class="k">if</span> <span class="s2">&quot;max_exp_avg_sq&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                <span class="n">max_exp_avg_sq</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.CapacityMetric" class="doc doc-heading">
          <code>CapacityMetric</code>


<a href="#sparse_autoencoder.CapacityMetric" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric" href="metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric">AbstractTrainMetric</a></code></p>

  
      <p>Capacities Metrics for Learned Features.</p>
<p>Measure the capacity of a set of features as defined in <a href="https://arxiv.org/pdf/2210.01892.pdf">Polysemanticity and Capacity in Neural Networks</a>.</p>
<p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.
Formally it's the ratio of the squared dot product of a feature with itself to the sum of its
squared dot products of all features.</p>
<p>If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is
1/n.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CapacityMetric</span><span class="p">(</span><span class="n">AbstractTrainMetric</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Capacities Metrics for Learned Features.</span>

<span class="sd">    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf).</span>

<span class="sd">    Capacity is intuitively measuring the &#39;proportion of a dimension&#39; assigned to a feature.</span>
<span class="sd">    Formally it&#39;s the ratio of the squared dot product of a feature with itself to the sum of its</span>
<span class="sd">    squared dot products of all features.</span>

<span class="sd">    If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is</span>
<span class="sd">    1/n.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">capacities</span><span class="p">(</span><span class="n">features</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate capacities.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])</span>
<span class="sd">            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)</span>
<span class="sd">            &gt;&gt;&gt; orthogonal_caps</span>
<span class="sd">            tensor([1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            features: A collection of features.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A 1D tensor of capacities, where each element is the capacity of the corresponding</span>
<span class="sd">            feature.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">squared_dot_products</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                <span class="n">features</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="s2">&quot;n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2&quot;</span>
            <span class="p">)</span>
            <span class="o">**</span> <span class="mi">2</span>
        <span class="p">)</span>
        <span class="n">sum_of_sq_dot</span> <span class="o">=</span> <span class="n">squared_dot_products</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">squared_dot_products</span><span class="p">)</span> <span class="o">/</span> <span class="n">sum_of_sq_dot</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">wandb_capacities_histogram</span><span class="p">(</span>
        <span class="n">capacities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the capacities.</span>

<span class="sd">        This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;capacities_histogram&quot;:</span>
<span class="sd">        wandb_capacities_histogram(capacities)})`.</span>

<span class="sd">        Args:</span>
<span class="sd">            capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">numpy_capacities</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float_</span><span class="p">]</span> <span class="o">=</span> <span class="n">capacities</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">bins</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">numpy_capacities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the capacities for a training batch.&quot;&quot;&quot;</span>
        <span class="n">train_batch_capacities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacities</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span><span class="p">)</span>
        <span class="n">train_batch_capacities_histogram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wandb_capacities_histogram</span><span class="p">(</span><span class="n">train_batch_capacities</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;train_batch_capacities_histogram&quot;</span><span class="p">:</span> <span class="n">train_batch_capacities_histogram</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.CapacityMetric.calculate" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">calculate</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.CapacityMetric.calculate" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate the capacities for a training batch.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the capacities for a training batch.&quot;&quot;&quot;</span>
    <span class="n">train_batch_capacities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacities</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span><span class="p">)</span>
    <span class="n">train_batch_capacities_histogram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wandb_capacities_histogram</span><span class="p">(</span><span class="n">train_batch_capacities</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;train_batch_capacities_histogram&quot;</span><span class="p">:</span> <span class="n">train_batch_capacities_histogram</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.CapacityMetric.capacities" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">capacities</span><span class="p">(</span><span class="n">features</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.CapacityMetric.capacities" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate capacities.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])
orthogonal_caps = CapacityMetric.capacities(orthogonal_features)
orthogonal_caps
tensor([1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>features</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A collection of features.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>feature.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">capacities</span><span class="p">(</span><span class="n">features</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate capacities.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])</span>
<span class="sd">        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)</span>
<span class="sd">        &gt;&gt;&gt; orthogonal_caps</span>
<span class="sd">        tensor([1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        features: A collection of features.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1D tensor of capacities, where each element is the capacity of the corresponding</span>
<span class="sd">        feature.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">squared_dot_products</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">einops</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="n">features</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="s2">&quot;n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2&quot;</span>
        <span class="p">)</span>
        <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="n">sum_of_sq_dot</span> <span class="o">=</span> <span class="n">squared_dot_products</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">squared_dot_products</span><span class="p">)</span> <span class="o">/</span> <span class="n">sum_of_sq_dot</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.CapacityMetric.wandb_capacities_histogram" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">wandb_capacities_histogram</span><span class="p">(</span><span class="n">capacities</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.CapacityMetric.wandb_capacities_histogram" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Create a W&amp;B histogram of the capacities.</p>
<p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({"capacities_histogram":
wandb_capacities_histogram(capacities)})</code>.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>capacities</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Capacity of each feature. Can be calculated using :func:<code>calc_capacities</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="wandb.Histogram">Histogram</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">wandb_capacities_histogram</span><span class="p">(</span>
    <span class="n">capacities</span><span class="p">:</span> <span class="n">TrainBatchStatistic</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the capacities.</span>

<span class="sd">    This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;capacities_histogram&quot;:</span>
<span class="sd">    wandb_capacities_histogram(capacities)})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">numpy_capacities</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float_</span><span class="p">]</span> <span class="o">=</span> <span class="n">capacities</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">bins</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">numpy_capacities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.DiskActivationStore" class="doc doc-heading">
          <code>DiskActivationStore</code>


<a href="#sparse_autoencoder.DiskActivationStore" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code></p>

  
      <p>Disk Activation Store.</p>
<p>Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up
activation vectors and then write them to the disk in batches.</p>
<p>Multiprocess safe (supports writing from multiple GPU workers).</p>
<p>Warning:
Unless you want to keep and use existing .pt files in the storage directory when initialized,
set <code>empty_dir</code> to <code>True</code>.</p>
<p>Note also that :meth:<code>close</code> must be called to ensure all activation vectors are written to disk
after the last batch has been added to the store.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">DiskActivationStore</span><span class="p">(</span><span class="n">ActivationStore</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Disk Activation Store.</span>

<span class="sd">    Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up</span>
<span class="sd">    activation vectors and then write them to the disk in batches.</span>

<span class="sd">    Multiprocess safe (supports writing from multiple GPU workers).</span>

<span class="sd">    Warning:</span>
<span class="sd">    Unless you want to keep and use existing .pt files in the storage directory when initialized,</span>
<span class="sd">    set `empty_dir` to `True`.</span>

<span class="sd">    Note also that :meth:`close` must be called to ensure all activation vectors are written to disk</span>
<span class="sd">    after the last batch has been added to the store.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_storage_path</span><span class="p">:</span> <span class="n">Path</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Path to the Directory where the Activation Vectors are Stored.&quot;&quot;&quot;</span>

    <span class="n">_cache</span><span class="p">:</span> <span class="n">ListProxy</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Cache for Activation Vectors.</span>

<span class="sd">    Activation vectors are buffered in memory until the cache is full, at which point they are</span>
<span class="sd">    written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_cache_lock</span><span class="p">:</span> <span class="n">Lock</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lock for the Cache.&quot;&quot;&quot;</span>

    <span class="n">_max_cache_size</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum Number of Activation Vectors to cache in Memory.&quot;&quot;&quot;</span>

    <span class="n">_thread_pool</span><span class="p">:</span> <span class="n">ThreadPoolExecutor</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Threadpool for non-blocking writes to the file system.&quot;&quot;&quot;</span>

    <span class="n">_disk_n_activation_vectors</span><span class="p">:</span> <span class="n">ValueProxy</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length of the Store (on disk).</span>

<span class="sd">    Minus 1 signifies not calculated yet.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">storage_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">DEFAULT_DISK_ACTIVATION_STORE_PATH</span><span class="p">,</span>
        <span class="n">max_cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">empty_dir</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Disk Activation Store.</span>

<span class="sd">        Args:</span>
<span class="sd">            storage_path: Path to the directory where the activation vectors will be stored.</span>
<span class="sd">            max_cache_size: The maximum number of activation vectors to cache in memory before</span>
<span class="sd">                writing to disk. Note this is only followed approximately.</span>
<span class="sd">            num_workers: Number of CPU workers to use for non-blocking writes to the file system (so</span>
<span class="sd">                that the model can keep running whilst it writes the previous activations to disk).</span>
<span class="sd">                This should be less than the number of CPU cores available. You don&#39;t need multiple</span>
<span class="sd">                GPUs to take advantage of this feature.</span>
<span class="sd">            empty_dir: Whether to empty the directory before writing. Generally you want to set this</span>
<span class="sd">                to `True` as otherwise the directory may contain stale activation vectors from</span>
<span class="sd">                previous runs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Setup the storage directory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">=</span> <span class="n">storage_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Setup the Cache</span>
        <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span> <span class="o">=</span> <span class="n">max_cache_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Value</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Empty the directory if needed</span>
        <span class="k">if</span> <span class="n">empty_dir</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>

        <span class="c1"># Create a threadpool for non-blocking writes to the cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span> <span class="o">=</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_write_to_disk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Write the contents of the queue to disk.</span>

<span class="sd">        Args:</span>
<span class="sd">            wait_for_max: Whether to wait until the cache is full before writing to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
            <span class="c1"># Check we have enough items</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="n">size_to_get</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">wait_for_max</span> <span class="ow">and</span> <span class="n">size_to_get</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="c1"># Get the activations from the cache and delete them</span>
            <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">size_to_get</span><span class="p">]</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">size_to_get</span><span class="p">]</span>

            <span class="c1"># Update the length cache</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>

        <span class="n">stacked_activations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>

        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="fm">__len__</span><span class="si">}</span><span class="s2">.pt&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">stacked_activations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">/</span> <span class="n">filename</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a Single Item to the Store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; future.result()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        1</span>

<span class="sd">        Args:</span>
<span class="sd">            item: Activation vector to add to the store.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Future that completes when the activation vector has queued to be written to disk, and</span>
<span class="sd">            if needed, written to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

            <span class="c1"># Write to disk if needed</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a Batch to the Store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))</span>
<span class="sd">        &gt;&gt;&gt; future.result()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        10</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: Batch of activation vectors to add to the store.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Future that completes when the activation vectors have queued to be written to disk, and</span>
<span class="sd">            if needed, written to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">items</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">=</span> <span class="n">resize_to_list_vectors</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>

            <span class="c1"># Write to disk if needed</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>

    <span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">        This should be called after the last batch has been added to the store. It will wait for</span>
<span class="sd">        all activation vectors to be written to disk.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        1</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_all_filenames</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Path</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a List of All Activation Vector Filenames.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*.pt&quot;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the Store.</span>

<span class="sd">        Warning:</span>
<span class="sd">        This will delete all .pt files in the top level of the storage directory.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; future.result()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        1</span>

<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
            <span class="n">file</span><span class="o">.</span><span class="n">unlink</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">        Args:</span>
<span class="sd">            index: The index of the tensor to fetch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The activation store item at the given index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Find the file containing the activation vector</span>
        <span class="n">file_index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span>
        <span class="n">file</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">file_index</span><span class="si">}</span><span class="s2">.pt&quot;</span>

        <span class="c1"># Load the file and return the activation vector</span>
        <span class="n">activation_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">activation_vectors</span><span class="p">[</span><span class="n">index</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">            &gt;&gt;&gt; print(len(store))</span>
<span class="sd">            0</span>

<span class="sd">        Returns:</span>
<span class="sd">            The number of activation vectors in the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Calculate the length if not cached</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
                <span class="n">cache_size</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">cache_size</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
        <span class="c1"># Shutdown the thread pool after everything is complete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__del__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__del__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.__del__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Delete Dunder Method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
    <span class="c1"># Shutdown the thread pool after everything is complete</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.__getitem__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Get Item Dunder Method.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>index</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The index of the tensor to fetch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The activation store item at the given index.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">    Args:</span>
<span class="sd">        index: The index of the tensor to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The activation store item at the given index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Find the file containing the activation vector</span>
    <span class="n">file_index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span>
    <span class="n">file</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">file_index</span><span class="si">}</span><span class="s2">.pt&quot;</span>

    <span class="c1"># Load the file and return the activation vector</span>
    <span class="n">activation_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">activation_vectors</span><span class="p">[</span><span class="n">index</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">storage_path</span><span class="o">=</span><span class="n">DEFAULT_DISK_ACTIVATION_STORE_PATH</span><span class="p">,</span> <span class="n">max_cache_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">empty_dir</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Disk Activation Store.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>storage_path</code></td>
          <td>
                <code><span title="pathlib.Path">Path</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path to the directory where the activation vectors will be stored.</p>
            </div>
          </td>
          <td>
                <code><span title="sparse_autoencoder.activation_store.disk_store.DEFAULT_DISK_ACTIVATION_STORE_PATH">DEFAULT_DISK_ACTIVATION_STORE_PATH</span></code>
          </td>
        </tr>
        <tr>
          <td><code>max_cache_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The maximum number of activation vectors to cache in memory before
writing to disk. Note this is only followed approximately.</p>
            </div>
          </td>
          <td>
                <code>10000</code>
          </td>
        </tr>
        <tr>
          <td><code>num_workers</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of CPU workers to use for non-blocking writes to the file system (so
that the model can keep running whilst it writes the previous activations to disk).
This should be less than the number of CPU cores available. You don't need multiple
GPUs to take advantage of this feature.</p>
            </div>
          </td>
          <td>
                <code>6</code>
          </td>
        </tr>
        <tr>
          <td><code>empty_dir</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to empty the directory before writing. Generally you want to set this
to <code>True</code> as otherwise the directory may contain stale activation vectors from
previous runs.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">storage_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">DEFAULT_DISK_ACTIVATION_STORE_PATH</span><span class="p">,</span>
    <span class="n">max_cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">empty_dir</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Disk Activation Store.</span>

<span class="sd">    Args:</span>
<span class="sd">        storage_path: Path to the directory where the activation vectors will be stored.</span>
<span class="sd">        max_cache_size: The maximum number of activation vectors to cache in memory before</span>
<span class="sd">            writing to disk. Note this is only followed approximately.</span>
<span class="sd">        num_workers: Number of CPU workers to use for non-blocking writes to the file system (so</span>
<span class="sd">            that the model can keep running whilst it writes the previous activations to disk).</span>
<span class="sd">            This should be less than the number of CPU cores available. You don&#39;t need multiple</span>
<span class="sd">            GPUs to take advantage of this feature.</span>
<span class="sd">        empty_dir: Whether to empty the directory before writing. Generally you want to set this</span>
<span class="sd">            to `True` as otherwise the directory may contain stale activation vectors from</span>
<span class="sd">            previous runs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Setup the storage directory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span> <span class="o">=</span> <span class="n">storage_path</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_storage_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Setup the Cache</span>
    <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span> <span class="o">=</span> <span class="n">max_cache_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">Value</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Empty the directory if needed</span>
    <span class="k">if</span> <span class="n">empty_dir</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>

    <span class="c1"># Create a threadpool for non-blocking writes to the cache</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span> <span class="o">=</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">num_workers</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.__len__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Length Dunder Method.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
print(len(store))
0</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of activation vectors in the dataset.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">        &gt;&gt;&gt; print(len(store))</span>
<span class="sd">        0</span>

<span class="sd">    Returns:</span>
<span class="sd">        The number of activation vectors in the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate the length if not cached</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
            <span class="n">cache_size</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">cache_size</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.append" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.append" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Add a Single Item to the Store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
future = store.append(torch.randn(100))
future.result()
print(len(store))
1</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>item</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation vector to add to the store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Future that completes when the activation vector has queued to be written to disk, and</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>if needed, written to disk.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a Single Item to the Store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; future.result()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    1</span>

<span class="sd">    Args:</span>
<span class="sd">        item: Activation vector to add to the store.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future that completes when the activation vector has queued to be written to disk, and</span>
<span class="sd">        if needed, written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

        <span class="c1"># Write to disk if needed</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.empty" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.empty" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Empty the Store.</p>
<p>Warning:
This will delete all .pt files in the top level of the storage directory.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
future = store.append(torch.randn(100))
future.result()
print(len(store))
1</p>
<p>store.empty()
print(len(store))
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the Store.</span>

<span class="sd">    Warning:</span>
<span class="sd">    This will delete all .pt files in the top level of the storage directory.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; future.result()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    1</span>

<span class="sd">    &gt;&gt;&gt; store.empty()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_filenames</span><span class="p">:</span>
        <span class="n">file</span><span class="o">.</span><span class="n">unlink</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_disk_n_activation_vectors</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.extend" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.extend" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Add a Batch to the Store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=10, empty_dir=True)
future = store.extend(torch.randn(10, 100))
future.result()
print(len(store))
10</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SourceModelActivations" href="tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations">SourceModelActivations</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Batch of activation vectors to add to the store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Future that completes when the activation vectors have queued to be written to disk, and</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>if needed, written to disk.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a Batch to the Store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))</span>
<span class="sd">    &gt;&gt;&gt; future.result()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    10</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: Batch of activation vectors to add to the store.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future that completes when the activation vectors have queued to be written to disk, and</span>
<span class="sd">        if needed, written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">items</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">=</span> <span class="n">resize_to_list_vectors</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache_lock</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>

        <span class="c1"># Write to disk if needed</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_cache_size</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">,</span> <span class="n">wait_for_max</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Keep mypy happy</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.DiskActivationStore.wait_for_writes_to_complete" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">wait_for_writes_to_complete</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.DiskActivationStore.wait_for_writes_to_complete" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Wait for Writes to Complete.</p>
<p>This should be called after the last batch has been added to the store. It will wait for
all activation vectors to be written to disk.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>store = DiskActivationStore(max_cache_size=1, empty_dir=True)
future = store.append(torch.randn(100))
store.wait_for_writes_to_complete()
print(len(store))
1</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">    This should be called after the last batch has been added to the store. It will wait for</span>
<span class="sd">    all activation vectors to be written to disk.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)</span>
<span class="sd">    &gt;&gt;&gt; future = store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">    &gt;&gt;&gt; print(len(store))</span>
<span class="sd">    1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_write_to_disk</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.L2ReconstructionLoss" class="doc doc-heading">
          <code>L2ReconstructionLoss</code>


<a href="#sparse_autoencoder.L2ReconstructionLoss" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>L2 Reconstruction loss.</p>
<p>L2 reconstruction loss is calculated as the sum squared error between each each input vector
and it's corresponding decoded vector. The original paper found that models trained with some
loss functions such as cross-entropy loss generally prefer to represent features
polysemantically, whereas models trained with L2 may achieve the same loss for both
polysemantic and monosemantic representations of true features.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
loss = L2ReconstructionLoss()
input_activations = torch.tensor([[5.0, 4], [3.0, 4]])
output_activations = torch.tensor([[1.0, 5], [1.0, 5]])
unused_activations = torch.zeros_like(input_activations)</p>
<h3 id="sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log">Outputs both loss and metrics to log<a class="headerlink" href="#sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log" title="Permanent link">¤</a></h3>
<p>loss(input_activations, unused_activations, output_activations)
(tensor(11.), {'l2_reconstruction_loss': 11.0})</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">L2ReconstructionLoss</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;L2 Reconstruction loss.</span>

<span class="sd">    L2 reconstruction loss is calculated as the sum squared error between each each input vector</span>
<span class="sd">    and it&#39;s corresponding decoded vector. The original paper found that models trained with some</span>
<span class="sd">    loss functions such as cross-entropy loss generally prefer to represent features</span>
<span class="sd">    polysemantically, whereas models trained with L2 may achieve the same loss for both</span>
<span class="sd">    polysemantic and monosemantic representations of true features.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; loss = L2ReconstructionLoss()</span>
<span class="sd">        &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])</span>
<span class="sd">        &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])</span>
<span class="sd">        &gt;&gt;&gt; unused_activations = torch.zeros_like(input_activations)</span>
<span class="sd">        &gt;&gt;&gt; # Outputs both loss and metrics to log</span>
<span class="sd">        &gt;&gt;&gt; loss(input_activations, unused_activations, output_activations)</span>
<span class="sd">        (tensor(11.), {&#39;l2_reconstruction_loss&#39;: 11.0})</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Name of the loss module for logging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;l2_reconstruction_loss&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the L2 reconstruction loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss per batch item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">square_error_loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

        <span class="c1"># Sum over just the features dimension (i.e. batch itemwise loss). Note this is sum rather</span>
        <span class="c1"># than mean to be consistent with L1 loss (and thus make the l1 coefficient stable to number</span>
        <span class="c1"># of features).</span>
        <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.L2ReconstructionLoss.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.L2ReconstructionLoss.forward" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate the L2 reconstruction loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss per batch item.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the L2 reconstruction loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss per batch item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">square_error_loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

    <span class="c1"># Sum over just the features dimension (i.e. batch itemwise loss). Note this is sum rather</span>
    <span class="c1"># than mean to be consistent with L1 loss (and thus make the l1 coefficient stable to number</span>
    <span class="c1"># of features).</span>
    <span class="k">return</span> <span class="n">square_error_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.L2ReconstructionLoss.log_name" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">log_name</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.L2ReconstructionLoss.log_name" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Log name.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the loss module for logging.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Name of the loss module for logging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;l2_reconstruction_loss&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.LearnedActivationsL1Loss" class="doc doc-heading">
          <code>LearnedActivationsL1Loss</code>


<a href="#sparse_autoencoder.LearnedActivationsL1Loss" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>Learned activations L1 (absolute error) loss.</p>
<p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this
multiplied by the l1_coefficient (designed to encourage sparsity).</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>l1_loss = LearnedActivationsL1Loss(0.1)
learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])
unused_activations = torch.zeros_like(learned_activations)</p>
<h3 id="sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log">Returns loss and metrics to log<a class="headerlink" href="#sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log" title="Permanent link">¤</a></h3>
<p>l1_loss(unused_activations, learned_activations, unused_activations)[0]
tensor(0.5000)</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LearnedActivationsL1Loss</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this</span>
<span class="sd">    multiplied by the l1_coefficient (designed to encourage sparsity).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; l1_loss = LearnedActivationsL1Loss(0.1)</span>
<span class="sd">        &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])</span>
<span class="sd">        &gt;&gt;&gt; unused_activations = torch.zeros_like(learned_activations)</span>
<span class="sd">        &gt;&gt;&gt; # Returns loss and metrics to log</span>
<span class="sd">        &gt;&gt;&gt; l1_loss(unused_activations, learned_activations, unused_activations)[0]</span>
<span class="sd">        tensor(0.5000)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;L1 coefficient.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Name of the loss module for logging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;learned_activations_l1_loss_penalty&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the absolute error loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of</span>
<span class="sd">                [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an</span>
<span class="sd">                approximate guide if you use e.g. 2x this number of tokens you might consider using</span>
<span class="sd">                0.5x the l1 coefficient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span> <span class="o">=</span> <span class="n">l1_coefficient</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_l1_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>  <span class="c1"># noqa: ARG002</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">TrainBatchStatistic</span><span class="p">,</span> <span class="n">TrainBatchStatistic</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of itemwise absolute loss, and itemwise absolute loss multiplied by the l1</span>
<span class="sd">            coefficient.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">absolute_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">absolute_loss_penalty</span> <span class="o">=</span> <span class="n">absolute_loss</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span>
        <span class="k">return</span> <span class="n">absolute_loss</span><span class="p">,</span> <span class="n">absolute_loss_penalty</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loss per batch item.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Override to add both the loss and the penalty to the log</span>
    <span class="k">def</span> <span class="nf">batch_scalar_loss_with_log</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">,</span> <span class="n">LossLogType</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss, with log.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>
<span class="sd">            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">                make the loss independent of the batch size.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log</span>
<span class="sd">                (loss before and after the l1 coefficient).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">absolute_loss</span><span class="p">,</span> <span class="n">absolute_loss_penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span>
            <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
        <span class="p">)</span>

        <span class="k">match</span> <span class="n">reduction</span><span class="p">:</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
                <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">absolute_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
                <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">absolute_loss_penalty</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
                <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">absolute_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
                <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">absolute_loss_penalty</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;learned_activations_l1_loss&quot;</span><span class="p">:</span> <span class="n">batch_scalar_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_name</span><span class="p">():</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">batch_scalar_loss_penalty</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extra representation string.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;l1_coefficient=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">l1_coefficient</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>L1 coefficient.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">l1_coefficient</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the absolute error loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>l1_coefficient</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>L1 coefficient. The original paper experimented with L1 coefficients of
[0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an
approximate guide if you use e.g. 2x this number of tokens you might consider using
0.5x the l1 coefficient.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1_coefficient</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the absolute error loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of</span>
<span class="sd">            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an</span>
<span class="sd">            approximate guide if you use e.g. 2x this number of tokens you might consider using</span>
<span class="sd">            0.5x the l1 coefficient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span> <span class="o">=</span> <span class="n">l1_coefficient</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.batch_scalar_loss_with_log" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.batch_scalar_loss_with_log" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Learned activations L1 (absolute error) loss, with log.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>reduction</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType">LossReductionType</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to
make the loss independent of the batch size.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN">MEAN</a></code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.ItemTensor" href="tensor_types/#sparse_autoencoder.tensor_types.ItemTensor">ItemTensor</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.LossLogType" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossLogType">LossLogType</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log
(loss before and after the l1 coefficient).</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">batch_scalar_loss_with_log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="n">LossReductionType</span> <span class="o">=</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">ItemTensor</span><span class="p">,</span> <span class="n">LossLogType</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss, with log.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>
<span class="sd">        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to</span>
<span class="sd">            make the loss independent of the batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log</span>
<span class="sd">            (loss before and after the l1 coefficient).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">absolute_loss</span><span class="p">,</span> <span class="n">absolute_loss_penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span>
        <span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
    <span class="p">)</span>

    <span class="k">match</span> <span class="n">reduction</span><span class="p">:</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">MEAN</span><span class="p">:</span>
            <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">absolute_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">absolute_loss_penalty</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="k">case</span> <span class="n">LossReductionType</span><span class="o">.</span><span class="n">SUM</span><span class="p">:</span>
            <span class="n">batch_scalar_loss</span> <span class="o">=</span> <span class="n">absolute_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">batch_scalar_loss_penalty</span> <span class="o">=</span> <span class="n">absolute_loss_penalty</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;learned_activations_l1_loss&quot;</span><span class="p">:</span> <span class="n">batch_scalar_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_name</span><span class="p">():</span> <span class="n">batch_scalar_loss_penalty</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">batch_scalar_loss_penalty</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.extra_repr" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extra_repr</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.extra_repr" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Extra representation string.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extra representation string.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;l1_coefficient=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_coefficient</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.forward" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Learned activations L1 (absolute error) loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss per batch item.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learned activations L1 (absolute error) loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loss per batch item.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_l1_loss</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LearnedActivationsL1Loss.log_name" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">log_name</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LearnedActivationsL1Loss.log_name" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Log name.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the loss module for logging.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Name of the loss module for logging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;learned_activations_l1_loss_penalty&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.ListActivationStore" class="doc doc-heading">
          <code>ListActivationStore</code>


<a href="#sparse_autoencoder.ListActivationStore" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code></p>

  
      <p>List Activation Store.</p>
<p>Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick
experiments where you don't want to calculate how much memory you need in advance.</p>
<p>Multiprocess safe if the <code>multiprocessing_enabled</code> argument is set to <code>True</code>. This works in two
ways:</p>
<ol>
<li>The list of activation vectors is stored in a multiprocessing manager, which allows multiple
    processes (typically multiple GPUs) to read/write to the list.</li>
<li>The <code>extend</code> method is non-blocking, and uses a threadpool to write to the list in the
    background, which allows the main process to continue working even if there is just one GPU.</li>
</ol>
<p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with
additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p>
<p>Note that the built-in :meth:<code>shuffle</code> method is much faster than using the <code>shuffle</code> argument
on <code>torch.utils.data.DataLoader</code>. You should therefore call this method before passing the
dataset to the loader and then set the DataLoader <code>shuffle</code> argument to <code>False</code>.</p>
<p>Examples:
Create an empty activation dataset:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; store = ListActivationStore()
</code></pre>
<p>Add a single activation vector to the dataset (this is blocking):</p>
<pre><code>&gt;&gt;&gt; store.append(torch.randn(100))
&gt;&gt;&gt; len(store)
1
</code></pre>
<p>Add a batch of activation vectors to the dataset (non-blocking):</p>
<pre><code>&gt;&gt;&gt; batch = torch.randn(10, 100)
&gt;&gt;&gt; store.extend(batch)
&gt;&gt;&gt; len(store)
11
</code></pre>
<p>Shuffle the dataset <strong>before passing it to the DataLoader</strong>:</p>
<pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument
&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)
</code></pre>
<p>Use the dataloader to iterate over the dataset:</p>
<pre><code>&gt;&gt;&gt; next_item = next(iter(loader))
&gt;&gt;&gt; next_item.shape
torch.Size([2, 100])
</code></pre>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ListActivationStore</span><span class="p">(</span><span class="n">ActivationStore</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;List Activation Store.</span>

<span class="sd">    Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick</span>
<span class="sd">    experiments where you don&#39;t want to calculate how much memory you need in advance.</span>

<span class="sd">    Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two</span>
<span class="sd">    ways:</span>

<span class="sd">    1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple</span>
<span class="sd">        processes (typically multiple GPUs) to read/write to the list.</span>
<span class="sd">    2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the</span>
<span class="sd">        background, which allows the main process to continue working even if there is just one GPU.</span>

<span class="sd">    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with</span>
<span class="sd">    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).</span>

<span class="sd">    Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument</span>
<span class="sd">    on `torch.utils.data.DataLoader`. You should therefore call this method before passing the</span>
<span class="sd">    dataset to the loader and then set the DataLoader `shuffle` argument to `False`.</span>

<span class="sd">    Examples:</span>
<span class="sd">    Create an empty activation dataset:</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>

<span class="sd">    Add a single activation vector to the dataset (this is blocking):</span>

<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        1</span>

<span class="sd">    Add a batch of activation vectors to the dataset (non-blocking):</span>

<span class="sd">        &gt;&gt;&gt; batch = torch.randn(10, 100)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(batch)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        11</span>

<span class="sd">    Shuffle the dataset **before passing it to the DataLoader**:</span>

<span class="sd">        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument</span>
<span class="sd">        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)</span>

<span class="sd">    Use the dataloader to iterate over the dataset:</span>

<span class="sd">        &gt;&gt;&gt; next_item = next(iter(loader))</span>
<span class="sd">        &gt;&gt;&gt; next_item.shape</span>
<span class="sd">        torch.Size([2, 100])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">|</span> <span class="n">ListProxy</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Underlying List Data Store.&quot;&quot;&quot;</span>

    <span class="n">_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Device to Store the Activation Vectors On.&quot;&quot;&quot;</span>

    <span class="n">_pool</span><span class="p">:</span> <span class="n">ProcessPoolExecutor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiprocessing Pool.&quot;&quot;&quot;</span>

    <span class="n">_pool_exceptions</span><span class="p">:</span> <span class="n">ListProxy</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="ne">Exception</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pool Exceptions.</span>

<span class="sd">    Used to keep track of exceptions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_pool_futures</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Future</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pool Futures.</span>

<span class="sd">    Used to keep track of processes running in the pool.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">multiprocessing_enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the List Activation Store.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Data to initialize the dataset with.</span>
<span class="sd">            device: Device to store the activation vectors on.</span>
<span class="sd">            max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.</span>
<span class="sd">                Default is the number of cores you have.</span>
<span class="sd">            multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU</span>
<span class="sd">                workers. This creates significant overhead, so you should only enable it if you have</span>
<span class="sd">                multiple GPUs (and experiment with enabling/disabling it).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Default to empty</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># If multiprocessing is enabled, use a multiprocessing manager to create a shared list</span>
        <span class="c1"># between processes. Otherwise, just use a normal list.</span>
        <span class="k">if</span> <span class="n">multiprocessing_enabled</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span> <span class="o">=</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">max_workers</span><span class="p">)</span>
            <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Device for storing the activation vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">        Returns the number of activation vectors in the dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">            &gt;&gt;&gt; len(store)</span>
<span class="sd">            2</span>

<span class="sd">        Returns:</span>
<span class="sd">            The number of activation vectors in the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The size of the dataset in bytes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># The list of tensors is really a list of pointers to tensors, so we need to account for</span>
        <span class="c1"># this as well as the size of the tensors themselves.</span>
        <span class="n">list_of_pointers_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">__sizeof__</span><span class="p">()</span>

        <span class="c1"># Handle 0 items</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">list_of_pointers_size</span>

        <span class="c1"># Otherwise, get the size of the first tensor</span>
        <span class="n">first_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">first_tensor_size</span> <span class="o">=</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
        <span class="n">num_tensors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
        <span class="n">total_tensors_size</span> <span class="o">=</span> <span class="n">first_tensor_size</span> <span class="o">*</span> <span class="n">num_tensors</span>

        <span class="k">return</span> <span class="n">total_tensors_size</span> <span class="o">+</span> <span class="n">list_of_pointers_size</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; store[1]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            index: The index of the tensor to fetch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The activation store item at the given index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([3.]))</span>
<span class="sd">        &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        3</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Append a single item to the dataset.</span>

<span class="sd">        Note **append is blocking**. For better performance use extend instead with batches.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">        Args:</span>
<span class="sd">            item: The item to append to the dataset.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Future that completes when the activation vector has queued to be written to disk, and</span>
<span class="sd">            if needed, written to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extend threadpool method.</span>

<span class="sd">        To be called by :meth:`extend`.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: A batch of items to add to the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Unstack to a list of tensors</span>
            <span class="n">items</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">=</span> <span class="n">resize_to_list_vectors</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># noqa: BLE001</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extend the dataset with multiple items (non-blocking).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">            &gt;&gt;&gt; batch = torch.randn(10, 100)</span>
<span class="sd">            &gt;&gt;&gt; async_result = store.extend(batch)</span>
<span class="sd">            &gt;&gt;&gt; len(store)</span>
<span class="sd">            10</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: A batch of items to add to the dataset.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Future that completes when the activation vectors have queued to be written to disk, and</span>
<span class="sd">            if needed, written to disk.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Schedule _extend to run in a separate process</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
            <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>

        <span class="c1"># Fallback to synchronous execution if not multiprocessing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">        Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)</span>
<span class="sd">            &gt;&gt;&gt; store.extend(torch.randn(3, 100))</span>
<span class="sd">            &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">            &gt;&gt;&gt; len(store)</span>
<span class="sd">            3</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If any exceptions occurred in the background workers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Restart the pool</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">:</span>
            <span class="n">exceptions_report</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">])</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Exceptions occurred in background workers:</span><span class="se">\n</span><span class="si">{</span><span class="n">exceptions_report</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>

        <span class="c1"># Clearing a list like this works for both standard and multiprocessing lists</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__del__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__del__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.ListActivationStore.__del__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Delete Dunder Method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Delete Dunder Method.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cancel_futures</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ListActivationStore.__getitem__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Get Item Dunder Method.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.zeros(5))
store.append(torch.ones(5))
store[1]
tensor([1., 1., 1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>index</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The index of the tensor to fetch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The activation store item at the given index.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">    &gt;&gt;&gt; store[1]</span>
<span class="sd">    tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        index: The index of the tensor to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The activation store item at the given index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_workers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">multiprocessing_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ListActivationStore.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the List Activation Store.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data</code></td>
          <td>
                <code>list[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a>] | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Data to initialize the dataset with.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code><span title="torch.device">device</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Device to store the activation vectors on.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_workers</code></td>
          <td>
                <code>int | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Max CPU workers if multiprocessing is enabled, for writing to the list.
Default is the number of cores you have.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>multiprocessing_enabled</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Support reading/writing to the dataset with multiple GPU
workers. This creates significant overhead, so you should only enable it if you have
multiple GPUs (and experiment with enabling/disabling it).</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">InputOutputActivationVector</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">multiprocessing_enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the List Activation Store.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: Data to initialize the dataset with.</span>
<span class="sd">        device: Device to store the activation vectors on.</span>
<span class="sd">        max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.</span>
<span class="sd">            Default is the number of cores you have.</span>
<span class="sd">        multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU</span>
<span class="sd">            workers. This creates significant overhead, so you should only enable it if you have</span>
<span class="sd">            multiple GPUs (and experiment with enabling/disabling it).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Default to empty</span>
    <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># If multiprocessing is enabled, use a multiprocessing manager to create a shared list</span>
    <span class="c1"># between processes. Otherwise, just use a normal list.</span>
    <span class="k">if</span> <span class="n">multiprocessing_enabled</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span> <span class="o">=</span> <span class="n">ProcessPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">max_workers</span><span class="p">)</span>
        <span class="n">manager</span> <span class="o">=</span> <span class="n">Manager</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Device for storing the activation vectors</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.ListActivationStore.__len__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Length Dunder Method.</p>
<p>Returns the number of activation vectors in the dataset.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of activation vectors in the dataset.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">    Returns the number of activation vectors in the dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">    Returns:</span>
<span class="sd">        The number of activation vectors in the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.__sizeof__" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">__sizeof__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.ListActivationStore.__sizeof__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Sizeof Dunder Method.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The size of the dataset in bytes.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The size of the dataset in bytes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># The list of tensors is really a list of pointers to tensors, so we need to account for</span>
    <span class="c1"># this as well as the size of the tensors themselves.</span>
    <span class="n">list_of_pointers_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">__sizeof__</span><span class="p">()</span>

    <span class="c1"># Handle 0 items</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">list_of_pointers_size</span>

    <span class="c1"># Otherwise, get the size of the first tensor</span>
    <span class="n">first_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">first_tensor_size</span> <span class="o">=</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">first_tensor</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
    <span class="n">num_tensors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
    <span class="n">total_tensors_size</span> <span class="o">=</span> <span class="n">first_tensor_size</span> <span class="o">*</span> <span class="n">num_tensors</span>

    <span class="k">return</span> <span class="n">total_tensors_size</span> <span class="o">+</span> <span class="n">list_of_pointers_size</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.append" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ListActivationStore.append" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Append a single item to the dataset.</p>
<p>Note <strong>append is blocking</strong>. For better performance use extend instead with batches.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>item</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The item to append to the dataset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Future that completes when the activation vector has queued to be written to disk, and</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>if needed, written to disk.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Append a single item to the dataset.</span>

<span class="sd">    Note **append is blocking**. For better performance use extend instead with batches.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>

<span class="sd">    Args:</span>
<span class="sd">        item: The item to append to the dataset.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future that completes when the activation vector has queued to be written to disk, and</span>
<span class="sd">        if needed, written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.empty" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.ListActivationStore.empty" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Empty the dataset.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
<p>store.empty()
len(store)
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    2</span>

<span class="sd">    &gt;&gt;&gt; store.empty()</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>

    <span class="c1"># Clearing a list like this works for both standard and multiprocessing lists</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.extend" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.ListActivationStore.extend" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Extend the dataset with multiple items (non-blocking).</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore()
batch = torch.randn(10, 100)
async_result = store.extend(batch)
len(store)
10</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SourceModelActivations" href="tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations">SourceModelActivations</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A batch of items to add to the dataset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Future that completes when the activation vectors have queued to be written to disk, and</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code><span title="concurrent.futures.Future">Future</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>if needed, written to disk.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Future</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extend the dataset with multiple items (non-blocking).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">        &gt;&gt;&gt; batch = torch.randn(10, 100)</span>
<span class="sd">        &gt;&gt;&gt; async_result = store.extend(batch)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        10</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: A batch of items to add to the dataset.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future that completes when the activation vectors have queued to be written to disk, and</span>
<span class="sd">        if needed, written to disk.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Schedule _extend to run in a separate process</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
        <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>

    <span class="c1"># Fallback to synchronous execution if not multiprocessing</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.shuffle" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">shuffle</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.ListActivationStore.shuffle" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Shuffle the Data In-Place.</p>
<p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
_seed = torch.manual_seed(42)
store = ListActivationStore()
store.append(torch.tensor([1.]))
store.append(torch.tensor([2.]))
store.append(torch.tensor([3.]))
store.shuffle()
len(store)
3</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">    &gt;&gt;&gt; store = ListActivationStore()</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([3.]))</span>
<span class="sd">    &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">    &gt;&gt;&gt; len(store)</span>
<span class="sd">    3</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wait_for_writes_to_complete</span><span class="p">()</span>
    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.ListActivationStore.wait_for_writes_to_complete" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">wait_for_writes_to_complete</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.ListActivationStore.wait_for_writes_to_complete" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Wait for Writes to Complete.</p>
<p>Wait for any non-blocking writes (e.g. calls to :meth:<code>append</code>) to complete.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = ListActivationStore(multiprocessing_enabled=True)
store.extend(torch.randn(3, 100))
store.wait_for_writes_to_complete()
len(store)
3</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>RuntimeError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If any exceptions occurred in the background workers.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/list_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">wait_for_writes_to_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wait for Writes to Complete.</span>

<span class="sd">    Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.randn(3, 100))</span>
<span class="sd">        &gt;&gt;&gt; store.wait_for_writes_to_complete()</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        3</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If any exceptions occurred in the background workers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Restart the pool</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="p">):</span>
            <span class="k">pass</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pool_futures</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">:</span>
        <span class="n">exceptions_report</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool_exceptions</span><span class="p">])</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Exceptions occurred in background workers:</span><span class="se">\n</span><span class="si">{</span><span class="n">exceptions_report</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.LossReducer" class="doc doc-heading">
          <code>LossReducer</code>


<a href="#sparse_autoencoder.LossReducer" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code></p>

  
      <p>Loss reducer.</p>
<p>Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to
nn.Sequential.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss
from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss
LossReducer(
...     L2ReconstructionLoss(),
...     LearnedActivationsL1Loss(0.001),
... )
LossReducer(
  (0): L2ReconstructionLoss()
  (1): LearnedActivationsL1Loss(l1_coefficient=0.001)
)</p>
</blockquote>
</blockquote>
</blockquote>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">LossReducer</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss reducer.</span>

<span class="sd">    Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to</span>
<span class="sd">    nn.Sequential.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss</span>
<span class="sd">        &gt;&gt;&gt; from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss</span>
<span class="sd">        &gt;&gt;&gt; LossReducer(</span>
<span class="sd">        ...     L2ReconstructionLoss(),</span>
<span class="sd">        ...     LearnedActivationsL1Loss(0.001),</span>
<span class="sd">        ... )</span>
<span class="sd">        LossReducer(</span>
<span class="sd">          (0): L2ReconstructionLoss()</span>
<span class="sd">          (1): LearnedActivationsL1Loss(l1_coefficient=0.001)</span>
<span class="sd">        )</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_modules</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;AbstractLoss&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Children loss modules.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Name of the loss module for logging.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;total_loss&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">loss_modules</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the loss reducer.</span>

<span class="sd">        Args:</span>
<span class="sd">            *loss_modules: Loss modules to reduce.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the loss reducer has no loss modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss_modules</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">loss_module</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Loss reducer must have at least one loss module.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
        <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reduce loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">                source model).</span>
<span class="sd">            learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">            decoded_activations: Decoded activations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Mean loss across the batch, summed across the loss modules.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_modules_loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;module train_batch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">loss_module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">all_modules_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Dir dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractLoss</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get item dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">AbstractLoss</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iterator dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length dunder method.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__dir__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__dir__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.__dir__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Dir dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dir dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LossReducer.__getitem__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Get item dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AbstractLoss</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get item dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">loss_modules</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LossReducer.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the loss reducer.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>*loss_modules</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss modules to reduce.</p>
            </div>
          </td>
          <td>
                <code>()</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the loss reducer has no loss modules.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">loss_modules</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the loss reducer.</span>

<span class="sd">    Args:</span>
<span class="sd">        *loss_modules: Loss modules to reduce.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the loss reducer has no loss modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss_modules</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">loss_module</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s2">&quot;Loss reducer must have at least one loss module.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__iter__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__iter__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.__iter__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Iterator dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">AbstractLoss</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterator dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.__len__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Length dunder method.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length dunder method.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.LossReducer.forward" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Reduce loss.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source activations (input activations to the autoencoder from the
source model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>learned_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Learned activations (intermediate activations in the autoencoder).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decoded_activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Decoded activations.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.TrainBatchStatistic" href="tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic">TrainBatchStatistic</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Mean loss across the batch, summed across the loss modules.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="n">learned_activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">decoded_activations</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainBatchStatistic</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduce loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_activations: Source activations (input activations to the autoencoder from the</span>
<span class="sd">            source model).</span>
<span class="sd">        learned_activations: Learned activations (intermediate activations in the autoencoder).</span>
<span class="sd">        decoded_activations: Decoded activations.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Mean loss across the batch, summed across the loss modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_modules_loss</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;module train_batch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">loss_module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">source_activations</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">loss_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">all_modules_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.LossReducer.log_name" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">log_name</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.LossReducer.log_name" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Log name.</p>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the loss module for logging.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/loss/reducer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log name.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Name of the loss module for logging.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;total_loss&quot;</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.LossReductionType" class="doc doc-heading">
          <code>LossReductionType</code>


<a href="#sparse_autoencoder.LossReductionType" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="strenum.LowercaseStrEnum">LowercaseStrEnum</span></code></p>

  
      <p>Loss reduction type (across batch items).</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LossReductionType</span><span class="p">(</span><span class="n">LowercaseStrEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss reduction type (across batch items).&quot;&quot;&quot;</span>

    <span class="n">MEAN</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mean loss across batch items.&quot;&quot;&quot;</span>

    <span class="n">SUM</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sum the loss from all batch items.&quot;&quot;&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.LossReductionType.MEAN" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">MEAN</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.LossReductionType.MEAN" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Mean loss across batch items.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.LossReductionType.SUM" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">SUM</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.LossReductionType.SUM" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Sum the loss from all batch items.</p>
  </div>

</div>





  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.NeuronActivityMetric" class="doc doc-heading">
          <code>NeuronActivityMetric</code>


<a href="#sparse_autoencoder.NeuronActivityMetric" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.resample.abstract_resample_metric.AbstractResampleMetric" href="metrics/resample/abstract_resample_metric/#sparse_autoencoder.metrics.resample.abstract_resample_metric.AbstractResampleMetric">AbstractResampleMetric</a></code></p>

  
      <p>Neuron activity metric.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/metrics/resample/neuron_activity_metric.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">NeuronActivityMetric</span><span class="p">(</span><span class="n">AbstractResampleMetric</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Neuron activity metric.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">ResampleMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the neuron activity metrics.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Resample metric data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary of metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">neuron_activity</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">neuron_activity</span>

        <span class="c1"># Histogram of neuron activity</span>
        <span class="n">numpy_neuron_activity</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float_</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">bins</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">numpy_neuron_activity</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="n">histogram</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;resample_alive_neuron_count&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;resample_dead_neuron_count&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">neuron_activity</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;resample_neuron_activity_histogram&quot;</span><span class="p">:</span> <span class="n">histogram</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.NeuronActivityMetric.calculate" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">calculate</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.NeuronActivityMetric.calculate" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate the neuron activity metrics.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.resample.abstract_resample_metric.ResampleMetricData" href="metrics/resample/abstract_resample_metric/#sparse_autoencoder.metrics.resample.abstract_resample_metric.ResampleMetricData">ResampleMetricData</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Resample metric data.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>dict[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dictionary of metrics.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/resample/neuron_activity_metric.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">ResampleMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the neuron activity metrics.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: Resample metric data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary of metrics.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">neuron_activity</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">neuron_activity</span>

    <span class="c1"># Histogram of neuron activity</span>
    <span class="n">numpy_neuron_activity</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float_</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron_activity</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">bins</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">numpy_neuron_activity</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">histogram</span> <span class="o">=</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;resample_alive_neuron_count&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">neuron_activity</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="s2">&quot;resample_dead_neuron_count&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">neuron_activity</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
        <span class="s2">&quot;resample_neuron_activity_histogram&quot;</span><span class="p">:</span> <span class="n">histogram</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.Pipeline" class="doc doc-heading">
          <code>Pipeline</code>


<a href="#sparse_autoencoder.Pipeline" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">

  
      <p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p>
<p>Includes all the key functionality to train a sparse autoencoder, with a specific set of
    hyperparameters.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Pipeline</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pipeline for training a Sparse Autoencoder on TransformerLens activations.</span>

<span class="sd">    Includes all the key functionality to train a sparse autoencoder, with a specific set of</span>
<span class="sd">        hyperparameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">activation_resampler</span><span class="p">:</span> <span class="n">AbstractActivationResampler</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Activation resampler to use.&quot;&quot;&quot;</span>

    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse autoencoder to train.&quot;&quot;&quot;</span>

    <span class="n">cache_name</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Name of the cache to use in the source model (hook point).&quot;&quot;&quot;</span>

    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Layer to get activations from with the source model.&quot;&quot;&quot;</span>

    <span class="n">log_frequency</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Frequency at which to log metrics (in steps).&quot;&quot;&quot;</span>

    <span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss function to use.&quot;&quot;&quot;</span>

    <span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Metrics to use.&quot;&quot;&quot;</span>

    <span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimizer to use.&quot;&quot;&quot;</span>

    <span class="n">progress_bar</span><span class="p">:</span> <span class="n">tqdm</span> <span class="o">|</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Progress bar for the pipeline.&quot;&quot;&quot;</span>

    <span class="n">source_data</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterable over the source data.&quot;&quot;&quot;</span>

    <span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source dataset to generate activation data from (tokenized prompts).&quot;&quot;&quot;</span>

    <span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Source model to get activations from.&quot;&quot;&quot;</span>

    <span class="n">total_training_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Total number of training steps state.&quot;&quot;&quot;</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">activation_resampler</span><span class="p">:</span> <span class="n">AbstractActivationResampler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
        <span class="n">cache_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span><span class="p">,</span>
        <span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span><span class="p">,</span>
        <span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span>
        <span class="n">checkpoint_directory</span><span class="p">:</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span> <span class="o">=</span> <span class="n">default_metrics</span><span class="p">,</span>
        <span class="n">source_data_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the pipeline.</span>

<span class="sd">        Args:</span>
<span class="sd">            activation_resampler: Activation resampler to use.</span>
<span class="sd">            autoencoder: Sparse autoencoder to train.</span>
<span class="sd">            cache_name: Name of the cache to use in the source model (hook point).</span>
<span class="sd">            layer: Layer to get activations from with the source model.</span>
<span class="sd">            loss: Loss function to use.</span>
<span class="sd">            optimizer: Optimizer to use.</span>
<span class="sd">            source_dataset: Source dataset to get data from.</span>
<span class="sd">            source_model: Source model to get activations from.</span>
<span class="sd">            checkpoint_directory: Directory to save checkpoints to.</span>
<span class="sd">            log_frequency: Frequency at which to log metrics (in steps)</span>
<span class="sd">            metrics: Metrics to use.</span>
<span class="sd">            source_data_batch_size: Batch size for the source data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="o">=</span> <span class="n">activation_resampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span> <span class="o">=</span> <span class="n">autoencoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span> <span class="o">=</span> <span class="n">cache_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">=</span> <span class="n">checkpoint_directory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span> <span class="o">=</span> <span class="n">log_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="n">metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">=</span> <span class="n">source_data_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span> <span class="o">=</span> <span class="n">source_dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span> <span class="o">=</span> <span class="n">source_model</span>

        <span class="n">source_dataloader</span> <span class="o">=</span> <span class="n">source_dataset</span><span class="o">.</span><span class="n">get_dataloader</span><span class="p">(</span><span class="n">source_data_batch_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateful_dataloader_iterable</span><span class="p">(</span><span class="n">source_dataloader</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorActivationStore</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate activations.</span>

<span class="sd">        Args:</span>
<span class="sd">            store_size: Number of activations to generate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Activation store for the train section.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the store size is not positive or is not divisible by the batch size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check the store size is positive and divisible by the batch size</span>
        <span class="k">if</span> <span class="n">store_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Store size must be positive, got </span><span class="si">{</span><span class="n">store_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">store_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Store size must be divisible by the batch size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span><span class="si">}</span><span class="s2">), &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">store_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

        <span class="c1"># Setup the store</span>
        <span class="n">num_neurons</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">n_input_features</span>
        <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>
        <span class="n">store</span> <span class="o">=</span> <span class="n">TensorActivationStore</span><span class="p">(</span><span class="n">store_size</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>

        <span class="c1"># Add the hook to the model (will automatically store the activations every time the model</span>
        <span class="c1"># runs)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
        <span class="n">hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">store_activations_hook</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">add_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>

        <span class="c1"># Loop through the dataloader until the store reaches the desired size</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">:</span>
                <span class="n">input_ids</span><span class="p">:</span> <span class="n">BatchTokenizedPrompts</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                    <span class="n">input_ids</span><span class="p">,</span> <span class="n">stop_at_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>  <span class="c1"># type: ignore (TLens is typed incorrectly)</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">store_size</span><span class="p">:</span>
                    <span class="k">break</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
        <span class="n">store</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">store</span>

    <span class="k">def</span> <span class="nf">train_autoencoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuronActivity</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Train the sparse autoencoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            activation_store: Activation store from the generate section.</span>
<span class="sd">            train_batch_size: Train batch size.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number of times each neuron fired.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">autoencoder_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">)</span>

        <span class="n">activations_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">activation_store</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">learned_activations_fired_count</span><span class="p">:</span> <span class="n">NeuronActivity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">autoencoder_device</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">store_batch</span> <span class="ow">in</span> <span class="n">activations_dataloader</span><span class="p">:</span>
            <span class="c1"># Zero the gradients</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Move the batch to the device (in place)</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">store_batch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">autoencoder_device</span><span class="p">)</span>

            <span class="c1"># Forward pass</span>
            <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># Get loss &amp; metrics</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">total_loss</span><span class="p">,</span> <span class="n">loss_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span>
                <span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span>
            <span class="p">)</span>
            <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss_metrics</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">:</span>
                    <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span>
                        <span class="n">TrainMetricData</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">calculated</span><span class="p">)</span>

            <span class="c1"># Store count of how many neurons have fired</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">fired</span> <span class="o">=</span> <span class="n">learned_activations</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="n">learned_activations_fired_count</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">fired</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

            <span class="c1"># Backwards pass</span>
            <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">constrain_weights_unit_norm</span><span class="p">()</span>

            <span class="c1"># Log</span>
            <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="o">**</span><span class="n">metrics</span><span class="p">,</span> <span class="o">**</span><span class="n">loss_metrics</span><span class="p">},</span> <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">learned_activations_fired_count</span>

    <span class="k">def</span> <span class="nf">resample_neurons</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span>
        <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">        Args:</span>
<span class="sd">            activation_store: Activation store.</span>
<span class="sd">            neuron_activity_sample_size: Sample size for resampling.</span>
<span class="sd">            neuron_activity: Number of times each neuron fired.</span>
<span class="sd">            train_batch_size: Train batch size (also used for resampling).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Get the updates</span>
            <span class="n">parameter_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dead_neurons</span><span class="p">(</span>
                <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                <span class="n">autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">,</span>
                <span class="n">loss_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
                <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                <span class="n">neuron_activity_sample_size</span><span class="o">=</span><span class="n">neuron_activity_sample_size</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Update the weights and biases</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
                <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_encoder_weight_updates</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_bias</span><span class="p">(</span>
                <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
                <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
                <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Log any metrics</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">resample_metrics</span><span class="p">:</span>
                        <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span>
                            <span class="n">ResampleMetricData</span><span class="p">(</span>
                                <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                        <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">calculated</span><span class="p">)</span>
                    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># Reset the optimizer (TODO: Consider resetting just the relevant parameters)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">reset_state_all_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">validate_sae</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_number_activations</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get validation metrics.</span>

<span class="sd">        Args:</span>
<span class="sd">            validation_number_activations: Number of activations to use for validation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">losses</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">losses_with_reconstruction</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">losses_with_zero_ablation</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">:</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">BatchTokenizedPrompts</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>

            <span class="c1"># Run a forward pass with and without the replaced activations</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
            <span class="n">replacement_hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">replace_activations_hook</span><span class="p">,</span> <span class="n">sparse_autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span>
            <span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
            <span class="n">loss_with_reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[</span>
                    <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span><span class="p">,</span>
                        <span class="n">replacement_hook</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">],</span>
            <span class="p">)</span>
            <span class="n">loss_with_zero_ablation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
                <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">zero_ablate_hook</span><span class="p">)],</span>
            <span class="p">)</span>

            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">losses_with_reconstruction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_with_reconstruction</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">losses_with_zero_ablation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_with_zero_ablation</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">validation_number_activations</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="c1"># Log</span>
        <span class="n">validation_data</span> <span class="o">=</span> <span class="n">ValidationMetricData</span><span class="p">(</span>
            <span class="n">source_model_loss</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span>
            <span class="n">source_model_loss_with_reconstruction</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">losses_with_reconstruction</span><span class="p">),</span>
            <span class="n">source_model_loss_with_zero_ablation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">losses_with_zero_ablation</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">validation_metrics</span><span class="p">:</span>
            <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">calculated</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@final</span>
    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save the model as a checkpoint.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span><span class="p">:</span>
            <span class="n">file_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;sae_state_dict-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span><span class="si">}</span><span class="s2">.pt&quot;</span>
            <span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">file_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run_pipeline</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_store_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">max_activations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">resample_frequency</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">validation_number_activations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="n">validate_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">checkpoint_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run the full training pipeline.</span>

<span class="sd">        Args:</span>
<span class="sd">            train_batch_size: Train batch size.</span>
<span class="sd">            max_store_size: Maximum size of the activation store.</span>
<span class="sd">            max_activations: Maximum total number of activations to train on (the original paper</span>
<span class="sd">                used 8bn, although others have had success with 100m+).</span>
<span class="sd">            resample_frequency: Frequency at which to resample dead neurons (the original paper used</span>
<span class="sd">                every 200m).</span>
<span class="sd">            validation_number_activations: Number of activations to use for validation.</span>
<span class="sd">            validate_frequency: Frequency at which to get validation metrics.</span>
<span class="sd">            checkpoint_frequency: Frequency at which to save a checkpoint.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">last_resampled</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">last_validated</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">last_checkpoint</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total_activations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set the source model to evaluation (inference) mode</span>

        <span class="c1"># Get the store size</span>
        <span class="n">store_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">max_store_size</span> <span class="o">-</span> <span class="n">max_store_size</span> <span class="o">%</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span><span class="o">.</span><span class="n">context_size</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Activations trained on&quot;</span><span class="p">,</span>
            <span class="n">total</span><span class="o">=</span><span class="n">max_activations</span><span class="p">,</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">progress_bar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_activations</span><span class="p">,</span> <span class="n">store_size</span><span class="p">):</span>
                <span class="c1"># Generate</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;generate&quot;</span><span class="p">})</span>
                <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_activations</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>

                <span class="c1"># Update the counters</span>
                <span class="n">num_activation_vectors_in_store</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>
                <span class="n">last_resampled</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
                <span class="n">last_validated</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
                <span class="n">last_checkpoint</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
                <span class="n">total_activations</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
                <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s2">&quot;activations_generated&quot;</span><span class="p">:</span> <span class="n">total_activations</span><span class="p">},</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="c1"># Train</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;train&quot;</span><span class="p">})</span>
                <span class="n">batch_neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_autoencoder</span><span class="p">(</span>
                    <span class="n">activation_store</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span>
                <span class="p">)</span>
                <span class="n">detached_neuron_activity</span> <span class="o">=</span> <span class="n">batch_neuron_activity</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="n">is_second_half_resample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">last_resampled</span> <span class="o">&gt;</span> <span class="n">resample_frequency</span> <span class="o">/</span> <span class="mi">2</span>
                <span class="k">if</span> <span class="n">is_second_half_resample</span><span class="p">:</span>
                    <span class="n">neuron_activity_sample_size</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
                    <span class="n">neuron_activity</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">detached_neuron_activity</span><span class="p">)</span>

                <span class="c1"># Resample dead neurons (if needed)</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;resample&quot;</span><span class="p">})</span>
                <span class="k">if</span> <span class="n">last_resampled</span> <span class="o">&gt;=</span> <span class="n">resample_frequency</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">resample_neurons</span><span class="p">(</span>
                        <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                        <span class="n">neuron_activity_sample_size</span><span class="o">=</span><span class="n">neuron_activity_sample_size</span><span class="p">,</span>
                        <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
                        <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                    <span class="p">)</span>

                    <span class="c1"># Reset</span>
                    <span class="n">last_resampled</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">neuron_activity_sample_size</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">neuron_activity</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

                <span class="c1"># Get validation metrics (if needed)</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;validate&quot;</span><span class="p">})</span>
                <span class="k">if</span> <span class="n">validate_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_validated</span> <span class="o">&gt;=</span> <span class="n">validate_frequency</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">validate_sae</span><span class="p">(</span><span class="n">validation_number_activations</span><span class="p">)</span>
                    <span class="n">last_validated</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="c1"># Checkpoint (if needed)</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">})</span>
                <span class="k">if</span> <span class="n">checkpoint_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_checkpoint</span> <span class="o">&gt;=</span> <span class="n">checkpoint_frequency</span><span class="p">:</span>
                    <span class="n">last_checkpoint</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

                <span class="c1"># Update the progress bar</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">stateful_dataloader_iterable</span><span class="p">(</span>
        <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a stateful dataloader iterable.</span>

<span class="sd">        Create an iterable that maintains it&#39;s position in the dataloader between loops.</span>

<span class="sd">        Examples:</span>
<span class="sd">            Without this, when iterating over a DataLoader with 2 loops, each loop get the same data</span>
<span class="sd">            (assuming shuffle is turned off). That is to say, the second loop won&#39;t maintain the</span>
<span class="sd">            position from where the first loop left off.</span>

<span class="sd">            &gt;&gt;&gt; from datasets import Dataset</span>
<span class="sd">            &gt;&gt;&gt; from torch.utils.data import DataLoader</span>
<span class="sd">            &gt;&gt;&gt; def gen():</span>
<span class="sd">            ...     yield {&quot;int&quot;: 0}</span>
<span class="sd">            ...     yield {&quot;int&quot;: 1}</span>
<span class="sd">            &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))</span>
<span class="sd">            &gt;&gt;&gt; next(iter(data))[&quot;int&quot;], next(iter(data))[&quot;int&quot;]</span>
<span class="sd">            (tensor([0]), tensor([0]))</span>

<span class="sd">            By contrast if you create a stateful iterable from the dataloader, each loop will get</span>
<span class="sd">            different data.</span>

<span class="sd">            &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)</span>
<span class="sd">            &gt;&gt;&gt; next(iterator)[&quot;int&quot;], next(iterator)[&quot;int&quot;]</span>
<span class="sd">            (tensor([0]), tensor([1]))</span>

<span class="sd">        Args:</span>
<span class="sd">            dataloader: PyTorch DataLoader.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Stateful iterable over the data in the dataloader.</span>

<span class="sd">        Yields:</span>
<span class="sd">            Data from the dataloader.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">yield from</span> <span class="n">dataloader</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.activation_resampler" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">activation_resampler</span><span class="p">:</span> <span class="n">AbstractActivationResampler</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">activation_resampler</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.activation_resampler" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Activation resampler to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.autoencoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span> <span class="o">=</span> <span class="n">autoencoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.autoencoder" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Sparse autoencoder to train.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.cache_name" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">cache_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">cache_name</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.cache_name" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Name of the cache to use in the source model (hook point).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.layer" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">layer</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.layer" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Layer to get activations from with the source model.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.log_frequency" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">log_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">log_frequency</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.log_frequency" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Frequency at which to log metrics (in steps).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span> <span class="o">=</span> <span class="n">loss</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.loss" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Loss function to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.metrics" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span> <span class="o">=</span> <span class="n">metrics</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.metrics" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Metrics to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.optimizer" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span> <span class="o">=</span> <span class="n">optimizer</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.optimizer" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Optimizer to use.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.progress_bar" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">progress_bar</span><span class="p">:</span> <span class="n">tqdm</span> <span class="o">|</span> <span class="kc">None</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.progress_bar" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Progress bar for the pipeline.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.source_data" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">source_data</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateful_dataloader_iterable</span><span class="p">(</span><span class="n">source_dataloader</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.source_data" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Iterable over the source data.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.source_dataset" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span> <span class="o">=</span> <span class="n">source_dataset</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.source_dataset" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Source dataset to generate activation data from (tokenized prompts).</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.source_model" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span> <span class="o">=</span> <span class="n">source_model</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.source_model" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Source model to get activations from.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.Pipeline.total_training_steps" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">total_training_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.total_training_steps" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Total number of training steps state.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">activation_resampler</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">,</span> <span class="n">cache_name</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">source_dataset</span><span class="p">,</span> <span class="n">source_model</span><span class="p">,</span> <span class="n">checkpoint_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">log_frequency</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">default_metrics</span><span class="p">,</span> <span class="n">source_data_batch_size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the pipeline.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation_resampler</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler" href="activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler">AbstractActivationResampler</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation resampler to use.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>autoencoder</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.model.SparseAutoencoder" href="autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder">SparseAutoencoder</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sparse autoencoder to train.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>cache_name</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Name of the cache to use in the source model (hook point).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>layer</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Layer to get activations from with the source model.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>loss</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.loss.abstract_loss.AbstractLoss" href="loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss">AbstractLoss</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Loss function to use.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>optimizer</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset" href="optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset">AbstractOptimizerWithReset</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Optimizer to use.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_dataset</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source dataset to get data from.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>source_model</code></td>
          <td>
                <code><span title="transformer_lens.HookedTransformer">HookedTransformer</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Source model to get activations from.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>checkpoint_directory</code></td>
          <td>
                <code><span title="pathlib.Path">Path</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Directory to save checkpoints to.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>log_frequency</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Frequency at which to log metrics (in steps)</p>
            </div>
          </td>
          <td>
                <code>100</code>
          </td>
        </tr>
        <tr>
          <td><code>metrics</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.metrics_container.MetricsContainer" href="metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer">MetricsContainer</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Metrics to use.</p>
            </div>
          </td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.metrics_container.default_metrics" href="metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.default_metrics">default_metrics</a></code>
          </td>
        </tr>
        <tr>
          <td><code>source_data_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Batch size for the source data.</p>
            </div>
          </td>
          <td>
                <code>12</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa: PLR0913</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">activation_resampler</span><span class="p">:</span> <span class="n">AbstractActivationResampler</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">autoencoder</span><span class="p">:</span> <span class="n">SparseAutoencoder</span><span class="p">,</span>
    <span class="n">cache_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">AbstractLoss</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">AbstractOptimizerWithReset</span><span class="p">,</span>
    <span class="n">source_dataset</span><span class="p">:</span> <span class="n">SourceDataset</span><span class="p">,</span>
    <span class="n">source_model</span><span class="p">:</span> <span class="n">HookedTransformer</span><span class="p">,</span>
    <span class="n">checkpoint_directory</span><span class="p">:</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">log_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">MetricsContainer</span> <span class="o">=</span> <span class="n">default_metrics</span><span class="p">,</span>
    <span class="n">source_data_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the pipeline.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_resampler: Activation resampler to use.</span>
<span class="sd">        autoencoder: Sparse autoencoder to train.</span>
<span class="sd">        cache_name: Name of the cache to use in the source model (hook point).</span>
<span class="sd">        layer: Layer to get activations from with the source model.</span>
<span class="sd">        loss: Loss function to use.</span>
<span class="sd">        optimizer: Optimizer to use.</span>
<span class="sd">        source_dataset: Source dataset to get data from.</span>
<span class="sd">        source_model: Source model to get activations from.</span>
<span class="sd">        checkpoint_directory: Directory to save checkpoints to.</span>
<span class="sd">        log_frequency: Frequency at which to log metrics (in steps)</span>
<span class="sd">        metrics: Metrics to use.</span>
<span class="sd">        source_data_batch_size: Batch size for the source data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="o">=</span> <span class="n">activation_resampler</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span> <span class="o">=</span> <span class="n">autoencoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span> <span class="o">=</span> <span class="n">cache_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">=</span> <span class="n">checkpoint_directory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span> <span class="o">=</span> <span class="n">log_frequency</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="n">metrics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">=</span> <span class="n">source_data_batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span> <span class="o">=</span> <span class="n">source_dataset</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span> <span class="o">=</span> <span class="n">source_model</span>

    <span class="n">source_dataloader</span> <span class="o">=</span> <span class="n">source_dataset</span><span class="o">.</span><span class="n">get_dataloader</span><span class="p">(</span><span class="n">source_data_batch_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateful_dataloader_iterable</span><span class="p">(</span><span class="n">source_dataloader</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.generate_activations" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">generate_activations</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.generate_activations" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Generate activations.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>store_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of activations to generate.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.tensor_store.TensorActivationStore" href="activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore">TensorActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store for the train section.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the store size is not positive or is not divisible by the batch size.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate_activations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorActivationStore</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate activations.</span>

<span class="sd">    Args:</span>
<span class="sd">        store_size: Number of activations to generate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Activation store for the train section.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the store size is not positive or is not divisible by the batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check the store size is positive and divisible by the batch size</span>
    <span class="k">if</span> <span class="n">store_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Store size must be positive, got </span><span class="si">{</span><span class="n">store_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">store_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Store size must be divisible by the batch size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span><span class="si">}</span><span class="s2">), &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">store_size</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

    <span class="c1"># Setup the store</span>
    <span class="n">num_neurons</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">n_input_features</span>
    <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">TensorActivationStore</span><span class="p">(</span><span class="n">store_size</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">)</span>

    <span class="c1"># Add the hook to the model (will automatically store the activations every time the model</span>
    <span class="c1"># runs)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
    <span class="n">hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">store_activations_hook</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">add_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>

    <span class="c1"># Loop through the dataloader until the store reaches the desired size</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">:</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">BatchTokenizedPrompts</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                <span class="n">input_ids</span><span class="p">,</span> <span class="n">stop_at_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>  <span class="c1"># type: ignore (TLens is typed incorrectly)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">store_size</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
    <span class="n">store</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">store</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.resample_neurons" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">resample_neurons</span><span class="p">(</span><span class="n">activation_store</span><span class="p">,</span> <span class="n">neuron_activity_sample_size</span><span class="p">,</span> <span class="n">neuron_activity</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.resample_neurons" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Resample dead neurons.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation_store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.tensor_store.TensorActivationStore" href="activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore">TensorActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_activity_sample_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sample size for resampling.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>neuron_activity</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each neuron fired.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size (also used for resampling).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">resample_neurons</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span>
    <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resample dead neurons.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_store: Activation store.</span>
<span class="sd">        neuron_activity_sample_size: Sample size for resampling.</span>
<span class="sd">        neuron_activity: Number of times each neuron fired.</span>
<span class="sd">        train_batch_size: Train batch size (also used for resampling).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Get the updates</span>
        <span class="n">parameter_updates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span><span class="o">.</span><span class="n">resample_dead_neurons</span><span class="p">(</span>
            <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
            <span class="n">autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">,</span>
            <span class="n">loss_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
            <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
            <span class="n">neuron_activity_sample_size</span><span class="o">=</span><span class="n">neuron_activity_sample_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Update the weights and biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
            <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_encoder_weight_updates</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">update_bias</span><span class="p">(</span>
            <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_encoder_bias_updates</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">update_dictionary_vectors</span><span class="p">(</span>
            <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_neuron_indices</span><span class="p">,</span>
            <span class="n">parameter_updates</span><span class="o">.</span><span class="n">dead_decoder_weight_updates</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Log any metrics</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">resample_metrics</span><span class="p">:</span>
                    <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span>
                        <span class="n">ResampleMetricData</span><span class="p">(</span>
                            <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">calculated</span><span class="p">)</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Reset the optimizer (TODO: Consider resetting just the relevant parameters)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">reset_state_all_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.run_pipeline" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">run_pipeline</span><span class="p">(</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">max_store_size</span><span class="p">,</span> <span class="n">max_activations</span><span class="p">,</span> <span class="n">resample_frequency</span><span class="p">,</span> <span class="n">validation_number_activations</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">validate_frequency</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">checkpoint_frequency</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.run_pipeline" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Run the full training pipeline.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>max_store_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum size of the activation store.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>max_activations</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum total number of activations to train on (the original paper
used 8bn, although others have had success with 100m+).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>resample_frequency</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Frequency at which to resample dead neurons (the original paper used
every 200m).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>validation_number_activations</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of activations to use for validation.</p>
            </div>
          </td>
          <td>
                <code>1024</code>
          </td>
        </tr>
        <tr>
          <td><code>validate_frequency</code></td>
          <td>
                <code>int | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Frequency at which to get validation metrics.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>checkpoint_frequency</code></td>
          <td>
                <code>int | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Frequency at which to save a checkpoint.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">run_pipeline</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_store_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_activations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">resample_frequency</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">validation_number_activations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="n">validate_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">checkpoint_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run the full training pipeline.</span>

<span class="sd">    Args:</span>
<span class="sd">        train_batch_size: Train batch size.</span>
<span class="sd">        max_store_size: Maximum size of the activation store.</span>
<span class="sd">        max_activations: Maximum total number of activations to train on (the original paper</span>
<span class="sd">            used 8bn, although others have had success with 100m+).</span>
<span class="sd">        resample_frequency: Frequency at which to resample dead neurons (the original paper used</span>
<span class="sd">            every 200m).</span>
<span class="sd">        validation_number_activations: Number of activations to use for validation.</span>
<span class="sd">        validate_frequency: Frequency at which to get validation metrics.</span>
<span class="sd">        checkpoint_frequency: Frequency at which to save a checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">last_resampled</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">neuron_activity_sample_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">last_validated</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">last_checkpoint</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_activations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set the source model to evaluation (inference) mode</span>

    <span class="c1"># Get the store size</span>
    <span class="n">store_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">max_store_size</span> <span class="o">-</span> <span class="n">max_store_size</span> <span class="o">%</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_data_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_dataset</span><span class="o">.</span><span class="n">context_size</span>
    <span class="p">)</span>

    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span>
        <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Activations trained on&quot;</span><span class="p">,</span>
        <span class="n">total</span><span class="o">=</span><span class="n">max_activations</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">progress_bar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_activations</span><span class="p">,</span> <span class="n">store_size</span><span class="p">):</span>
            <span class="c1"># Generate</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;generate&quot;</span><span class="p">})</span>
            <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_activations</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>

            <span class="c1"># Update the counters</span>
            <span class="n">num_activation_vectors_in_store</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_store</span><span class="p">)</span>
            <span class="n">last_resampled</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
            <span class="n">last_validated</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
            <span class="n">last_checkpoint</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
            <span class="n">total_activations</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
            <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span><span class="s2">&quot;activations_generated&quot;</span><span class="p">:</span> <span class="n">total_activations</span><span class="p">},</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># Train</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;train&quot;</span><span class="p">})</span>
            <span class="n">batch_neuron_activity</span><span class="p">:</span> <span class="n">NeuronActivity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_autoencoder</span><span class="p">(</span>
                <span class="n">activation_store</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span>
            <span class="p">)</span>
            <span class="n">detached_neuron_activity</span> <span class="o">=</span> <span class="n">batch_neuron_activity</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="n">is_second_half_resample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">last_resampled</span> <span class="o">&gt;</span> <span class="n">resample_frequency</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">is_second_half_resample</span><span class="p">:</span>
                <span class="n">neuron_activity_sample_size</span> <span class="o">+=</span> <span class="n">num_activation_vectors_in_store</span>
                <span class="n">neuron_activity</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">detached_neuron_activity</span><span class="p">)</span>

            <span class="c1"># Resample dead neurons (if needed)</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;resample&quot;</span><span class="p">})</span>
            <span class="k">if</span> <span class="n">last_resampled</span> <span class="o">&gt;=</span> <span class="n">resample_frequency</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_resampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">resample_neurons</span><span class="p">(</span>
                    <span class="n">activation_store</span><span class="o">=</span><span class="n">activation_store</span><span class="p">,</span>
                    <span class="n">neuron_activity_sample_size</span><span class="o">=</span><span class="n">neuron_activity_sample_size</span><span class="p">,</span>
                    <span class="n">neuron_activity</span><span class="o">=</span><span class="n">neuron_activity</span><span class="p">,</span>
                    <span class="n">train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># Reset</span>
                <span class="n">last_resampled</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">neuron_activity_sample_size</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">neuron_activity</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="c1"># Get validation metrics (if needed)</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;validate&quot;</span><span class="p">})</span>
            <span class="k">if</span> <span class="n">validate_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_validated</span> <span class="o">&gt;=</span> <span class="n">validate_frequency</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">validate_sae</span><span class="p">(</span><span class="n">validation_number_activations</span><span class="p">)</span>
                <span class="n">last_validated</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># Checkpoint (if needed)</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">})</span>
            <span class="k">if</span> <span class="n">checkpoint_frequency</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_checkpoint</span> <span class="o">&gt;=</span> <span class="n">checkpoint_frequency</span><span class="p">:</span>
                <span class="n">last_checkpoint</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

            <span class="c1"># Update the progress bar</span>
            <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">store_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.save_checkpoint" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">save_checkpoint</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.Pipeline.save_checkpoint" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Save the model as a checkpoint.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save the model as a checkpoint.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span><span class="p">:</span>
        <span class="n">file_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_directory</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;sae_state_dict-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span><span class="si">}</span><span class="s2">.pt&quot;</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">file_path</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.stateful_dataloader_iterable" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">stateful_dataloader_iterable</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.Pipeline.stateful_dataloader_iterable" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Create a stateful dataloader iterable.</p>
<p>Create an iterable that maintains it's position in the dataloader between loops.</p>



<p><strong>Examples:</strong></p>
    <p>Without this, when iterating over a DataLoader with 2 loops, each loop get the same data
(assuming shuffle is turned off). That is to say, the second loop won't maintain the
position from where the first loop left off.</p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">gen</span><span class="p">():</span>
<span class="gp">... </span>    <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;int&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="gp">... </span>    <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;int&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span><span class="n">gen</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data</span><span class="p">))[</span><span class="s2">&quot;int&quot;</span><span class="p">],</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data</span><span class="p">))[</span><span class="s2">&quot;int&quot;</span><span class="p">]</span>
<span class="go">(tensor([0]), tensor([0]))</span>
</code></pre></div>
    <p>By contrast if you create a stateful iterable from the dataloader, each loop will get
different data.</p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">iterator</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="o">.</span><span class="n">stateful_dataloader_iterable</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)[</span><span class="s2">&quot;int&quot;</span><span class="p">],</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)[</span><span class="s2">&quot;int&quot;</span><span class="p">]</span>
<span class="go">(tensor([0]), tensor([1]))</span>
</code></pre></div>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts">TorchTokenizedPrompts</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>PyTorch DataLoader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="collections.abc.Iterable">Iterable</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts">TorchTokenizedPrompts</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Stateful iterable over the data in the dataloader.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Yields:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="collections.abc.Iterable">Iterable</span>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts">TorchTokenizedPrompts</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Data from the dataloader.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">stateful_dataloader_iterable</span><span class="p">(</span>
    <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">TorchTokenizedPrompts</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a stateful dataloader iterable.</span>

<span class="sd">    Create an iterable that maintains it&#39;s position in the dataloader between loops.</span>

<span class="sd">    Examples:</span>
<span class="sd">        Without this, when iterating over a DataLoader with 2 loops, each loop get the same data</span>
<span class="sd">        (assuming shuffle is turned off). That is to say, the second loop won&#39;t maintain the</span>
<span class="sd">        position from where the first loop left off.</span>

<span class="sd">        &gt;&gt;&gt; from datasets import Dataset</span>
<span class="sd">        &gt;&gt;&gt; from torch.utils.data import DataLoader</span>
<span class="sd">        &gt;&gt;&gt; def gen():</span>
<span class="sd">        ...     yield {&quot;int&quot;: 0}</span>
<span class="sd">        ...     yield {&quot;int&quot;: 1}</span>
<span class="sd">        &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))</span>
<span class="sd">        &gt;&gt;&gt; next(iter(data))[&quot;int&quot;], next(iter(data))[&quot;int&quot;]</span>
<span class="sd">        (tensor([0]), tensor([0]))</span>

<span class="sd">        By contrast if you create a stateful iterable from the dataloader, each loop will get</span>
<span class="sd">        different data.</span>

<span class="sd">        &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)</span>
<span class="sd">        &gt;&gt;&gt; next(iterator)[&quot;int&quot;], next(iterator)[&quot;int&quot;]</span>
<span class="sd">        (tensor([0]), tensor([1]))</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader: PyTorch DataLoader.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stateful iterable over the data in the dataloader.</span>

<span class="sd">    Yields:</span>
<span class="sd">        Data from the dataloader.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">yield from</span> <span class="n">dataloader</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.train_autoencoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">train_autoencoder</span><span class="p">(</span><span class="n">activation_store</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.train_autoencoder" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Train the sparse autoencoder.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activation_store</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.tensor_store.TensorActivationStore" href="activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore">TensorActivationStore</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Activation store from the generate section.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>train_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train batch size.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.NeuronActivity" href="tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity">NeuronActivity</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each neuron fired.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train_autoencoder</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">activation_store</span><span class="p">:</span> <span class="n">TensorActivationStore</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NeuronActivity</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train the sparse autoencoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        activation_store: Activation store from the generate section.</span>
<span class="sd">        train_batch_size: Train batch size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Number of times each neuron fired.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">autoencoder_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">)</span>

    <span class="n">activations_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">activation_store</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">learned_activations_fired_count</span><span class="p">:</span> <span class="n">NeuronActivity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">autoencoder_device</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">store_batch</span> <span class="ow">in</span> <span class="n">activations_dataloader</span><span class="p">:</span>
        <span class="c1"># Zero the gradients</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Move the batch to the device (in place)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">store_batch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">autoencoder_device</span><span class="p">)</span>

        <span class="c1"># Forward pass</span>
        <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="c1"># Get loss &amp; metrics</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">total_loss</span><span class="p">,</span> <span class="n">loss_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">batch_scalar_loss_with_log</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span>
        <span class="p">)</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loss_metrics</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">:</span>
                <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span>
                    <span class="n">TrainMetricData</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">reconstructed_activations</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">calculated</span><span class="p">)</span>

        <span class="c1"># Store count of how many neurons have fired</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">fired</span> <span class="o">=</span> <span class="n">learned_activations</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="n">learned_activations_fired_count</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">fired</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="c1"># Backwards pass</span>
        <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">constrain_weights_unit_norm</span><span class="p">()</span>

        <span class="c1"># Log</span>
        <span class="k">if</span> <span class="n">wandb</span><span class="o">.</span><span class="n">run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="o">**</span><span class="n">metrics</span><span class="p">,</span> <span class="o">**</span><span class="n">loss_metrics</span><span class="p">},</span> <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">learned_activations_fired_count</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.Pipeline.validate_sae" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">validate_sae</span><span class="p">(</span><span class="n">validation_number_activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.Pipeline.validate_sae" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Get validation metrics.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>validation_number_activations</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of activations to use for validation.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/train/pipeline.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">validate_sae</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_number_activations</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get validation metrics.</span>

<span class="sd">    Args:</span>
<span class="sd">        validation_number_activations: Number of activations to use for validation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">losses</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">losses_with_reconstruction</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">losses_with_zero_ablation</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">source_model_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">get_model_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_data</span><span class="p">:</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">BatchTokenizedPrompts</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">source_model_device</span><span class="p">)</span>

        <span class="c1"># Run a forward pass with and without the replaced activations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">remove_all_hook_fns</span><span class="p">()</span>
        <span class="n">replacement_hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">replace_activations_hook</span><span class="p">,</span> <span class="n">sparse_autoencoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">autoencoder</span>
        <span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
        <span class="n">loss_with_reconstruction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
            <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[</span>
                <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span><span class="p">,</span>
                    <span class="n">replacement_hook</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">],</span>
        <span class="p">)</span>
        <span class="n">loss_with_zero_ablation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_model</span><span class="o">.</span><span class="n">run_with_hooks</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
            <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_name</span><span class="p">,</span> <span class="n">zero_ablate_hook</span><span class="p">)],</span>
        <span class="p">)</span>

        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">losses_with_reconstruction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_with_reconstruction</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">losses_with_zero_ablation</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_with_zero_ablation</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">validation_number_activations</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="c1"># Log</span>
    <span class="n">validation_data</span> <span class="o">=</span> <span class="n">ValidationMetricData</span><span class="p">(</span>
        <span class="n">source_model_loss</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span>
        <span class="n">source_model_loss_with_reconstruction</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">losses_with_reconstruction</span><span class="p">),</span>
        <span class="n">source_model_loss_with_zero_ablation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">losses_with_zero_ablation</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">validation_metrics</span><span class="p">:</span>
        <span class="n">calculated</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">calculated</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_training_steps</span><span class="p">,</span> <span class="n">commit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.PreTokenizedDataset" class="doc doc-heading">
          <code>PreTokenizedDataset</code>


<a href="#sparse_autoencoder.PreTokenizedDataset" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataBatch" href="source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataBatch">PreTokenizedDataBatch</a>]</code></p>

  
      <p>General Pre-Tokenized Dataset from Hugging Face.</p>
<p>Can be used for various datasets available on Hugging Face.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">PreTokenizedDataset</span><span class="p">(</span><span class="n">SourceDataset</span><span class="p">[</span><span class="n">PreTokenizedDataBatch</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;General Pre-Tokenized Dataset from Hugging Face.</span>

<span class="sd">    Can be used for various datasets available on Hugging Face.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_batch</span><span class="p">:</span> <span class="n">PreTokenizedDataBatch</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">        The method splits each pre-tokenized item based on the context size.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_batch: A batch of source data.</span>
<span class="sd">            context_size: The context size to use for tokenized prompts.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tokenized prompts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tokenized_prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span>

        <span class="c1"># Chunk each tokenized prompt into blocks of context_size,</span>
        <span class="c1"># discarding the last block if too small.</span>
        <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="n">tokenized_prompts</span><span class="p">:</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
            <span class="p">]</span>
            <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize a pre-tokenized dataset from Hugging Face.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset_path: The path to the dataset on Hugging Face.</span>
<span class="sd">            context_size: The context size for tokenized prompts.</span>
<span class="sd">            buffer_size: Buffer size for shuffling the dataset.</span>
<span class="sd">            preprocess_batch_size: Batch size for preprocessing.</span>
<span class="sd">            dataset_split: Dataset split (e.g., `train`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
            <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
            <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
            <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
            <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.PreTokenizedDataset.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dataset_split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.PreTokenizedDataset.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize a pre-tokenized dataset from Hugging Face.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataset_path</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The path to the dataset on Hugging Face.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The context size for tokenized prompts.</p>
            </div>
          </td>
          <td>
                <code>256</code>
          </td>
        </tr>
        <tr>
          <td><code>buffer_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Buffer size for shuffling the dataset.</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>preprocess_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Batch size for preprocessing.</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_split</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dataset split (e.g., <code>train</code>).</p>
            </div>
          </td>
          <td>
                <code>&#39;train&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize a pre-tokenized dataset from Hugging Face.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset_path: The path to the dataset on Hugging Face.</span>
<span class="sd">        context_size: The context size for tokenized prompts.</span>
<span class="sd">        buffer_size: Buffer size for shuffling the dataset.</span>
<span class="sd">        preprocess_batch_size: Batch size for preprocessing.</span>
<span class="sd">        dataset_split: Dataset split (e.g., `train`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
        <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.PreTokenizedDataset.preprocess" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">preprocess</span><span class="p">(</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.PreTokenizedDataset.preprocess" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Preprocess a batch of prompts.</p>
<p>The method splits each pre-tokenized item based on the context size.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_batch</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataBatch" href="source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataBatch">PreTokenizedDataBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A batch of source data.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The context size to use for tokenized prompts.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts">TokenizedPrompts</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tokenized prompts.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_batch</span><span class="p">:</span> <span class="n">PreTokenizedDataBatch</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">    The method splits each pre-tokenized item based on the context size.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_batch: A batch of source data.</span>
<span class="sd">        context_size: The context size to use for tokenized prompts.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tokenized prompts.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tokenized_prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span>

    <span class="c1"># Chunk each tokenized prompt into blocks of context_size,</span>
    <span class="c1"># discarding the last block if too small.</span>
    <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="n">tokenized_prompts</span><span class="p">:</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
        <span class="p">]</span>
        <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.SparseAutoencoder" class="doc doc-heading">
          <code>SparseAutoencoder</code>


<a href="#sparse_autoencoder.SparseAutoencoder" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder" href="autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder">AbstractAutoencoder</a></code></p>

  
      <p>Sparse Autoencoder Model.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">AbstractAutoencoder</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse Autoencoder Model.&quot;&quot;&quot;</span>

    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimated Geometric Median of the Dataset.</span>

<span class="sd">    Used for initialising :attr:`tied_bias`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tied_bias</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tied Bias Parameter.</span>

<span class="sd">    The same bias is used pre-encoder and post-decoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of Input Features.&quot;&quot;&quot;</span>

    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of Learned Features.&quot;&quot;&quot;</span>

    <span class="n">_pre_encoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span>

    <span class="n">_encoder</span><span class="p">:</span> <span class="n">LinearEncoder</span>

    <span class="n">_decoder</span><span class="p">:</span> <span class="n">UnitNormDecoder</span>

    <span class="n">_post_decoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pre_encoder_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TiedBias</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pre-encoder bias.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LinearEncoder</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Encoder.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">UnitNormDecoder</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Decoder.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">post_decoder_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TiedBias</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Post-decoder bias.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">        Args:</span>
<span class="sd">            n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations</span>
<span class="sd">                from TransformerLens).</span>
<span class="sd">            n_learned_features: Number of learned features. The initial paper experimented with 1 to</span>
<span class="sd">                256 times the number of input features, and primarily used a multiple of 8.</span>
<span class="sd">            geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>

        <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
        <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
        <span class="k">if</span> <span class="n">geometric_median_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Initialize the tied bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span> <span class="o">=</span> <span class="n">LinearEncoder</span><span class="p">(</span>
            <span class="n">input_features</span><span class="o">=</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span> <span class="o">=</span> <span class="n">UnitNormDecoder</span><span class="p">(</span>
            <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">decoded_features</span><span class="o">=</span><span class="n">n_input_features</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
        <span class="n">LearnedActivationBatch</span><span class="p">,</span>
        <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of learned activations and decoded activations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
        <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>

    <span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
        <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.decoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">decoder</span><span class="p">:</span> <span class="n">UnitNormDecoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.decoder" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Decoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.encoder" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">encoder</span><span class="p">:</span> <span class="n">LinearEncoder</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.encoder" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Encoder.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.geometric_median_dataset" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Estimated Geometric Median of the Dataset.</p>
<p>Used for initialising :attr:<code>tied_bias</code>.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.n_input_features" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_input_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.n_input_features" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of Input Features.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.n_learned_features" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">n_learned_features</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.n_learned_features" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of Learned Features.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.post_decoder_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">post_decoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.post_decoder_bias" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Post-decoder bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.pre_encoder_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">pre_encoder_bias</span><span class="p">:</span> <span class="n">TiedBias</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.pre_encoder_bias" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Pre-encoder bias.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.SparseAutoencoder.tied_bias" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">tied_bias</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">))</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.SparseAutoencoder.tied_bias" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Tied Bias Parameter.</p>
<p>The same bias is used pre-encoder and post-decoder.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">n_learned_features</span><span class="p">,</span> <span class="n">geometric_median_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the Sparse Autoencoder Model.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>n_input_features</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of input features (e.g. <code>d_mlp</code> if training on MLP activations
from TransformerLens).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>n_learned_features</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of learned features. The initial paper experimented with 1 to
256 times the number of input features, and primarily used a multiple of 8.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>geometric_median_dataset</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Estimated geometric median of the dataset.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">n_input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_learned_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">geometric_median_dataset</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the Sparse Autoencoder Model.</span>

<span class="sd">    Args:</span>
<span class="sd">        n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations</span>
<span class="sd">            from TransformerLens).</span>
<span class="sd">        n_learned_features: Number of learned features. The initial paper experimented with 1 to</span>
<span class="sd">            256 times the number of input features, and primarily used a multiple of 8.</span>
<span class="sd">        geometric_median_dataset: Estimated geometric median of the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_features</span> <span class="o">=</span> <span class="n">n_input_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_learned_features</span> <span class="o">=</span> <span class="n">n_learned_features</span>

    <span class="c1"># Store the geometric median of the dataset (so that we can reset parameters). This is not a</span>
    <span class="c1"># parameter itself (the tied bias parameter is used for that), so gradients are disabled.</span>
    <span class="k">if</span> <span class="n">geometric_median_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Initialize the tied bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_input_features</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">PRE_ENCODER</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span> <span class="o">=</span> <span class="n">LinearEncoder</span><span class="p">(</span>
        <span class="n">input_features</span><span class="o">=</span><span class="n">n_input_features</span><span class="p">,</span> <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span> <span class="o">=</span> <span class="n">UnitNormDecoder</span><span class="p">(</span>
        <span class="n">learnt_features</span><span class="o">=</span><span class="n">n_learned_features</span><span class="p">,</span> <span class="n">decoded_features</span><span class="o">=</span><span class="n">n_input_features</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span> <span class="o">=</span> <span class="n">TiedBias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="p">,</span> <span class="n">TiedBiasPosition</span><span class="o">.</span><span class="n">POST_DECODER</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.forward" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Forward Pass.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>tuple[<a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a>, <a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch">InputOutputActivationBatch</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tuple of learned activations and decoded activations.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span>
    <span class="n">LearnedActivationBatch</span><span class="p">,</span>
    <span class="n">InputOutputActivationBatch</span><span class="p">,</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward Pass.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Input activations (e.g. activations from an MLP layer in a transformer model).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple of learned activations and decoded activations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_encoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">learned_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span><span class="n">learned_activations</span><span class="p">)</span>
    <span class="n">decoded_activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_decoder_bias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learned_activations</span><span class="p">,</span> <span class="n">decoded_activations</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">initialize_tied_parameters</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize the tied parameters.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">initialize_tied_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the tied parameters.&quot;&quot;&quot;</span>
    <span class="c1"># The tied bias is initialised as the geometric median of the dataset</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tied_bias</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">geometric_median_dataset</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.SparseAutoencoder.reset_parameters" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.SparseAutoencoder.reset_parameters" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Reset the parameters.</p>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/autoencoder/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset the parameters.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">initialize_tied_parameters</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;reset_parameters&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.TensorActivationStore" class="doc doc-heading">
          <code>TensorActivationStore</code>


<a href="#sparse_autoencoder.TensorActivationStore" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.activation_store.base_store.ActivationStore" href="activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore">ActivationStore</a></code></p>

  
      <p>Tensor Activation Store.</p>
<p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation
vectors to be stored to be known in advance. Multiprocess safe.</p>
<p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with
additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p>
<p>Examples:
Create an empty activation dataset:</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)
</code></pre>
<p>Add a single activation vector to the dataset:</p>
<pre><code>&gt;&gt;&gt; store.append(torch.randn(100))
&gt;&gt;&gt; len(store)
1
</code></pre>
<p>Add a [batch, pos, neurons] activation tensor to the dataset:</p>
<pre><code>&gt;&gt;&gt; store.empty()
&gt;&gt;&gt; batch = torch.randn(10, 10, 100)
&gt;&gt;&gt; store.extend(batch)
&gt;&gt;&gt; len(store)
100
</code></pre>
<p>Shuffle the dataset <strong>before passing it to the DataLoader</strong>:</p>
<pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument
&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)
</code></pre>
<p>Use the dataloader to iterate over the dataset:</p>
<pre><code>&gt;&gt;&gt; next_item = next(iter(loader))
&gt;&gt;&gt; next_item.shape
torch.Size([2, 100])
</code></pre>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TensorActivationStore</span><span class="p">(</span><span class="n">ActivationStore</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tensor Activation Store.</span>

<span class="sd">    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation</span>
<span class="sd">    vectors to be stored to be known in advance. Multiprocess safe.</span>

<span class="sd">    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with</span>
<span class="sd">    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).</span>

<span class="sd">    Examples:</span>
<span class="sd">    Create an empty activation dataset:</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)</span>

<span class="sd">    Add a single activation vector to the dataset:</span>

<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        1</span>

<span class="sd">    Add a [batch, pos, neurons] activation tensor to the dataset:</span>

<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; batch = torch.randn(10, 10, 100)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(batch)</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        100</span>

<span class="sd">    Shuffle the dataset **before passing it to the DataLoader**:</span>

<span class="sd">        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument</span>
<span class="sd">        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)</span>

<span class="sd">    Use the dataloader to iterate over the dataset:</span>

<span class="sd">        &gt;&gt;&gt; next_item = next(iter(loader))</span>
<span class="sd">        &gt;&gt;&gt; next_item.shape</span>
<span class="sd">        torch.Size([2, 100])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_data</span><span class="p">:</span> <span class="n">StoreActivations</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Underlying Tensor Data Store.&quot;&quot;&quot;</span>

    <span class="n">items_stored</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number of items stored.&quot;&quot;&quot;</span>

    <span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Maximum Number of Items to Store.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the Tensor Activation Store.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_items: Maximum number of items to store (individual activation vectors)</span>
<span class="sd">            num_neurons: Number of neurons in each activation vector.</span>
<span class="sd">            device: Device to store the activation vectors on.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_items</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span> <span class="o">=</span> <span class="n">max_items</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">        Returns the number of activation vectors in the dataset.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">            &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">            &gt;&gt;&gt; len(store)</span>
<span class="sd">            2</span>

<span class="sd">        Returns:</span>
<span class="sd">            The number of activation vectors in the dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span>

    <span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)</span>
<span class="sd">            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100</span>
<span class="sd">            800</span>

<span class="sd">        Returns:</span>
<span class="sd">            The size of the underlying tensor in bytes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; store[1]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            index: The index of the tensor to fetch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The activation store item at the given index.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If the index is out of range.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check in range</span>
        <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2"> out of range (only </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="si">}</span><span class="s2"> items stored)&quot;</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([0.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">        &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">        &gt;&gt;&gt; [store[i].item() for i in range(3)]</span>
<span class="sd">        [0.0, 2.0, 1.0]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Generate a permutation of the indices for the active data</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">)</span>

        <span class="c1"># Use this permutation to shuffle the active data in-place</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a single item to the store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; store[1]</span>
<span class="sd">        tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">        Args:</span>
<span class="sd">            item: The item to append to the dataset.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If there is no space remaining.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check we have space</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">StoreFullError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a batch to the store.</span>

<span class="sd">        Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        2</span>

<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        9</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The batch to append to the dataset.</span>

<span class="sd">        Raises:</span>
<span class="sd">            IndexError: If there is no space remaining.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reshaped</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span> <span class="o">=</span> <span class="n">resize_to_single_item_dimension</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Check we have space</span>
        <span class="n">num_activation_tensors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Single batch of </span><span class="si">{</span><span class="n">num_activation_tensors</span><span class="si">}</span><span class="s2"> activations is larger than the </span><span class="se">\</span>
<span class="s2">                    total maximum in the store of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

            <span class="k">raise</span> <span class="n">StoreFullError</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span><span class="p">]</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="n">num_activation_tensors</span>

    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empty the store.</span>

<span class="sd">        Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; store.empty()</span>
<span class="sd">        &gt;&gt;&gt; store.items_stored</span>
<span class="sd">        0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We don&#39;t need to zero the data, just reset the number of items stored</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.TensorActivationStore.items_stored" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">items_stored</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.TensorActivationStore.items_stored" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Number of items stored.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="sparse_autoencoder.TensorActivationStore.max_items" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

<a href="#sparse_autoencoder.TensorActivationStore.max_items" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Maximum Number of Items to Store.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__getitem__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="n">index</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__getitem__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Get Item Dunder Method.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=2, num_neurons=5)
store.append(torch.zeros(5))
store.append(torch.ones(5))
store[1]
tensor([1., 1., 1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>index</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The index of the tensor to fetch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The activation store item at the given index.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>IndexError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the index is out of range.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InputOutputActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get Item Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">    &gt;&gt;&gt; store[1]</span>
<span class="sd">    tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        index: The index of the tensor to fetch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The activation store item at the given index.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If the index is out of range.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check in range</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2"> out of range (only </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="si">}</span><span class="s2"> items stored)&quot;</span>
        <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">max_items</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialise the Tensor Activation Store.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>max_items</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum number of items to store (individual activation vectors)</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>num_neurons</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of neurons in each activation vector.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>device</code></td>
          <td>
                <code><span title="torch.device">device</span> | None</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Device to store the activation vectors on.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">max_items</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_neurons</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialise the Tensor Activation Store.</span>

<span class="sd">    Args:</span>
<span class="sd">        max_items: Maximum number of items to store (individual activation vectors)</span>
<span class="sd">        num_neurons: Number of neurons in each activation vector.</span>
<span class="sd">        device: Device to store the activation vectors on.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_items</span><span class="p">,</span> <span class="n">num_neurons</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span> <span class="o">=</span> <span class="n">max_items</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__len__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__len__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__len__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Length Dunder Method.</p>
<p>Returns the number of activation vectors in the dataset.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10_000_000, num_neurons=100)
store.append(torch.randn(100))
store.append(torch.randn(100))
len(store)
2</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of activation vectors in the dataset.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Length Dunder Method.</span>

<span class="sd">    Returns the number of activation vectors in the dataset.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; store.append(torch.randn(100))</span>
<span class="sd">        &gt;&gt;&gt; len(store)</span>
<span class="sd">        2</span>

<span class="sd">    Returns:</span>
<span class="sd">        The number of activation vectors in the dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.__sizeof__" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">__sizeof__</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.__sizeof__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Sizeof Dunder Method.</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=2, num_neurons=100)
store.<strong>sizeof</strong>() # Pre-allocated tensor of 2x100
800</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The size of the underlying tensor in bytes.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sizeof Dunder Method.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)</span>
<span class="sd">        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100</span>
<span class="sd">        800</span>

<span class="sd">    Returns:</span>
<span class="sd">        The size of the underlying tensor in bytes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.append" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.append" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Add a single item to the store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, num_neurons=5)
store.append(torch.zeros(5))
store.append(torch.ones(5))
store[1]
tensor([1., 1., 1., 1., 1.])</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>item</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.InputOutputActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector">InputOutputActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The item to append to the dataset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>IndexError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If there is no space remaining.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">InputOutputActivationVector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a single item to the store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.zeros(5))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.ones(5))</span>
<span class="sd">    &gt;&gt;&gt; store[1]</span>
<span class="sd">    tensor([1., 1., 1., 1., 1.])</span>

<span class="sd">    Args:</span>
<span class="sd">        item: The item to append to the dataset.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If there is no space remaining.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check we have space</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">StoreFullError</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.empty" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">empty</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.empty" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Empty the store.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, num_neurons=5)
store.extend(torch.zeros(2, 5))
store.items_stored
2
store.empty()
store.items_stored
0</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Empty the store.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    2</span>
<span class="sd">    &gt;&gt;&gt; store.empty()</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We don&#39;t need to zero the data, just reset the number of items stored</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.extend" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.extend" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Add a batch to the store.</p>
<p>Examples:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
store = TensorActivationStore(max_items=10, num_neurons=5)
store.extend(torch.zeros(2, 5))
store.items_stored
2</p>
<p>store = TensorActivationStore(max_items=10, num_neurons=5)
store.extend(torch.zeros(3, 3, 5))
store.items_stored
9</p>
</blockquote>
</blockquote>
</blockquote>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.SourceModelActivations" href="tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations">SourceModelActivations</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The batch to append to the dataset.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Raises:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>IndexError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If there is no space remaining.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">SourceModelActivations</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add a batch to the store.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    2</span>

<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)</span>
<span class="sd">    &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))</span>
<span class="sd">    &gt;&gt;&gt; store.items_stored</span>
<span class="sd">    9</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: The batch to append to the dataset.</span>

<span class="sd">    Raises:</span>
<span class="sd">        IndexError: If there is no space remaining.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reshaped</span><span class="p">:</span> <span class="n">InputOutputActivationBatch</span> <span class="o">=</span> <span class="n">resize_to_single_item_dimension</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Check we have space</span>
    <span class="n">num_activation_tensors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Single batch of </span><span class="si">{</span><span class="n">num_activation_tensors</span><span class="si">}</span><span class="s2"> activations is larger than the </span><span class="se">\</span>
<span class="s2">                total maximum in the store of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_items</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="k">raise</span> <span class="n">StoreFullError</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+</span> <span class="n">num_activation_tensors</span><span class="p">]</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span> <span class="o">+=</span> <span class="n">num_activation_tensors</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TensorActivationStore.shuffle" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">shuffle</span><span class="p">()</span></code>

<a href="#sparse_autoencoder.TensorActivationStore.shuffle" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Shuffle the Data In-Place.</p>
<p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p>
<p>Example:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import torch
_seed = torch.manual_seed(42)
store = TensorActivationStore(max_items=10, num_neurons=1)
store.append(torch.tensor([0.]))
store.append(torch.tensor([1.]))
store.append(torch.tensor([2.]))
store.shuffle()
[store[i].item() for i in range(3)]
[0.0, 2.0, 1.0]</p>
</blockquote>
</blockquote>
</blockquote>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">shuffle</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Shuffle the Data In-Place.</span>

<span class="sd">    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.</span>

<span class="sd">    Example:</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; _seed = torch.manual_seed(42)</span>
<span class="sd">    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([0.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([1.]))</span>
<span class="sd">    &gt;&gt;&gt; store.append(torch.tensor([2.]))</span>
<span class="sd">    &gt;&gt;&gt; store.shuffle()</span>
<span class="sd">    &gt;&gt;&gt; [store[i].item() for i in range(3)]</span>
<span class="sd">    [0.0, 2.0, 1.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Generate a permutation of the indices for the active data</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">)</span>

    <span class="c1"># Use this permutation to shuffle the active data in-place</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_stored</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.TextDataset" class="doc doc-heading">
          <code>TextDataset</code>


<a href="#sparse_autoencoder.TextDataset" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.SourceDataset" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset">SourceDataset</a>[<a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch" href="source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch">GenericTextDataBatch</a>]</code></p>

  
      <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p>

            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@final</span>
<span class="k">class</span> <span class="nc">TextDataset</span><span class="p">(</span><span class="n">SourceDataset</span><span class="p">[</span><span class="n">GenericTextDataBatch</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generic Text Dataset for any text-based dataset from Hugging Face.&quot;&quot;&quot;</span>

    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">source_batch</span><span class="p">:</span> <span class="n">GenericTextDataBatch</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</span>

<span class="sd">        Args:</span>
<span class="sd">            source_batch: A batch of source data, including &#39;text&#39; with a list of strings.</span>
<span class="sd">            context_size: Context size for tokenized prompts.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tokenized prompts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>

        <span class="n">tokenized_prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.</span>
        <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]):</span>  <span class="c1"># type: ignore</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
            <span class="p">]</span>
            <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">,</span>
        <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;monology/pile-uncopyrighted&quot;</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize a generic text dataset from Hugging Face.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokenizer: Tokenizer to process text data.</span>
<span class="sd">            context_size: Context size for tokenized prompts.</span>
<span class="sd">            buffer_size: Buffer size for shuffling the dataset.</span>
<span class="sd">            preprocess_batch_size: Batch size for preprocessing.</span>
<span class="sd">            dataset_path: Path to the dataset on Hugging Face.</span>
<span class="sd">            dataset_split: Dataset split (e.g., &#39;train&#39;).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
            <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
            <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
            <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
            <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TextDataset.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">dataset_path</span><span class="o">=</span><span class="s1">&#39;monology/pile-uncopyrighted&#39;</span><span class="p">,</span> <span class="n">dataset_split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TextDataset.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialize a generic text dataset from Hugging Face.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>tokenizer</code></td>
          <td>
                <code><span title="transformers.PreTrainedTokenizerBase">PreTrainedTokenizerBase</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tokenizer to process text data.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Context size for tokenized prompts.</p>
            </div>
          </td>
          <td>
                <code>256</code>
          </td>
        </tr>
        <tr>
          <td><code>buffer_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Buffer size for shuffling the dataset.</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>preprocess_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Batch size for preprocessing.</p>
            </div>
          </td>
          <td>
                <code>1000</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_path</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Path to the dataset on Hugging Face.</p>
            </div>
          </td>
          <td>
                <code>&#39;monology/pile-uncopyrighted&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>dataset_split</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dataset split (e.g., 'train').</p>
            </div>
          </td>
          <td>
                <code>&#39;train&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">preprocess_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">dataset_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;monology/pile-uncopyrighted&quot;</span><span class="p">,</span>
    <span class="n">dataset_split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize a generic text dataset from Hugging Face.</span>

<span class="sd">    Args:</span>
<span class="sd">        tokenizer: Tokenizer to process text data.</span>
<span class="sd">        context_size: Context size for tokenized prompts.</span>
<span class="sd">        buffer_size: Buffer size for shuffling the dataset.</span>
<span class="sd">        preprocess_batch_size: Batch size for preprocessing.</span>
<span class="sd">        dataset_path: Path to the dataset on Hugging Face.</span>
<span class="sd">        dataset_split: Dataset split (e.g., &#39;train&#39;).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">dataset_path</span><span class="o">=</span><span class="n">dataset_path</span><span class="p">,</span>
        <span class="n">dataset_split</span><span class="o">=</span><span class="n">dataset_split</span><span class="p">,</span>
        <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
        <span class="n">preprocess_batch_size</span><span class="o">=</span><span class="n">preprocess_batch_size</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TextDataset.preprocess" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">preprocess</span><span class="p">(</span><span class="n">source_batch</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">context_size</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TextDataset.preprocess" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Preprocess a batch of prompts.</p>
<p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>source_batch</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch" href="source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch">GenericTextDataBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A batch of source data, including 'text' with a list of strings.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>context_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Context size for tokenized prompts.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts" href="source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts">TokenizedPrompts</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Tokenized prompts.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">source_batch</span><span class="p">:</span> <span class="n">GenericTextDataBatch</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenizedPrompts</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preprocess a batch of prompts.</span>

<span class="sd">    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_batch: A batch of source data, including &#39;text&#39; with a list of strings.</span>
<span class="sd">        context_size: Context size for tokenized prompts.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tokenized prompts.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">source_batch</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>

    <span class="n">tokenized_prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.</span>
    <span class="n">context_size_prompts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">encoding</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenized_prompts</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]):</span>  <span class="c1"># type: ignore</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">),</span> <span class="n">context_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">context_size</span><span class="p">])</span> <span class="o">==</span> <span class="n">context_size</span>
        <span class="p">]</span>
        <span class="n">context_size_prompts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">context_size_prompts</span><span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="sparse_autoencoder.TrainBatchFeatureDensityMetric" class="doc doc-heading">
          <code>TrainBatchFeatureDensityMetric</code>


<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric" class="headerlink" title="Permanent link">¤</a></h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric" href="metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric">AbstractTrainMetric</a></code></p>

  
      <p>Train batch feature density.</p>
<p>Percentage of samples in which each feature was active (i.e. the neuron has "fired"), in a
training batch.</p>
<p>Generally we want a small number of features to be active in each batch, so average feature
density should be low. By contrast if the average feature density is high, it means that the
features are not sparse enough.</p>

<details class="warning" open>
  <summary>Warning</summary>
  <p>This is not the same as the feature density of the entire training set. It's main use is
tracking the progress of training.</p>
</details>
            <details class="quote">
              <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TrainBatchFeatureDensityMetric</span><span class="p">(</span><span class="n">AbstractTrainMetric</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train batch feature density.</span>

<span class="sd">    Percentage of samples in which each feature was active (i.e. the neuron has &quot;fired&quot;), in a</span>
<span class="sd">    training batch.</span>

<span class="sd">    Generally we want a small number of features to be active in each batch, so average feature</span>
<span class="sd">    density should be low. By contrast if the average feature density is high, it means that the</span>
<span class="sd">    features are not sparse enough.</span>

<span class="sd">    Warning:</span>
<span class="sd">        This is not the same as the feature density of the entire training set. It&#39;s main use is</span>
<span class="sd">        tracking the progress of training.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialise the train batch feature density metric.</span>

<span class="sd">        Args:</span>
<span class="sd">            threshold: Threshold for considering a feature active (i.e. the neuron has &quot;fired&quot;).</span>
<span class="sd">                This should be close to zero.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>

    <span class="k">def</span> <span class="nf">feature_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntActivationVector</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Count how many times each feature was active.</span>

<span class="sd">        Percentage of samples in which each feature was active (i.e. the neuron has &quot;fired&quot;).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])</span>
<span class="sd">            &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()</span>
<span class="sd">            [1.0, 0.5, 0.0]</span>

<span class="sd">        Args:</span>
<span class="sd">            activations: Sample of cached activations (the Autoencoder&#39;s learned features).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number of times each feature was active in a sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">has_fired</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span>  <span class="c1"># Move to float so it can be averaged</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">einops</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">has_fired</span><span class="p">,</span> <span class="s2">&quot;sample activation -&gt; activation&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">wandb_feature_density_histogram</span><span class="p">(</span>
        <span class="n">feature_density</span><span class="p">:</span> <span class="n">LearntActivationVector</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the feature density.</span>

<span class="sd">        This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;feature_density_histogram&quot;:</span>
<span class="sd">        wandb_feature_density_histogram(feature_density)})`.</span>

<span class="sd">        Args:</span>
<span class="sd">            feature_density: Number of times each feature was active in a sample. Can be calculated</span>
<span class="sd">                using :func:`feature_activity_count`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">numpy_feature_density</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float_</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_density</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">bins</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">numpy_feature_density</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate the train batch feature density metrics.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Train metric data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary with the train batch feature density metric, and a histogram of the feature</span>
<span class="sd">            density.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_batch_feature_density</span><span class="p">:</span> <span class="n">LearntActivationVector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_density</span><span class="p">(</span>
            <span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span>
        <span class="p">)</span>

        <span class="n">train_batch_feature_density_histogram</span><span class="p">:</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wandb_feature_density_histogram</span><span class="p">(</span><span class="n">train_batch_feature_density</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;train_batch_feature_density_histogram&quot;</span><span class="p">:</span> <span class="n">train_batch_feature_density_histogram</span><span class="p">,</span>
        <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Initialise the train batch feature density metric.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>threshold</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Threshold for considering a feature active (i.e. the neuron has "fired").
This should be close to zero.</p>
            </div>
          </td>
          <td>
                <code>0.0</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialise the train batch feature density metric.</span>

<span class="sd">    Args:</span>
<span class="sd">        threshold: Threshold for considering a feature active (i.e. the neuron has &quot;fired&quot;).</span>
<span class="sd">            This should be close to zero.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">calculate</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Calculate the train batch feature density metrics.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.metrics.train.abstract_train_metric.TrainMetricData" href="metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.TrainMetricData">TrainMetricData</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Train metric data.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>dict[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dictionary with the train batch feature density metric, and a histogram of the feature</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
                <code>dict[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>density.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">calculate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TrainMetricData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the train batch feature density metrics.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: Train metric data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dictionary with the train batch feature density metric, and a histogram of the feature</span>
<span class="sd">        density.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_batch_feature_density</span><span class="p">:</span> <span class="n">LearntActivationVector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_density</span><span class="p">(</span>
        <span class="n">data</span><span class="o">.</span><span class="n">learned_activations</span>
    <span class="p">)</span>

    <span class="n">train_batch_feature_density_histogram</span><span class="p">:</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wandb_feature_density_histogram</span><span class="p">(</span><span class="n">train_batch_feature_density</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;train_batch_feature_density_histogram&quot;</span><span class="p">:</span> <span class="n">train_batch_feature_density_histogram</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">feature_density</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span></code>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Count how many times each feature was active.</p>
<p>Percentage of samples in which each feature was active (i.e. the neuron has "fired").</p>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>import torch
activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])
TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()
[1.0, 0.5, 0.0]</p>
</blockquote>
</blockquote>
</blockquote>
</details>


  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>activations</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearnedActivationBatch" href="tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch">LearnedActivationBatch</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sample of cached activations (the Autoencoder's learned features).</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearntActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.LearntActivationVector">LearntActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each feature was active in a sample.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">feature_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activations</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearntActivationVector</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Count how many times each feature was active.</span>

<span class="sd">    Percentage of samples in which each feature was active (i.e. the neuron has &quot;fired&quot;).</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])</span>
<span class="sd">        &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()</span>
<span class="sd">        [1.0, 0.5, 0.0]</span>

<span class="sd">    Args:</span>
<span class="sd">        activations: Sample of cached activations (the Autoencoder&#39;s learned features).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Number of times each feature was active in a sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">has_fired</span><span class="p">:</span> <span class="n">LearnedActivationBatch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span>  <span class="c1"># Move to float so it can be averaged</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">einops</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">has_fired</span><span class="p">,</span> <span class="s2">&quot;sample activation -&gt; activation&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">wandb_feature_density_histogram</span><span class="p">(</span><span class="n">feature_density</span><span class="p">)</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram" class="headerlink" title="Permanent link">¤</a></h3>


  <div class="doc doc-contents ">
  
      <p>Create a W&amp;B histogram of the feature density.</p>
<p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({"feature_density_histogram":
wandb_feature_density_histogram(feature_density)})</code>.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>feature_density</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="sparse_autoencoder.tensor_types.LearntActivationVector" href="tensor_types/#sparse_autoencoder.tensor_types.LearntActivationVector">LearntActivationVector</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times each feature was active in a sample. Can be calculated
using :func:<code>feature_activity_count</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="wandb.Histogram">Histogram</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">wandb_feature_density_histogram</span><span class="p">(</span>
    <span class="n">feature_density</span><span class="p">:</span> <span class="n">LearntActivationVector</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a W&amp;B histogram of the feature density.</span>

<span class="sd">    This can be logged with Weights &amp; Biases using e.g. `wandb.log({&quot;feature_density_histogram&quot;:</span>
<span class="sd">    wandb_feature_density_histogram(feature_density)})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_density: Number of times each feature was active in a sample. Can be calculated</span>
<span class="sd">            using :func:`feature_activity_count`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Weights &amp; Biases histogram for logging with `wandb.log`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">numpy_feature_density</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float_</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_density</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">bins</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">histogram</span><span class="p">(</span><span class="n">numpy_feature_density</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">np_histogram</span><span class="o">=</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">values</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.action.edit"], "search": "../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.cd18aaf1.min.js"></script>
      
        <script src="../javascript/custom_formatting.js"></script>
      
        <script src="../javascript/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>