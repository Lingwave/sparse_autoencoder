{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sparse Autoencoder","text":"<p>A sparse autoencoder for mechanistic interpretability research.</p> <pre><code>pip install sparse_autoencoder\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Check out the demo notebook for a guide to using this library.</p> <p>We also highly recommend skimming the reference docs to see all the features that are available.</p>"},{"location":"#features","title":"Features","text":"<p>This library contains:</p> <ol> <li>A sparse autoencoder model, along with all the underlying PyTorch components you need to       customise and/or build your own:<ul> <li>Encoder, constrained unit norm decoder and tied bias PyTorch modules in     sparse_autoencoder.autoencoder.</li> <li>L1 and L2 loss modules in sparse_autoencoder.loss.</li> <li>Adam module with helper method to reset state in sparse_autoencoder.optimizer.</li> </ul> </li> <li>Activations data generator using TransformerLens, with the underlying steps in case you       want to customise the approach:<ul> <li>Activation store options (in-memory or on disk) in sparse_autoencoder.activation_store.</li> <li>Hook to get the activations from TransformerLens in an efficient way in     sparse_autoencoder.source_model.</li> <li>Source dataset (i.e. prompts to generate these activations) utils in     sparse_autoencoder.source_data, that stream data from HuggingFace and pre-process     (tokenize &amp; shuffle).</li> </ul> </li> <li>Activation resampler to help reduce the number of dead neurons.</li> <li>Metrics that log at various stages of training (e.g. during training, resampling and       validation), and integrate with wandb.</li> <li>Training pipeline that combines everything together, allowing you to run hyperparameter       sweeps and view progress on wandb.</li> </ol>"},{"location":"#designed-for-research","title":"Designed for Research","text":"<p>The library is designed to be modular. By default it takes the approach from Towards Monosemanticity: Decomposing Language Models With Dictionary Learning , so you can pip install the library and get started quickly. Then when you need to customise something, you can just extend the abstract class for that component (e.g. you can extend <code>AbstractEncoder</code> if you want to customise the encoder layer, and then easily drop it in the standard <code>SparseAutoencoder</code> model to keep everything else as is. Every component is fully documented, so it's nice and easy to do this.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Demo</li> <li>Reference</li> <li>Contributing</li> <li>Citation</li> </ul>"},{"location":"citation/","title":"Citation","text":"<p>Please cite this library as:</p> <pre><code>@misc{cooney2023SparseAutoencoder,\n    title = {Sparse Autoencoder Library},\n    author = {Alan Cooney},\n    year = {2023},\n    howpublished = {\\url{https://github.com/ai-safety-foundation/sparse_autoencoder}},\n}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#setup","title":"Setup","text":"<p>This project uses Poetry for dependency management, and PoeThePoet for scripts. After checking out the repo, we recommend setting poetry's config to create the <code>.venv</code> in the root directory (note this is a global setting) and then installing with the dev and demos dependencies.</p> <pre><code>poetry config virtualenvs.in-project true\npoetry install --with dev,demos\n</code></pre> <p>If you are using VSCode we highly recommend installing the recommended extensions as well (it will prompt you to do this when you checkout the repo).</p>"},{"location":"contributing/#checks","title":"Checks","text":"<p>For a full list of available commands (e.g. <code>test</code> or <code>typecheck</code>), run this in your terminal (assumes the venv is active already).</p> <pre><code>poe\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Please make sure to add thorough documentation for any features you add. You should do this directly in the docstring, and this will then automatically generate the API docs when merged into <code>main</code>. They will also be automatically checked with pytest (via doctest).</p> <p>If you want to view your documentation changes, run <code>poe docs-hot-reload</code>. This will give you hot-reloading docs (they change in real time as you edit docstrings).</p>"},{"location":"contributing/#docstring-style-guide","title":"Docstring Style Guide","text":"<p>We follow the Google Python Docstring Style for writing docstrings. Some important details below:</p>"},{"location":"contributing/#sections-and-order","title":"Sections and Order","text":"<p>You should follow this order:</p> <pre><code>\"\"\"Title In Title Case.\n\nA description of what the function/class does, including as much detail as is necessary to fully understand it.\n\nWarning:\n\nAny warnings to the user (e.g. common pitfalls).\n\nExamples:\n\nInclude any examples here. They will be checked with doctest.\n\n  &gt;&gt;&gt; print(1 + 2)\n  3\n\nArgs:\n    param_without_type_signature:\n        Each description should be indented once more.\n    param_2:\n        Another example parameter.\n\nReturns:\n    Returns description without type signature.\n\nRaises:\n    Information about the error it may raise (if any).\n\"\"\"\n</code></pre>"},{"location":"contributing/#latex-support","title":"LaTeX support","text":"<p>You can use LaTeX, inside <code>$$</code> for blocks or <code>$</code> for inline</p> <pre><code>Some text $(a + b)^2 = a^2 + 2ab + b^2$\n</code></pre> <pre><code>Some text:\n\n$$\ny    &amp; = &amp; ax^2 + bx + c \\\\\nf(x) &amp; = &amp; x^2 + 2xy + y^2\n$$\n</code></pre>"},{"location":"contributing/#markup","title":"Markup","text":"<ul> <li>Italics - <code>*text*</code></li> <li>Bold - <code>**text**</code></li> <li>Code - <code>``code``</code></li> <li>List items - <code>*item</code></li> <li>Numbered items - <code>1. Item</code></li> <li>Quotes - indent one level</li> <li>External links = <code>`Link text &lt;https://domain.invalid/&gt;`</code></li> </ul>"},{"location":"demo/","title":"Demo","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n\nimport os\nfrom pathlib import Path\n\nimport torch\nfrom transformer_lens import HookedTransformer\nfrom transformer_lens.utils import get_device\nfrom transformers import PreTrainedTokenizerBase\nimport wandb\n\nfrom sparse_autoencoder import (\n    ActivationResampler,\n    AdamWithReset,\n    L2ReconstructionLoss,\n    LearnedActivationsL1Loss,\n    LossReducer,\n    Pipeline,\n    PreTokenizedDataset,\n    SparseAutoencoder,\n)\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndevice = get_device()\nprint(f\"Using device: {device}\")  # You will need a GPU\n</code></pre> <pre>\n<code>Using device: mps\n</code>\n</pre> <p>The way this library works is that you can define your own hyper-parameters and then setup the underlying components with them. This is extremely flexible, but to help you get started we've included some common ones below along with some sensible defaults. You can also easily sweep through multiple hyperparameters with <code>wandb.sweep</code>.</p> <pre><code>torch.random.manual_seed(49)\n\nhyperparameters = {\n    # Expansion factor is the number of features in the sparse representation, relative to the\n    # number of features in the original MLP layer. The original paper experimented with 1x to 256x,\n    # and we have found that 4x is a good starting point.\n    \"expansion_factor\": 4,\n    # L1 coefficient is the coefficient of the L1 regularization term (used to encourage sparsity).\n    \"l1_coefficient\": 1e-3,\n    # Adam parameters (set to the default ones here)\n    \"lr\": 3e-4,\n    \"adam_beta_1\": 0.9,\n    \"adam_beta_2\": 0.999,\n    \"adam_epsilon\": 1e-8,\n    \"adam_weight_decay\": 0.0,\n    # Batch sizes\n    \"train_batch_size\": 4096,\n    \"context_size\": 128,\n    # Source model hook point\n    \"source_model_name\": \"gelu-2l\",\n    \"source_model_dtype\": \"float32\",\n    \"source_model_hook_point\": \"blocks.0.hook_mlp_out\",\n    \"source_model_hook_point_layer\": 0,\n    # Train pipeline parameters\n    \"max_store_size\": 384 * 4096 * 2,\n    \"max_activations\": 2_000_000_000,\n    \"resample_frequency\": 122_880_000,\n    \"checkpoint_frequency\": 100_000_000,\n    \"validation_frequency\": 384 * 4096 * 2 * 100,  # Every 100 generations\n}\n</code></pre> <p>The source model is just a TransformerLens model (see here for a full list of supported models).</p> <p>In this example we're training a sparse autoencoder on the activations from the first MLP layer, so we'll also get some details about that hook point.</p> <pre><code># Source model setup with TransformerLens\nsrc_model = HookedTransformer.from_pretrained(\n    str(hyperparameters[\"source_model_name\"]), dtype=str(hyperparameters[\"source_model_dtype\"])\n)\n\n# Details about the activations we'll train the sparse autoencoder on\nautoencoder_input_dim: int = src_model.cfg.d_model  # type: ignore (TransformerLens typing is currently broken)\n\nf\"Source: {hyperparameters['source_model_name']}, \\\n    Hook: {hyperparameters['source_model_hook_point']}, \\\n    Features: {autoencoder_input_dim}\"\n</code></pre> <pre>\n<code>Loaded pretrained model gelu-2l into HookedTransformer\n</code>\n</pre> <pre>\n<code>'Source: gelu-2l, Hook: blocks.0.hook_mlp_out,     Features: 512'</code>\n</pre> <p>We can then setup the sparse autoencoder. The default model (<code>SparseAutoencoder</code>) is setup as per the original Anthropic paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning .</p> <p>However it's just a standard PyTorch model, so you can create your own model instead if you want to use a different architecture. To do this you just need to extend the <code>AbstractAutoencoder</code>, and optionally the underlying <code>AbstractEncoder</code>, <code>AbstractDecoder</code> and <code>AbstractOuterBias</code>. See these classes (which are fully documented) for more details.</p> <pre><code>expansion_factor = hyperparameters[\"expansion_factor\"]\nautoencoder = SparseAutoencoder(\n    n_input_features=autoencoder_input_dim,  # size of the activations we are autoencoding\n    n_learned_features=int(autoencoder_input_dim * expansion_factor),  # size of SAE\n    geometric_median_dataset=torch.zeros(\n        autoencoder_input_dim\n    ),  # this is used to initialize the tied bias\n).to(device)\nautoencoder  # Print the model (it's pretty straightforward)\n</code></pre> <pre>\n<code>SparseAutoencoder(\n  (_pre_encoder_bias): TiedBias(position=pre_encoder)\n  (_encoder): LinearEncoder(\n    in_features=512, out_features=2048\n    (activation_function): ReLU()\n  )\n  (_decoder): UnitNormDecoder(in_features=2048, out_features=512)\n  (_post_decoder_bias): TiedBias(position=post_decoder)\n)</code>\n</pre> <p>We'll also want to setup an Optimizer and Loss function. In this case we'll also use the standard approach from the original Anthropic paper. However you can create your own loss functions and optimizers by extending <code>AbstractLoss</code> and <code>AbstractOptimizerWithReset</code> respectively.</p> <pre><code># We use a loss reducer, which simply adds up the losses from the underlying loss functions.\nloss = LossReducer(\n    LearnedActivationsL1Loss(\n        l1_coefficient=float(hyperparameters[\"l1_coefficient\"]),\n    ),\n    L2ReconstructionLoss(),\n)\nloss\n</code></pre> <pre>\n<code>LossReducer(\n  (0): LearnedActivationsL1Loss(l1_coefficient=0.001)\n  (1): L2ReconstructionLoss()\n)</code>\n</pre> <pre><code>optimizer = AdamWithReset(\n    params=autoencoder.parameters(),\n    named_parameters=autoencoder.named_parameters(),\n    lr=float(hyperparameters[\"lr\"]),\n    betas=(float(hyperparameters[\"adam_beta_1\"]), float(hyperparameters[\"adam_beta_2\"])),\n    eps=float(hyperparameters[\"adam_epsilon\"]),\n    weight_decay=float(hyperparameters[\"adam_weight_decay\"]),\n)\noptimizer\n</code></pre> <pre>\n<code>AdamWithReset (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0.0\n)</code>\n</pre> <p>Finally we'll initialise an activation resampler.</p> <pre><code>activation_resampler = ActivationResampler()\n</code></pre> <p>This is just a dataset of tokenized prompts, to be used in generating activations (which are in turn used to train the SAE).</p> <pre><code>tokenizer: PreTrainedTokenizerBase = src_model.tokenizer  # type: ignore\nsource_data = PreTokenizedDataset(\n    dataset_path=\"NeelNanda/c4-code-tokenized-2b\", context_size=int(hyperparameters[\"context_size\"])\n)\n</code></pre> <pre>\n<code>Resolving data files:   0%|          | 0/28 [00:00&lt;?, ?it/s]</code>\n</pre> <p>If you initialise wandb, the pipeline will automatically log all metrics to wandb. However, we should pass in a dictionary with all of our hyperaparameters so they're on  wandb. </p> <p>We strongly encourage users to make use of wandb in order to understand the training process.</p> <pre><code>checkpoint_path = Path(\"../../.checkpoints\")\ncheckpoint_path.mkdir(exist_ok=True)\n</code></pre> <pre><code>Path(\".cache/\").mkdir(exist_ok=True)\nwandb.init(\n    project=\"sparse-autoencoder\",\n    dir=\".cache\",\n    config=hyperparameters,\n)\n</code></pre> <pre>\n<code>Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: alan-cooney. Use `wandb login --relogin` to force relogin\n</code>\n</pre>  Tracking run with wandb version 0.16.0   Run data is saved locally in <code>.cache/wandb/run-20231126_184500-2fnpg8zi</code>  Syncing run prime-star-105 to Weights &amp; Biases (docs)   View project at https://wandb.ai/alan-cooney/sparse-autoencoder   View run at https://wandb.ai/alan-cooney/sparse-autoencoder/runs/2fnpg8zi Display W&amp;B run <pre><code>pipeline = Pipeline(\n    activation_resampler=activation_resampler,\n    autoencoder=autoencoder,\n    cache_name=str(hyperparameters[\"source_model_hook_point\"]),\n    checkpoint_directory=checkpoint_path,\n    layer=int(hyperparameters[\"source_model_hook_point_layer\"]),\n    loss=loss,\n    optimizer=optimizer,\n    source_data_batch_size=6,\n    source_dataset=source_data,\n    source_model=src_model,\n)\n\npipeline.run_pipeline(\n    train_batch_size=int(hyperparameters[\"train_batch_size\"]),\n    max_store_size=int(hyperparameters[\"max_store_size\"]),\n    max_activations=int(hyperparameters[\"max_activations\"]),\n    resample_frequency=int(hyperparameters[\"resample_frequency\"]),\n    checkpoint_frequency=int(hyperparameters[\"checkpoint_frequency\"]),\n    validate_frequency=int(hyperparameters[\"validation_frequency\"]),\n)\n</code></pre> <pre>\n<code>Activations trained on:   0%|          | 0/2000000000 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>/Users/alan/Documents/Repos/sparse_autoencoder/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n</code>\n</pre> <pre><code>wandb.finish()\n</code></pre>"},{"location":"demo/#sparse-autoencoder-training-demo","title":"Sparse Autoencoder Training Demo","text":"<p>This demo trains a sparse autoencoder on activations from a Tiny Stories 1M model.</p> <p>To do this we setup a source model (the TinyStories model) that we want to generate activations from, along with a source dataset of prompts to help generate these activations.</p> <p>We also setup a sparse autoencoder model which we'll train on these generated activations, to learn a sparse representation of them in higher dimensional space.</p> <p>Finally we'll wrap this all together in a pipeline, which alternates between generating activations (storing them in ram), and training the SAE on said activations.</p>"},{"location":"demo/#setup","title":"Setup","text":""},{"location":"demo/#imports","title":"Imports","text":""},{"location":"demo/#hyperparameters","title":"Hyperparameters","text":""},{"location":"demo/#source-model","title":"Source Model","text":""},{"location":"demo/#sparse-autoencoder","title":"Sparse Autoencoder","text":""},{"location":"demo/#source-dataset","title":"Source dataset","text":""},{"location":"demo/#training","title":"Training","text":""},{"location":"reference/","title":"Sparse Autoencoder Library","text":"<p>Sparse Autoencoder Library.</p>"},{"location":"reference/#sparse_autoencoder.LossLogType","title":"<code>LossLogType: TypeAlias = dict[str, int | float | str]</code>  <code>module-attribute</code>","text":"<p>Loss log dict.</p>"},{"location":"reference/#sparse_autoencoder.ActivationResampler","title":"<code>ActivationResampler</code>","text":"<p>             Bases: <code>AbstractActivationResampler</code></p> <p>Activation resampler.</p> <p>Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.</p> <p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone.</p> <p>This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network.</p> Warning <p>The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases.</p> <p>Note this approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>class ActivationResampler(AbstractActivationResampler):\n    \"\"\"Activation resampler.\n\n    Over the course of training, a subset of autoencoder neurons will have zero activity across\n    a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language\n    Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training\n    improves the number of likely-interpretable features (i.e., those in the high density cluster)\n    and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and\n    increase the number of chances the network has to find promising feature directions.\n\n    An interesting nuance around dead neurons involves the ultralow density cluster. They found that\n    if we increase the number of training steps then networks will kill off more of these ultralow\n    density neurons. This reinforces the use of the high density cluster as a useful metric because\n    there can exist neurons that are de facto dead but will not appear to be when looking at the\n    number of dead neurons alone.\n\n    This approach is designed to seed new features to fit inputs where the current autoencoder\n    performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled\n    neuron will only fire weakly for inputs similar to the one used for its reinitialization. This\n    was done to minimize interference with the rest of the network.\n\n    Warning:\n        The optimizer should be reset after applying this function, as the Adam state will be\n        incorrect for the modified weights and biases.\n\n        Note this approach is also known to create sudden loss spikes, and resampling too frequently\n        causes training to diverge.\n    \"\"\"\n\n    @staticmethod\n    def get_dead_neuron_indices(\n        neuron_activity_sample_size: int,\n        neuron_activity: NeuronActivity,\n        threshold: float = 0,\n    ) -&gt; LearntNeuronIndices:\n        \"\"\"Identify the indices of neurons that have zero activity.\n\n        Example:\n            &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 1])\n            &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(\n            ...     neuron_activity_sample_size=10,\n            ...     neuron_activity=neuron_activity,\n            ...     threshold=0.2\n            ... )\n            &gt;&gt;&gt; dead_neuron_indices.tolist()\n            [0, 1, 4]\n\n        Args:\n            neuron_activity_sample_size: Sample size for resampling.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            threshold: Threshold for determining if a neuron is dead (has fired less than this\n                portion of the sample size).\n\n        Returns:\n            A tensor containing the indices of neurons that are 'dead' (zero activity).\n        \"\"\"\n        threshold_activity: int = int(neuron_activity_sample_size * threshold)\n        return torch.where(neuron_activity &lt;= threshold_activity)[0]\n\n    def compute_loss_and_get_activations(\n        self,\n        store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; tuple[TrainBatchStatistic, InputOutputActivationBatch]:\n        \"\"\"Compute the loss on a random subset of inputs.\n\n        Computes the loss and also stores the input activations (for use in resampling neurons).\n\n        Args:\n            store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            A tuple containing the loss per item, and all input activations.\n\n        Raises:\n            ValueError: If the number of items in the store is less than the number of inputs\n        \"\"\"\n        with torch.no_grad():\n            loss_batches: list[TrainBatchStatistic] = []\n            input_activations_batches: list[InputOutputActivationBatch] = []\n            dataloader = DataLoader(store, batch_size=train_batch_size)\n            num_inputs = self._resample_dataset_size or len(store)\n            batches: int = num_inputs // train_batch_size\n            model_device: torch.device = get_model_device(autoencoder)\n\n            for batch_idx, batch in enumerate(iter(dataloader)):\n                input_activations_batches.append(batch)\n                source_activations = batch.to(model_device)\n                learned_activations, reconstructed_activations = autoencoder(source_activations)\n                loss_batches.append(\n                    loss_fn.forward(\n                        source_activations, learned_activations, reconstructed_activations\n                    )\n                )\n                if batch_idx &gt;= batches:\n                    break\n\n            loss_result = torch.cat(loss_batches).to(model_device)\n            input_activations = torch.cat(input_activations_batches).to(model_device)\n\n            # Check we generated enough data\n            if len(loss_result) &lt; num_inputs:\n                error_message = (\n                    f\"Cannot get {num_inputs} items from the store, \"\n                    f\"as only {len(loss_result)} were available.\"\n                )\n                raise ValueError(error_message)\n\n            return loss_result, input_activations\n\n    @staticmethod\n    def assign_sampling_probabilities(loss: TrainBatchStatistic) -&gt; Tensor:\n        \"\"\"Assign the sampling probabilities for each input activations vector.\n\n        Assign each input vector a probability of being picked that is proportional to the square of\n        the autoencoder's loss on that input.\n\n        Example:\n            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n            tensor([0.1000, 0.3000, 0.6000])\n\n        Args:\n            loss: Loss per item.\n\n        Returns:\n            A tensor of probabilities for each item.\n        \"\"\"\n        square_loss = loss.pow(2)\n        return square_loss / square_loss.sum()\n\n    @staticmethod\n    def sample_input(\n        probabilities: TrainBatchStatistic,\n        input_activations: InputOutputActivationBatch,\n        num_samples: int,\n    ) -&gt; SampledDeadNeuronInputs:\n        \"\"\"Sample an input vector based on the provided probabilities.\n\n        Example:\n            &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n            &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n            ...     probabilities, input_activations, 2\n            ... )\n            &gt;&gt;&gt; sampled_input.tolist()\n            [[5.0, 6.0], [3.0, 4.0]]\n\n        Args:\n            probabilities: Probabilities for each input.\n            input_activations: Input activation vectors.\n            num_samples: Number of samples to take (number of dead neurons).\n\n        Returns:\n            Sampled input activation vector.\n\n        Raises:\n            ValueError: If the number of samples is greater than the number of input activations.\n        \"\"\"\n        if num_samples &gt; len(input_activations):\n            exception_message = (\n                f\"Cannot sample {num_samples} inputs from \"\n                f\"{len(input_activations)} input activations.\"\n            )\n            raise ValueError(exception_message)\n\n        if num_samples == 0:\n            return torch.empty(\n                (0, input_activations.shape[-1]),\n                dtype=input_activations.dtype,\n                device=input_activations.device,\n            ).to(input_activations.device)\n\n        sample_indices: LearntNeuronIndices = torch.multinomial(\n            probabilities, num_samples=num_samples\n        )\n        return input_activations[sample_indices, :]\n\n    @staticmethod\n    def renormalize_and_scale(\n        sampled_input: SampledDeadNeuronInputs,\n        neuron_activity: NeuronActivity,\n        encoder_weight: EncoderWeights,\n    ) -&gt; DeadEncoderNeuronWeightUpdates:\n        \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n        Renormalize the input vector to equal the average norm of the encoder weights for alive\n        neurons times 0.2.\n\n        Example:\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n            &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n            &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))\n            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n            ...     sampled_input,\n            ...     neuron_activity,\n            ...     encoder_weight\n            ... )\n            &gt;&gt;&gt; rescaled_input.round(decimals=1)\n            tensor([[0.2000, 0.2000]])\n\n        Args:\n            sampled_input: Tensor of the sampled input activation.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            encoder_weight: Tensor of encoder weights.\n\n        Returns:\n            Rescaled sampled input.\n\n        Raises:\n            ValueError: If there are no alive neurons.\n        \"\"\"\n        alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n        # Check there is at least one alive neuron\n        if not torch.any(alive_neuron_mask):\n            error_message = \"No alive neurons found.\"\n            raise ValueError(error_message)\n\n        # Handle all alive neurons\n        if torch.all(alive_neuron_mask):\n            return torch.empty(\n                (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n            )\n\n        # Calculate the average norm of the encoder weights for alive neurons.\n        alive_encoder_weights: AliveEncoderWeights = encoder_weight[alive_neuron_mask, :]\n        average_alive_norm: ItemTensor = alive_encoder_weights.norm(dim=-1).mean()\n\n        # Renormalize the input vector to equal the average norm of the encoder weights for alive\n        # neurons times 0.2.\n        renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n            sampled_input, dim=-1\n        )\n        return renormalized_input * (average_alive_norm * 0.2)\n\n    def resample_dead_neurons(\n        self,\n        activation_store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        neuron_activity_sample_size: int,\n        neuron_activity: NeuronActivity,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            neuron_activity_sample_size: Sample size for resampling.\n            neuron_activity: Number of times each neuron fired.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n        \"\"\"\n        with torch.no_grad():\n            dead_neuron_indices = self.get_dead_neuron_indices(\n                neuron_activity=neuron_activity,\n                neuron_activity_sample_size=neuron_activity_sample_size,\n            )\n\n            # Compute the loss for the current model on a random subset of inputs and get the\n            # activations.\n            loss, input_activations = self.compute_loss_and_get_activations(\n                store=activation_store,\n                autoencoder=autoencoder,\n                loss_fn=loss_fn,\n                train_batch_size=train_batch_size,\n            )\n\n            # Assign each input vector a probability of being picked that is proportional to the\n            # square of the autoencoder's loss on that input.\n            sample_probabilities: TrainBatchStatistic = self.assign_sampling_probabilities(loss)\n\n            # Get references to the encoder and decoder parameters\n            encoder_weight: EncoderWeights = autoencoder.encoder.weight\n\n            # For each dead neuron sample an input according to these probabilities.\n            sampled_input: SampledDeadNeuronInputs = self.sample_input(\n                sample_probabilities, input_activations, len(dead_neuron_indices)\n            )\n\n            # Renormalize the input vector to have unit L2 norm and set this to be the dictionary\n            # vector for the dead autoencoder neuron.\n            renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n                sampled_input, dim=-1\n            )\n\n            dead_decoder_weight_updates = rearrange(\n                renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n            )\n\n            # For the corresponding encoder vector, renormalize the input vector to equal the\n            # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n            # encoder bias element to zero.\n            rescaled_sampled_input = self.renormalize_and_scale(\n                sampled_input, neuron_activity, encoder_weight\n            )\n            dead_encoder_bias_updates = torch.zeros_like(\n                dead_neuron_indices,\n                dtype=dead_decoder_weight_updates.dtype,\n                device=dead_decoder_weight_updates.device,\n            )\n\n            return ParameterUpdateResults(\n                dead_neuron_indices=dead_neuron_indices,\n                dead_encoder_weight_updates=rescaled_sampled_input,\n                dead_encoder_bias_updates=dead_encoder_bias_updates,\n                dead_decoder_weight_updates=dead_decoder_weight_updates,\n            )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.assign_sampling_probabilities","title":"<code>assign_sampling_probabilities(loss)</code>  <code>staticmethod</code>","text":"<p>Assign the sampling probabilities for each input activations vector.</p> <p>Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input.</p> Example <p>loss = torch.tensor([1.0, 2.0, 3.0]) ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000])</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>TrainBatchStatistic</code> <p>Loss per item.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of probabilities for each item.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef assign_sampling_probabilities(loss: TrainBatchStatistic) -&gt; Tensor:\n    \"\"\"Assign the sampling probabilities for each input activations vector.\n\n    Assign each input vector a probability of being picked that is proportional to the square of\n    the autoencoder's loss on that input.\n\n    Example:\n        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n        tensor([0.1000, 0.3000, 0.6000])\n\n    Args:\n        loss: Loss per item.\n\n    Returns:\n        A tensor of probabilities for each item.\n    \"\"\"\n    square_loss = loss.pow(2)\n    return square_loss / square_loss.sum()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.compute_loss_and_get_activations","title":"<code>compute_loss_and_get_activations(store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Compute the loss on a random subset of inputs.</p> <p>Computes the loss and also stores the input activations (for use in resampling neurons).</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>tuple[TrainBatchStatistic, InputOutputActivationBatch]</code> <p>A tuple containing the loss per item, and all input activations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of items in the store is less than the number of inputs</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def compute_loss_and_get_activations(\n    self,\n    store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; tuple[TrainBatchStatistic, InputOutputActivationBatch]:\n    \"\"\"Compute the loss on a random subset of inputs.\n\n    Computes the loss and also stores the input activations (for use in resampling neurons).\n\n    Args:\n        store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        A tuple containing the loss per item, and all input activations.\n\n    Raises:\n        ValueError: If the number of items in the store is less than the number of inputs\n    \"\"\"\n    with torch.no_grad():\n        loss_batches: list[TrainBatchStatistic] = []\n        input_activations_batches: list[InputOutputActivationBatch] = []\n        dataloader = DataLoader(store, batch_size=train_batch_size)\n        num_inputs = self._resample_dataset_size or len(store)\n        batches: int = num_inputs // train_batch_size\n        model_device: torch.device = get_model_device(autoencoder)\n\n        for batch_idx, batch in enumerate(iter(dataloader)):\n            input_activations_batches.append(batch)\n            source_activations = batch.to(model_device)\n            learned_activations, reconstructed_activations = autoencoder(source_activations)\n            loss_batches.append(\n                loss_fn.forward(\n                    source_activations, learned_activations, reconstructed_activations\n                )\n            )\n            if batch_idx &gt;= batches:\n                break\n\n        loss_result = torch.cat(loss_batches).to(model_device)\n        input_activations = torch.cat(input_activations_batches).to(model_device)\n\n        # Check we generated enough data\n        if len(loss_result) &lt; num_inputs:\n            error_message = (\n                f\"Cannot get {num_inputs} items from the store, \"\n                f\"as only {len(loss_result)} were available.\"\n            )\n            raise ValueError(error_message)\n\n        return loss_result, input_activations\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.get_dead_neuron_indices","title":"<code>get_dead_neuron_indices(neuron_activity_sample_size, neuron_activity, threshold=0)</code>  <code>staticmethod</code>","text":"<p>Identify the indices of neurons that have zero activity.</p> Example <p>neuron_activity = torch.tensor([0, 0, 3, 10, 1]) dead_neuron_indices = ActivationResampler.get_dead_neuron_indices( ...     neuron_activity_sample_size=10, ...     neuron_activity=neuron_activity, ...     threshold=0.2 ... ) dead_neuron_indices.tolist() [0, 1, 4]</p> <p>Parameters:</p> Name Type Description Default <code>neuron_activity_sample_size</code> <code>int</code> <p>Sample size for resampling.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>threshold</code> <code>float</code> <p>Threshold for determining if a neuron is dead (has fired less than this portion of the sample size).</p> <code>0</code> <p>Returns:</p> Type Description <code>LearntNeuronIndices</code> <p>A tensor containing the indices of neurons that are 'dead' (zero activity).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef get_dead_neuron_indices(\n    neuron_activity_sample_size: int,\n    neuron_activity: NeuronActivity,\n    threshold: float = 0,\n) -&gt; LearntNeuronIndices:\n    \"\"\"Identify the indices of neurons that have zero activity.\n\n    Example:\n        &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 1])\n        &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(\n        ...     neuron_activity_sample_size=10,\n        ...     neuron_activity=neuron_activity,\n        ...     threshold=0.2\n        ... )\n        &gt;&gt;&gt; dead_neuron_indices.tolist()\n        [0, 1, 4]\n\n    Args:\n        neuron_activity_sample_size: Sample size for resampling.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        threshold: Threshold for determining if a neuron is dead (has fired less than this\n            portion of the sample size).\n\n    Returns:\n        A tensor containing the indices of neurons that are 'dead' (zero activity).\n    \"\"\"\n    threshold_activity: int = int(neuron_activity_sample_size * threshold)\n    return torch.where(neuron_activity &lt;= threshold_activity)[0]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.renormalize_and_scale","title":"<code>renormalize_and_scale(sampled_input, neuron_activity, encoder_weight)</code>  <code>staticmethod</code>","text":"<p>Renormalize and scale the resampled dictionary vectors.</p> <p>Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2.</p> Example <p>_seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) encoder_weight = torch.ones((6, 2)) rescaled_input = ActivationResampler.renormalize_and_scale( ...     sampled_input, ...     neuron_activity, ...     encoder_weight ... ) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]])</p> <p>Parameters:</p> Name Type Description Default <code>sampled_input</code> <code>SampledDeadNeuronInputs</code> <p>Tensor of the sampled input activation.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>encoder_weight</code> <code>EncoderWeights</code> <p>Tensor of encoder weights.</p> required <p>Returns:</p> Type Description <code>DeadEncoderNeuronWeightUpdates</code> <p>Rescaled sampled input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are no alive neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef renormalize_and_scale(\n    sampled_input: SampledDeadNeuronInputs,\n    neuron_activity: NeuronActivity,\n    encoder_weight: EncoderWeights,\n) -&gt; DeadEncoderNeuronWeightUpdates:\n    \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n    Renormalize the input vector to equal the average norm of the encoder weights for alive\n    neurons times 0.2.\n\n    Example:\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n        &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n        &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))\n        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n        ...     sampled_input,\n        ...     neuron_activity,\n        ...     encoder_weight\n        ... )\n        &gt;&gt;&gt; rescaled_input.round(decimals=1)\n        tensor([[0.2000, 0.2000]])\n\n    Args:\n        sampled_input: Tensor of the sampled input activation.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        encoder_weight: Tensor of encoder weights.\n\n    Returns:\n        Rescaled sampled input.\n\n    Raises:\n        ValueError: If there are no alive neurons.\n    \"\"\"\n    alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n    # Check there is at least one alive neuron\n    if not torch.any(alive_neuron_mask):\n        error_message = \"No alive neurons found.\"\n        raise ValueError(error_message)\n\n    # Handle all alive neurons\n    if torch.all(alive_neuron_mask):\n        return torch.empty(\n            (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n        )\n\n    # Calculate the average norm of the encoder weights for alive neurons.\n    alive_encoder_weights: AliveEncoderWeights = encoder_weight[alive_neuron_mask, :]\n    average_alive_norm: ItemTensor = alive_encoder_weights.norm(dim=-1).mean()\n\n    # Renormalize the input vector to equal the average norm of the encoder weights for alive\n    # neurons times 0.2.\n    renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n        sampled_input, dim=-1\n    )\n    return renormalized_input * (average_alive_norm * 0.2)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.resample_dead_neurons","title":"<code>resample_dead_neurons(activation_store, autoencoder, loss_fn, neuron_activity_sample_size, neuron_activity, train_batch_size)</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>neuron_activity_sample_size</code> <code>int</code> <p>Sample size for resampling.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Number of times each neuron fired.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults</code> <p>Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def resample_dead_neurons(\n    self,\n    activation_store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    neuron_activity_sample_size: int,\n    neuron_activity: NeuronActivity,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        neuron_activity_sample_size: Sample size for resampling.\n        neuron_activity: Number of times each neuron fired.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n    \"\"\"\n    with torch.no_grad():\n        dead_neuron_indices = self.get_dead_neuron_indices(\n            neuron_activity=neuron_activity,\n            neuron_activity_sample_size=neuron_activity_sample_size,\n        )\n\n        # Compute the loss for the current model on a random subset of inputs and get the\n        # activations.\n        loss, input_activations = self.compute_loss_and_get_activations(\n            store=activation_store,\n            autoencoder=autoencoder,\n            loss_fn=loss_fn,\n            train_batch_size=train_batch_size,\n        )\n\n        # Assign each input vector a probability of being picked that is proportional to the\n        # square of the autoencoder's loss on that input.\n        sample_probabilities: TrainBatchStatistic = self.assign_sampling_probabilities(loss)\n\n        # Get references to the encoder and decoder parameters\n        encoder_weight: EncoderWeights = autoencoder.encoder.weight\n\n        # For each dead neuron sample an input according to these probabilities.\n        sampled_input: SampledDeadNeuronInputs = self.sample_input(\n            sample_probabilities, input_activations, len(dead_neuron_indices)\n        )\n\n        # Renormalize the input vector to have unit L2 norm and set this to be the dictionary\n        # vector for the dead autoencoder neuron.\n        renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n            sampled_input, dim=-1\n        )\n\n        dead_decoder_weight_updates = rearrange(\n            renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n        )\n\n        # For the corresponding encoder vector, renormalize the input vector to equal the\n        # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n        # encoder bias element to zero.\n        rescaled_sampled_input = self.renormalize_and_scale(\n            sampled_input, neuron_activity, encoder_weight\n        )\n        dead_encoder_bias_updates = torch.zeros_like(\n            dead_neuron_indices,\n            dtype=dead_decoder_weight_updates.dtype,\n            device=dead_decoder_weight_updates.device,\n        )\n\n        return ParameterUpdateResults(\n            dead_neuron_indices=dead_neuron_indices,\n            dead_encoder_weight_updates=rescaled_sampled_input,\n            dead_encoder_bias_updates=dead_encoder_bias_updates,\n            dead_decoder_weight_updates=dead_decoder_weight_updates,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ActivationResampler.sample_input","title":"<code>sample_input(probabilities, input_activations, num_samples)</code>  <code>staticmethod</code>","text":"<p>Sample an input vector based on the provided probabilities.</p> Example <p>probabilities = torch.tensor([0.1, 0.2, 0.7]) input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = ActivationResampler.sample_input( ...     probabilities, input_activations, 2 ... ) sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]]</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>TrainBatchStatistic</code> <p>Probabilities for each input.</p> required <code>input_activations</code> <code>InputOutputActivationBatch</code> <p>Input activation vectors.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to take (number of dead neurons).</p> required <p>Returns:</p> Type Description <code>SampledDeadNeuronInputs</code> <p>Sampled input activation vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of samples is greater than the number of input activations.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef sample_input(\n    probabilities: TrainBatchStatistic,\n    input_activations: InputOutputActivationBatch,\n    num_samples: int,\n) -&gt; SampledDeadNeuronInputs:\n    \"\"\"Sample an input vector based on the provided probabilities.\n\n    Example:\n        &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n        &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n        ...     probabilities, input_activations, 2\n        ... )\n        &gt;&gt;&gt; sampled_input.tolist()\n        [[5.0, 6.0], [3.0, 4.0]]\n\n    Args:\n        probabilities: Probabilities for each input.\n        input_activations: Input activation vectors.\n        num_samples: Number of samples to take (number of dead neurons).\n\n    Returns:\n        Sampled input activation vector.\n\n    Raises:\n        ValueError: If the number of samples is greater than the number of input activations.\n    \"\"\"\n    if num_samples &gt; len(input_activations):\n        exception_message = (\n            f\"Cannot sample {num_samples} inputs from \"\n            f\"{len(input_activations)} input activations.\"\n        )\n        raise ValueError(exception_message)\n\n    if num_samples == 0:\n        return torch.empty(\n            (0, input_activations.shape[-1]),\n            dtype=input_activations.dtype,\n            device=input_activations.device,\n        ).to(input_activations.device)\n\n    sample_indices: LearntNeuronIndices = torch.multinomial(\n        probabilities, num_samples=num_samples\n    )\n    return input_activations[sample_indices, :]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset","title":"<code>AdamWithReset</code>","text":"<p>             Bases: <code>Adam</code>, <code>AbstractOptimizerWithReset</code></p> <p>Adam Optimizer with a reset method.</p> <p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed.</p> <p>Otherwise this is the same as the standard Adam optimizer.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>@final\nclass AdamWithReset(Adam, AbstractOptimizerWithReset):\n    \"\"\"Adam Optimizer with a reset method.\n\n    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when\n    manually editing the model parameters during training (e.g. when resampling dead neurons). This\n    is because Adam maintains running averages of the gradients and the squares of gradients, which\n    will be incorrect if the parameters are changed.\n\n    Otherwise this is the same as the standard Adam optimizer.\n    \"\"\"\n\n    parameter_names: list[str]\n    \"\"\"Parameter Names.\n\n    The names of the parameters, so that we can find them later when resetting the state.\n    \"\"\"\n\n    def __init__(  # noqa: PLR0913 (extending existing implementation)\n        self,\n        params: params_t,\n        lr: float | Tensor = 1e-3,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        *,\n        amsgrad: bool = False,\n        foreach: bool | None = None,\n        maximize: bool = False,\n        capturable: bool = False,\n        differentiable: bool = False,\n        fused: bool | None = None,\n        named_parameters: Iterator[tuple[str, Parameter]],\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer.\n\n        Warning:\n            Named parameters must be with default settings (remove duplicates and not recursive).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n        Args:\n            params: Iterable of parameters to optimize or dicts defining parameter groups.\n            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n                float LR unless specifying fused=True or capturable=True.\n            betas: Coefficients used for computing running averages of gradient and its square.\n            eps: Term added to the denominator to improve numerical stability.\n            weight_decay: Weight decay (L2 penalty).\n            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n                Convergence of Adam and Beyond\".\n            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n                over the for-loop implementation on CUDA if more performant. Note that foreach uses\n                more peak memory.\n            maximize: If True, maximizes the parameters based on the objective, instead of\n                minimizing.\n            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n                ungraphed performance.\n            differentiable: Whether autograd should occur through the optimizer step in training.\n                Setting to True can impair performance.\n            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n                torch.float32, torch.float16, and torch.bfloat16.\n            named_parameters: An iterator over the named parameters of the model. This is used to\n                find the parameters when resetting their state. You should set this as\n                `model.named_parameters()`.\n\n        Raises:\n            ValueError: If the number of parameter names does not match the number of parameters.\n        \"\"\"\n        # Initialise the parent class (note we repeat the parameter names so that type hints work).\n        super().__init__(\n            params=params,\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n            foreach=foreach,\n            maximize=maximize,\n            capturable=capturable,\n            differentiable=differentiable,\n            fused=fused,\n        )\n\n        # Store the names of the parameters, so that we can find them later when resetting the\n        # state.\n        self.parameter_names = [name for name, _value in named_parameters]\n\n        if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n            error_message = (\n                \"The number of parameter names does not match the number of parameters. \"\n                \"If using model.named_parameters() make sure remove_duplicates is True \"\n                \"and recursive is False (the default settings).\"\n            )\n            raise ValueError(error_message)\n\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Iterates over all parameters and resets both the running averages of the gradients and the\n        squares of gradients.\n        \"\"\"\n        # Iterate over every parameter\n        for group in self.param_groups:\n            for parameter in group[\"params\"]:\n                # Get the state\n                state = self.state[parameter]\n\n                # Check if state is initialized\n                if len(state) == 0:\n                    continue\n\n                # Reset running averages\n                exp_avg: Tensor = state[\"exp_avg\"]\n                exp_avg.zero_()\n                exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n                exp_avg_sq.zero_()\n\n                # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n                if \"max_exp_avg_sq\" in state:\n                    max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                    max_exp_avg_sq.zero_()\n\n    def _get_parameter_name_idx(self, parameter_name: str) -&gt; int:\n        \"\"\"Get the index of a parameter name.\n\n        Args:\n            parameter_name: The name of the parameter.\n\n        Returns:\n            int: The index of the parameter name.\n\n        Raises:\n            ValueError: If the parameter name is not found.\n        \"\"\"\n        if parameter_name not in self.parameter_names:\n            error_message = f\"Parameter name {parameter_name} not found.\"\n            raise ValueError(error_message)\n\n        return self.parameter_names.index(parameter_name)\n\n    def reset_neurons_state(\n        self,\n        parameter_name: str,\n        neuron_indices: LearntNeuronIndices,\n        axis: int,\n        parameter_group: int = 0,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._weight\", dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._bias\", dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\n            ...     \"_decoder._weight\",\n            ...     dead_neurons_indices,\n            ...     axis=1\n            ... )\n\n        Args:\n            parameter_name: The name of the parameter. Examples from the standard sparse autoencoder\n                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n                `_decoder._weight`.\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the parameter to reset.\n            parameter_group: The index of the parameter group to reset (typically this is just zero,\n                unless you have setup multiple parameter groups for e.g. different learning rates\n                for different parameters).\n        \"\"\"\n        # Get the state of the parameter\n        group = self.param_groups[parameter_group]\n        parameter_name_idx = self._get_parameter_name_idx(parameter_name)\n        parameter = group[\"params\"][parameter_name_idx]\n        state = self.state[parameter]\n\n        # Check if state is initialized\n        if len(state) == 0:\n            return\n\n        # Reset running averages for the specified neurons\n        if \"exp_avg\" in state:\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.index_fill_(axis, neuron_indices, 0)\n        if \"exp_avg_sq\" in state:\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n\n        # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n        if \"max_exp_avg_sq\" in state:\n            max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n            max_exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.parameter_names","title":"<code>parameter_names: list[str] = [name for (name, _value) in named_parameters]</code>  <code>instance-attribute</code>","text":"<p>Parameter Names.</p> <p>The names of the parameters, so that we can find them later when resetting the state.</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.__init__","title":"<code>__init__(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, *, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None, named_parameters)</code>","text":"<p>Initialize the optimizer.</p> Warning <p>Named parameters must be with default settings (remove duplicates and not recursive).</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... ) optimizer.reset_state_all_parameters()</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>params_t</code> <p>Iterable of parameters to optimize or dicts defining parameter groups.</p> required <code>lr</code> <code>float | Tensor</code> <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a float LR unless specifying fused=True or capturable=True.</p> <code>0.001</code> <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients used for computing running averages of gradient and its square.</p> <code>(0.9, 0.999)</code> <code>eps</code> <code>float</code> <p>Term added to the denominator to improve numerical stability.</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>Weight decay (L2 penalty).</p> <code>0</code> <code>amsgrad</code> <code>bool</code> <p>Whether to use the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Whether foreach implementation of optimizer is used. If None, foreach is used over the for-loop implementation on CUDA if more performant. Note that foreach uses more peak memory.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>If True, maximizes the parameters based on the objective, instead of minimizing.</p> <code>False</code> <code>capturable</code> <code>bool</code> <p>Whether this instance is safe to capture in a CUDA graph. True can impair ungraphed performance.</p> <code>False</code> <code>differentiable</code> <code>bool</code> <p>Whether autograd should occur through the optimizer step in training. Setting to True can impair performance.</p> <code>False</code> <code>fused</code> <code>bool | None</code> <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64, torch.float32, torch.float16, and torch.bfloat16.</p> <code>None</code> <code>named_parameters</code> <code>Iterator[tuple[str, Parameter]]</code> <p>An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as <code>model.named_parameters()</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of parameter names does not match the number of parameters.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def __init__(  # noqa: PLR0913 (extending existing implementation)\n    self,\n    params: params_t,\n    lr: float | Tensor = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0,\n    *,\n    amsgrad: bool = False,\n    foreach: bool | None = None,\n    maximize: bool = False,\n    capturable: bool = False,\n    differentiable: bool = False,\n    fused: bool | None = None,\n    named_parameters: Iterator[tuple[str, Parameter]],\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Warning:\n        Named parameters must be with default settings (remove duplicates and not recursive).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n    Args:\n        params: Iterable of parameters to optimize or dicts defining parameter groups.\n        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n            float LR unless specifying fused=True or capturable=True.\n        betas: Coefficients used for computing running averages of gradient and its square.\n        eps: Term added to the denominator to improve numerical stability.\n        weight_decay: Weight decay (L2 penalty).\n        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n            Convergence of Adam and Beyond\".\n        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n            over the for-loop implementation on CUDA if more performant. Note that foreach uses\n            more peak memory.\n        maximize: If True, maximizes the parameters based on the objective, instead of\n            minimizing.\n        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n            ungraphed performance.\n        differentiable: Whether autograd should occur through the optimizer step in training.\n            Setting to True can impair performance.\n        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n            torch.float32, torch.float16, and torch.bfloat16.\n        named_parameters: An iterator over the named parameters of the model. This is used to\n            find the parameters when resetting their state. You should set this as\n            `model.named_parameters()`.\n\n    Raises:\n        ValueError: If the number of parameter names does not match the number of parameters.\n    \"\"\"\n    # Initialise the parent class (note we repeat the parameter names so that type hints work).\n    super().__init__(\n        params=params,\n        lr=lr,\n        betas=betas,\n        eps=eps,\n        weight_decay=weight_decay,\n        amsgrad=amsgrad,\n        foreach=foreach,\n        maximize=maximize,\n        capturable=capturable,\n        differentiable=differentiable,\n        fused=fused,\n    )\n\n    # Store the names of the parameters, so that we can find them later when resetting the\n    # state.\n    self.parameter_names = [name for name, _value in named_parameters]\n\n    if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n        error_message = (\n            \"The number of parameter names does not match the number of parameters. \"\n            \"If using model.named_parameters() make sure remove_duplicates is True \"\n            \"and recursive is False (the default settings).\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter_name, neuron_indices, axis, parameter_group=0)</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... )</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>The name of the parameter. Examples from the standard sparse autoencoder implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>, <code>_decoder._weight</code>.</p> required <code>neuron_indices</code> <code>LearntNeuronIndices</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the parameter to reset.</p> required <code>parameter_group</code> <code>int</code> <p>The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters).</p> <code>0</code> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_neurons_state(\n    self,\n    parameter_name: str,\n    neuron_indices: LearntNeuronIndices,\n    axis: int,\n    parameter_group: int = 0,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._weight\", dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._bias\", dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\n        ...     \"_decoder._weight\",\n        ...     dead_neurons_indices,\n        ...     axis=1\n        ... )\n\n    Args:\n        parameter_name: The name of the parameter. Examples from the standard sparse autoencoder\n            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n            `_decoder._weight`.\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the parameter to reset.\n        parameter_group: The index of the parameter group to reset (typically this is just zero,\n            unless you have setup multiple parameter groups for e.g. different learning rates\n            for different parameters).\n    \"\"\"\n    # Get the state of the parameter\n    group = self.param_groups[parameter_group]\n    parameter_name_idx = self._get_parameter_name_idx(parameter_name)\n    parameter = group[\"params\"][parameter_name_idx]\n    state = self.state[parameter]\n\n    # Check if state is initialized\n    if len(state) == 0:\n        return\n\n    # Reset running averages for the specified neurons\n    if \"exp_avg\" in state:\n        exp_avg: Tensor = state[\"exp_avg\"]\n        exp_avg.index_fill_(axis, neuron_indices, 0)\n    if \"exp_avg_sq\" in state:\n        exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n        exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n\n    # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n    if \"max_exp_avg_sq\" in state:\n        max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n        max_exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this","title":"... train the model and then resample some dead neurons, then do this ...","text":"<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated","title":"Reset the optimizer state for parameters that have been updated","text":"<p>optimizer.reset_neurons_state(\"_encoder._weight\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state(\"_encoder._bias\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ...     \"_decoder._weight\", ...     dead_neurons_indices, ...     axis=1 ... )</p>"},{"location":"reference/#sparse_autoencoder.AdamWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>","text":"<p>Reset the state for all parameters.</p> <p>Iterates over all parameters and resets both the running averages of the gradients and the squares of gradients.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Iterates over all parameters and resets both the running averages of the gradients and the\n    squares of gradients.\n    \"\"\"\n    # Iterate over every parameter\n    for group in self.param_groups:\n        for parameter in group[\"params\"]:\n            # Get the state\n            state = self.state[parameter]\n\n            # Check if state is initialized\n            if len(state) == 0:\n                continue\n\n            # Reset running averages\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.zero_()\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.zero_()\n\n            # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n            if \"max_exp_avg_sq\" in state:\n                max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                max_exp_avg_sq.zero_()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric","title":"<code>CapacityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Capacities Metrics for Learned Features.</p> <p>Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks.</p> <p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features.</p> <p>If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>class CapacityMetric(AbstractTrainMetric):\n    \"\"\"Capacities Metrics for Learned Features.\n\n    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf).\n\n    Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.\n    Formally it's the ratio of the squared dot product of a feature with itself to the sum of its\n    squared dot products of all features.\n\n    If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is\n    1/n.\n    \"\"\"\n\n    @staticmethod\n    def capacities(features: LearnedActivationBatch) -&gt; TrainBatchStatistic:\n        \"\"\"Calculate capacities.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n            &gt;&gt;&gt; orthogonal_caps\n            tensor([1., 1., 1.])\n\n        Args:\n            features: A collection of features.\n\n        Returns:\n            A 1D tensor of capacities, where each element is the capacity of the corresponding\n            feature.\n        \"\"\"\n        squared_dot_products = (\n            einops.einsum(\n                features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n            )\n            ** 2\n        )\n        sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n        return torch.diag(squared_dot_products) / sum_of_sq_dot\n\n    @staticmethod\n    def wandb_capacities_histogram(\n        capacities: TrainBatchStatistic,\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the capacities.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n        wandb_capacities_histogram(capacities)})`.\n\n        Args:\n            capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the capacities for a training batch.\"\"\"\n        train_batch_capacities = self.capacities(data.learned_activations)\n        train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n        return {\n            \"train_batch_capacities_histogram\": train_batch_capacities_histogram,\n        }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the capacities for a training batch.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the capacities for a training batch.\"\"\"\n    train_batch_capacities = self.capacities(data.learned_activations)\n    train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n    return {\n        \"train_batch_capacities_histogram\": train_batch_capacities_histogram,\n    }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.capacities","title":"<code>capacities(features)</code>  <code>staticmethod</code>","text":"<p>Calculate capacities.</p> Example <p>import torch orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) orthogonal_caps = CapacityMetric.capacities(orthogonal_features) orthogonal_caps tensor([1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>LearnedActivationBatch</code> <p>A collection of features.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p> <code>TrainBatchStatistic</code> <p>feature.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef capacities(features: LearnedActivationBatch) -&gt; TrainBatchStatistic:\n    \"\"\"Calculate capacities.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n        &gt;&gt;&gt; orthogonal_caps\n        tensor([1., 1., 1.])\n\n    Args:\n        features: A collection of features.\n\n    Returns:\n        A 1D tensor of capacities, where each element is the capacity of the corresponding\n        feature.\n    \"\"\"\n    squared_dot_products = (\n        einops.einsum(\n            features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n        )\n        ** 2\n    )\n    sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n    return torch.diag(squared_dot_products) / sum_of_sq_dot\n</code></pre>"},{"location":"reference/#sparse_autoencoder.CapacityMetric.wandb_capacities_histogram","title":"<code>wandb_capacities_histogram(capacities)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the capacities.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>TrainBatchStatistic</code> <p>Capacity of each feature. Can be calculated using :func:<code>calc_capacities</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef wandb_capacities_histogram(\n    capacities: TrainBatchStatistic,\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the capacities.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n    wandb_capacities_histogram(capacities)})`.\n\n    Args:\n        capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore","title":"<code>DiskActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Disk Activation Store.</p> <p>Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches.</p> <p>Multiprocess safe (supports writing from multiple GPU workers).</p> <p>Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set <code>empty_dir</code> to <code>True</code>.</p> <p>Note also that :meth:<code>close</code> must be called to ensure all activation vectors are written to disk after the last batch has been added to the store.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>class DiskActivationStore(ActivationStore):\n    \"\"\"Disk Activation Store.\n\n    Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up\n    activation vectors and then write them to the disk in batches.\n\n    Multiprocess safe (supports writing from multiple GPU workers).\n\n    Warning:\n    Unless you want to keep and use existing .pt files in the storage directory when initialized,\n    set `empty_dir` to `True`.\n\n    Note also that :meth:`close` must be called to ensure all activation vectors are written to disk\n    after the last batch has been added to the store.\n    \"\"\"\n\n    _storage_path: Path\n    \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\"\n\n    _cache: ListProxy\n    \"\"\"Cache for Activation Vectors.\n\n    Activation vectors are buffered in memory until the cache is full, at which point they are\n    written to disk.\n    \"\"\"\n\n    _cache_lock: Lock\n    \"\"\"Lock for the Cache.\"\"\"\n\n    _max_cache_size: int\n    \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\"\n\n    _thread_pool: ThreadPoolExecutor\n    \"\"\"Threadpool for non-blocking writes to the file system.\"\"\"\n\n    _disk_n_activation_vectors: ValueProxy[int]\n    \"\"\"Length of the Store (on disk).\n\n    Minus 1 signifies not calculated yet.\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n        max_cache_size: int = 10_000,\n        num_workers: int = 6,\n        *,\n        empty_dir: bool = False,\n    ):\n        \"\"\"Initialize the Disk Activation Store.\n\n        Args:\n            storage_path: Path to the directory where the activation vectors will be stored.\n            max_cache_size: The maximum number of activation vectors to cache in memory before\n                writing to disk. Note this is only followed approximately.\n            num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n                that the model can keep running whilst it writes the previous activations to disk).\n                This should be less than the number of CPU cores available. You don't need multiple\n                GPUs to take advantage of this feature.\n            empty_dir: Whether to empty the directory before writing. Generally you want to set this\n                to `True` as otherwise the directory may contain stale activation vectors from\n                previous runs.\n        \"\"\"\n        super().__init__()\n\n        # Setup the storage directory\n        self._storage_path = storage_path\n        self._storage_path.mkdir(parents=True, exist_ok=True)\n\n        # Setup the Cache\n        manager = Manager()\n        self._cache = manager.list()\n        self._max_cache_size = max_cache_size\n        self._cache_lock = manager.Lock()\n        self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n        # Empty the directory if needed\n        if empty_dir:\n            self.empty()\n\n        # Create a threadpool for non-blocking writes to the cache\n        self._thread_pool = ThreadPoolExecutor(num_workers)\n\n    def _write_to_disk(self, *, wait_for_max: bool = False) -&gt; None:\n        \"\"\"Write the contents of the queue to disk.\n\n        Args:\n            wait_for_max: Whether to wait until the cache is full before writing to disk.\n        \"\"\"\n        with self._cache_lock:\n            # Check we have enough items\n            if len(self._cache) == 0:\n                return\n\n            size_to_get = min(self._max_cache_size, len(self._cache))\n            if wait_for_max and size_to_get &lt; self._max_cache_size:\n                return\n\n            # Get the activations from the cache and delete them\n            activations = self._cache[0:size_to_get]\n            del self._cache[0:size_to_get]\n\n            # Update the length cache\n            if self._disk_n_activation_vectors.value != -1:\n                self._disk_n_activation_vectors.value += len(activations)\n\n        stacked_activations = torch.stack(activations)\n\n        filename = f\"{self.__len__}.pt\"\n        torch.save(stacked_activations, self._storage_path / filename)\n\n    def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n        \"\"\"Add a Single Item to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        Args:\n            item: Activation vector to add to the store.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        with self._cache_lock:\n            self._cache.append(item)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n        \"\"\"Add a Batch to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n        &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        10\n\n        Args:\n            batch: Batch of activation vectors to add to the store.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        items: list[InputOutputActivationVector] = resize_to_list_vectors(batch)\n\n        with self._cache_lock:\n            self._cache.extend(items)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        This should be called after the last batch has been added to the store. It will wait for\n        all activation vectors to be written to disk.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; print(len(store))\n        1\n        \"\"\"\n        while len(self._cache) &gt; 0:\n            self._write_to_disk()\n\n    @property\n    def _all_filenames(self) -&gt; list[Path]:\n        \"\"\"Return a List of All Activation Vector Filenames.\"\"\"\n        return list(self._storage_path.glob(\"*.pt\"))\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the Store.\n\n        Warning:\n        This will delete all .pt files in the top level of the storage directory.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; print(len(store))\n        0\n        \"\"\"\n        for file in self._all_filenames:\n            file.unlink()\n        self._disk_n_activation_vectors.value = 0\n\n    def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n        \"\"\"Get Item Dunder Method.\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        # Find the file containing the activation vector\n        file_index = index // self._max_cache_size\n        file = self._storage_path / f\"{file_index}.pt\"\n\n        # Load the file and return the activation vector\n        activation_vectors = torch.load(file)\n        return activation_vectors[index % self._max_cache_size]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n            &gt;&gt;&gt; print(len(store))\n            0\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        # Calculate the length if not cached\n        if self._disk_n_activation_vectors.value == -1:\n            cache_size: int = 0\n            for file in self._all_filenames:\n                cache_size += len(torch.load(file))\n            self._disk_n_activation_vectors.value = cache_size\n\n        return self._disk_n_activation_vectors.value\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        # Shutdown the thread pool after everything is complete\n        self._thread_pool.shutdown(wait=True, cancel_futures=False)\n        self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    # Shutdown the thread pool after everything is complete\n    self._thread_pool.shutdown(wait=True, cancel_futures=False)\n    self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationVector</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n    \"\"\"Get Item Dunder Method.\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    # Find the file containing the activation vector\n    file_index = index // self._max_cache_size\n    file = self._storage_path / f\"{file_index}.pt\"\n\n    # Load the file and return the activation vector\n    activation_vectors = torch.load(file)\n    return activation_vectors[index % self._max_cache_size]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__init__","title":"<code>__init__(storage_path=DEFAULT_DISK_ACTIVATION_STORE_PATH, max_cache_size=10000, num_workers=6, *, empty_dir=False)</code>","text":"<p>Initialize the Disk Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>storage_path</code> <code>Path</code> <p>Path to the directory where the activation vectors will be stored.</p> <code>DEFAULT_DISK_ACTIVATION_STORE_PATH</code> <code>max_cache_size</code> <code>int</code> <p>The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately.</p> <code>10000</code> <code>num_workers</code> <code>int</code> <p>Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature.</p> <code>6</code> <code>empty_dir</code> <code>bool</code> <p>Whether to empty the directory before writing. Generally you want to set this to <code>True</code> as otherwise the directory may contain stale activation vectors from previous runs.</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __init__(\n    self,\n    storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n    max_cache_size: int = 10_000,\n    num_workers: int = 6,\n    *,\n    empty_dir: bool = False,\n):\n    \"\"\"Initialize the Disk Activation Store.\n\n    Args:\n        storage_path: Path to the directory where the activation vectors will be stored.\n        max_cache_size: The maximum number of activation vectors to cache in memory before\n            writing to disk. Note this is only followed approximately.\n        num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n            that the model can keep running whilst it writes the previous activations to disk).\n            This should be less than the number of CPU cores available. You don't need multiple\n            GPUs to take advantage of this feature.\n        empty_dir: Whether to empty the directory before writing. Generally you want to set this\n            to `True` as otherwise the directory may contain stale activation vectors from\n            previous runs.\n    \"\"\"\n    super().__init__()\n\n    # Setup the storage directory\n    self._storage_path = storage_path\n    self._storage_path.mkdir(parents=True, exist_ok=True)\n\n    # Setup the Cache\n    manager = Manager()\n    self._cache = manager.list()\n    self._max_cache_size = max_cache_size\n    self._cache_lock = manager.Lock()\n    self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n    # Empty the directory if needed\n    if empty_dir:\n        self.empty()\n\n    # Create a threadpool for non-blocking writes to the cache\n    self._thread_pool = ThreadPoolExecutor(num_workers)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> Example <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; print(len(store))\n        0\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    # Calculate the length if not cached\n    if self._disk_n_activation_vectors.value == -1:\n        cache_size: int = 0\n        for file in self._all_filenames:\n            cache_size += len(torch.load(file))\n        self._disk_n_activation_vectors.value = cache_size\n\n    return self._disk_n_activation_vectors.value\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a Single Item to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>InputOutputActivationVector</code> <p>Activation vector to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n    \"\"\"Add a Single Item to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    Args:\n        item: Activation vector to add to the store.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    with self._cache_lock:\n        self._cache.append(item)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the Store.</p> <p>Warning: This will delete all .pt files in the top level of the storage directory.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>store.empty() print(len(store)) 0</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the Store.\n\n    Warning:\n    This will delete all .pt files in the top level of the storage directory.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; print(len(store))\n    0\n    \"\"\"\n    for file in self._all_filenames:\n        file.unlink()\n    self._disk_n_activation_vectors.value = 0\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a Batch to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SourceModelActivations</code> <p>Batch of activation vectors to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n    \"\"\"Add a Batch to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n    &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    10\n\n    Args:\n        batch: Batch of activation vectors to add to the store.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    items: list[InputOutputActivationVector] = resize_to_list_vectors(batch)\n\n    with self._cache_lock:\n        self._cache.extend(items)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/#sparse_autoencoder.DiskActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    This should be called after the last batch has been added to the store. It will wait for\n    all activation vectors to be written to disk.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; store.wait_for_writes_to_complete()\n    &gt;&gt;&gt; print(len(store))\n    1\n    \"\"\"\n    while len(self._cache) &gt; 0:\n        self._write_to_disk()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss","title":"<code>L2ReconstructionLoss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>L2 Reconstruction loss.</p> <p>L2 reconstruction loss is calculated as the sum squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with L2 may achieve the same loss for both polysemantic and monosemantic representations of true features.</p> Example <p>import torch loss = L2ReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations)</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>@final\nclass L2ReconstructionLoss(AbstractLoss):\n    \"\"\"L2 Reconstruction loss.\n\n    L2 reconstruction loss is calculated as the sum squared error between each each input vector\n    and it's corresponding decoded vector. The original paper found that models trained with some\n    loss functions such as cross-entropy loss generally prefer to represent features\n    polysemantically, whereas models trained with L2 may achieve the same loss for both\n    polysemantic and monosemantic representations of true features.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; loss = L2ReconstructionLoss()\n        &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])\n        &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(input_activations)\n        &gt;&gt;&gt; # Outputs both loss and metrics to log\n        &gt;&gt;&gt; loss(input_activations, unused_activations, output_activations)\n        (tensor(11.), {'l2_reconstruction_loss': 11.0})\n    \"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"l2_reconstruction_loss\"\n\n    def forward(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,  # noqa: ARG002\n        decoded_activations: InputOutputActivationBatch,\n    ) -&gt; TrainBatchStatistic:\n        \"\"\"Calculate the L2 reconstruction loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n        # Sum over just the features dimension (i.e. batch itemwise loss). Note this is sum rather\n        # than mean to be consistent with L1 loss (and thus make the l1 coefficient stable to number\n        # of features).\n        return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log","title":"Outputs both loss and metrics to log","text":"<p>loss(input_activations, unused_activations, output_activations) (tensor(11.), {'l2_reconstruction_loss': 11.0})</p>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Calculate the L2 reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def forward(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,  # noqa: ARG002\n    decoded_activations: InputOutputActivationBatch,\n) -&gt; TrainBatchStatistic:\n    \"\"\"Calculate the L2 reconstruction loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n    # Sum over just the features dimension (i.e. batch itemwise loss). Note this is sum rather\n    # than mean to be consistent with L1 loss (and thus make the l1 coefficient stable to number\n    # of features).\n    return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.L2ReconstructionLoss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"l2_reconstruction_loss\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss","title":"<code>LearnedActivationsL1Loss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Learned activations L1 (absolute error) loss.</p> <p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity).</p> Example <p>l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations)</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>@final\nclass LearnedActivationsL1Loss(AbstractLoss):\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this\n    multiplied by the l1_coefficient (designed to encourage sparsity).\n\n    Example:\n        &gt;&gt;&gt; l1_loss = LearnedActivationsL1Loss(0.1)\n        &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(learned_activations)\n        &gt;&gt;&gt; # Returns loss and metrics to log\n        &gt;&gt;&gt; l1_loss(unused_activations, learned_activations, unused_activations)[0]\n        tensor(0.5000)\n    \"\"\"\n\n    l1_coefficient: float\n    \"\"\"L1 coefficient.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"learned_activations_l1_loss_penalty\"\n\n    def __init__(self, l1_coefficient: float) -&gt; None:\n        \"\"\"Initialize the absolute error loss.\n\n        Args:\n            l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n                [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n                approximate guide if you use e.g. 2x this number of tokens you might consider using\n                0.5x the l1 coefficient.\n        \"\"\"\n        self.l1_coefficient = l1_coefficient\n        super().__init__()\n\n    def _l1_loss(\n        self,\n        source_activations: InputOutputActivationBatch,  # noqa: ARG002\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,  # noqa: ARG002\n    ) -&gt; tuple[TrainBatchStatistic, TrainBatchStatistic]:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Tuple of itemwise absolute loss, and itemwise absolute loss multiplied by the l1\n            coefficient.\n        \"\"\"\n        absolute_loss = torch.abs(learned_activations).sum(dim=-1)\n        absolute_loss_penalty = absolute_loss * self.l1_coefficient\n        return absolute_loss, absolute_loss_penalty\n\n    def forward(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n    ) -&gt; TrainBatchStatistic:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n\n    # Override to add both the loss and the penalty to the log\n    def batch_scalar_loss_with_log(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[ItemTensor, LossLogType]:\n        \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n                (loss before and after the l1 coefficient).\n        \"\"\"\n        absolute_loss, absolute_loss_penalty = self._l1_loss(\n            source_activations, learned_activations, decoded_activations\n        )\n\n        match reduction:\n            case LossReductionType.MEAN:\n                batch_scalar_loss = absolute_loss.mean().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n            case LossReductionType.SUM:\n                batch_scalar_loss = absolute_loss.sum().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n        metrics = {\n            \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n            self.log_name(): batch_scalar_loss_penalty.item(),\n        }\n\n        return batch_scalar_loss_penalty, metrics\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Extra representation string.\"\"\"\n        return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log","title":"Returns loss and metrics to log","text":"<p>l1_loss(unused_activations, learned_activations, unused_activations)[0] tensor(0.5000)</p>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.l1_coefficient","title":"<code>l1_coefficient: float = l1_coefficient</code>  <code>instance-attribute</code>","text":"<p>L1 coefficient.</p>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.__init__","title":"<code>__init__(l1_coefficient)</code>","text":"<p>Initialize the absolute error loss.</p> <p>Parameters:</p> Name Type Description Default <code>l1_coefficient</code> <code>float</code> <p>L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient.</p> required Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def __init__(self, l1_coefficient: float) -&gt; None:\n    \"\"\"Initialize the absolute error loss.\n\n    Args:\n        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n            approximate guide if you use e.g. 2x this number of tokens you might consider using\n            0.5x the l1 coefficient.\n    \"\"\"\n    self.l1_coefficient = l1_coefficient\n    super().__init__()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.batch_scalar_loss_with_log","title":"<code>batch_scalar_loss_with_log(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Learned activations L1 (absolute error) loss, with log.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[ItemTensor, LossLogType]</code> <p>Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log (loss before and after the l1 coefficient).</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def batch_scalar_loss_with_log(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[ItemTensor, LossLogType]:\n    \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n            (loss before and after the l1 coefficient).\n    \"\"\"\n    absolute_loss, absolute_loss_penalty = self._l1_loss(\n        source_activations, learned_activations, decoded_activations\n    )\n\n    match reduction:\n        case LossReductionType.MEAN:\n            batch_scalar_loss = absolute_loss.mean().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n        case LossReductionType.SUM:\n            batch_scalar_loss = absolute_loss.sum().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n    metrics = {\n        \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n        self.log_name(): batch_scalar_loss_penalty.item(),\n    }\n\n    return batch_scalar_loss_penalty, metrics\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Extra representation string.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Extra representation string.\"\"\"\n    return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Learned activations L1 (absolute error) loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def forward(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n) -&gt; TrainBatchStatistic:\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LearnedActivationsL1Loss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"learned_activations_l1_loss_penalty\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore","title":"<code>ListActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>List Activation Store.</p> <p>Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance.</p> <p>Multiprocess safe if the <code>multiprocessing_enabled</code> argument is set to <code>True</code>. This works in two ways:</p> <ol> <li>The list of activation vectors is stored in a multiprocessing manager, which allows multiple     processes (typically multiple GPUs) to read/write to the list.</li> <li>The <code>extend</code> method is non-blocking, and uses a threadpool to write to the list in the     background, which allows the main process to continue working even if there is just one GPU.</li> </ol> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Note that the built-in :meth:<code>shuffle</code> method is much faster than using the <code>shuffle</code> argument on <code>torch.utils.data.DataLoader</code>. You should therefore call this method before passing the dataset to the loader and then set the DataLoader <code>shuffle</code> argument to <code>False</code>.</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = ListActivationStore()\n</code></pre> <p>Add a single activation vector to the dataset (this is blocking):</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a batch of activation vectors to the dataset (non-blocking):</p> <pre><code>&gt;&gt;&gt; batch = torch.randn(10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n11\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>class ListActivationStore(ActivationStore):\n    \"\"\"List Activation Store.\n\n    Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick\n    experiments where you don't want to calculate how much memory you need in advance.\n\n    Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two\n    ways:\n\n    1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple\n        processes (typically multiple GPUs) to read/write to the list.\n    2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the\n        background, which allows the main process to continue working even if there is just one GPU.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument\n    on `torch.utils.data.DataLoader`. You should therefore call this method before passing the\n    dataset to the loader and then set the DataLoader `shuffle` argument to `False`.\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n\n    Add a single activation vector to the dataset (this is blocking):\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a batch of activation vectors to the dataset (non-blocking):\n\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        11\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: list[InputOutputActivationVector] | ListProxy\n    \"\"\"Underlying List Data Store.\"\"\"\n\n    _device: torch.device | None\n    \"\"\"Device to Store the Activation Vectors On.\"\"\"\n\n    _pool: ProcessPoolExecutor | None = None\n    \"\"\"Multiprocessing Pool.\"\"\"\n\n    _pool_exceptions: ListProxy | list[Exception]\n    \"\"\"Pool Exceptions.\n\n    Used to keep track of exceptions.\n    \"\"\"\n\n    _pool_futures: list[Future]\n    \"\"\"Pool Futures.\n\n    Used to keep track of processes running in the pool.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: list[InputOutputActivationVector] | None = None,\n        device: torch.device | None = None,\n        max_workers: int | None = None,\n        *,\n        multiprocessing_enabled: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the List Activation Store.\n\n        Args:\n            data: Data to initialize the dataset with.\n            device: Device to store the activation vectors on.\n            max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n                Default is the number of cores you have.\n            multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n                workers. This creates significant overhead, so you should only enable it if you have\n                multiple GPUs (and experiment with enabling/disabling it).\n        \"\"\"\n        # Default to empty\n        if data is None:\n            data = []\n\n        # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n        # between processes. Otherwise, just use a normal list.\n        if multiprocessing_enabled:\n            self._pool = ProcessPoolExecutor(max_workers=max_workers)\n            manager = Manager()\n            self._data = manager.list(data)\n            self._data.extend(data)\n            self._pool_exceptions = manager.list()\n        else:\n            self._data = data\n            self._pool_exceptions = []\n\n        self._pool_futures = []\n\n        # Device for storing the activation vectors\n        self._device = device\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return len(self._data)\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Returns:\n            The size of the dataset in bytes.\n        \"\"\"\n        # The list of tensors is really a list of pointers to tensors, so we need to account for\n        # this as well as the size of the tensors themselves.\n        list_of_pointers_size = self._data.__sizeof__()\n\n        # Handle 0 items\n        if len(self._data) == 0:\n            return list_of_pointers_size\n\n        # Otherwise, get the size of the first tensor\n        first_tensor = self._data[0]\n        first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n        num_tensors = len(self._data)\n        total_tensors_size = first_tensor_size * num_tensors\n\n        return total_tensors_size + list_of_pointers_size\n\n    def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.append(torch.tensor([3.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; len(store)\n        3\n\n        \"\"\"\n        self.wait_for_writes_to_complete()\n        random.shuffle(self._data)\n\n    def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n        \"\"\"Append a single item to the dataset.\n\n        Note **append is blocking**. For better performance use extend instead with batches.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        Args:\n            item: The item to append to the dataset.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        self._data.append(item.to(self._device))\n\n    def _extend(self, batch: SourceModelActivations) -&gt; None:\n        \"\"\"Extend threadpool method.\n\n        To be called by :meth:`extend`.\n\n        Args:\n            batch: A batch of items to add to the dataset.\n        \"\"\"\n        try:\n            # Unstack to a list of tensors\n            items: list[InputOutputActivationVector] = resize_to_list_vectors(batch)\n\n            self._data.extend(items)\n        except Exception as e:  # noqa: BLE001\n            self._pool_exceptions.append(e)\n\n    def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n        \"\"\"Extend the dataset with multiple items (non-blocking).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; batch = torch.randn(10, 100)\n            &gt;&gt;&gt; async_result = store.extend(batch)\n            &gt;&gt;&gt; len(store)\n            10\n\n        Args:\n            batch: A batch of items to add to the dataset.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        # Schedule _extend to run in a separate process\n        if self._pool:\n            future = self._pool.submit(self._extend, batch)\n            self._pool_futures.append(future)\n\n        # Fallback to synchronous execution if not multiprocessing\n        self._extend(batch)\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n            &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n            &gt;&gt;&gt; store.wait_for_writes_to_complete()\n            &gt;&gt;&gt; len(store)\n            3\n\n        Raises:\n            RuntimeError: If any exceptions occurred in the background workers.\n        \"\"\"\n        # Restart the pool\n        if self._pool:\n            for _future in as_completed(self._pool_futures):\n                pass\n            self._pool_futures.clear()\n\n        time.sleep(1)\n\n        if self._pool_exceptions:\n            exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n            msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n            raise RuntimeError(msg)\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the dataset.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; len(store)\n        0\n        \"\"\"\n        self.wait_for_writes_to_complete()\n\n        # Clearing a list like this works for both standard and multiprocessing lists\n        self._data[:] = []\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        if self._pool:\n            self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    if self._pool:\n        self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationVector</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    return self._data[index]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__init__","title":"<code>__init__(data=None, device=None, max_workers=None, *, multiprocessing_enabled=False)</code>","text":"<p>Initialize the List Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[InputOutputActivationVector] | None</code> <p>Data to initialize the dataset with.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> <code>max_workers</code> <code>int | None</code> <p>Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have.</p> <code>None</code> <code>multiprocessing_enabled</code> <code>bool</code> <p>Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it).</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __init__(\n    self,\n    data: list[InputOutputActivationVector] | None = None,\n    device: torch.device | None = None,\n    max_workers: int | None = None,\n    *,\n    multiprocessing_enabled: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the List Activation Store.\n\n    Args:\n        data: Data to initialize the dataset with.\n        device: Device to store the activation vectors on.\n        max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n            Default is the number of cores you have.\n        multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n            workers. This creates significant overhead, so you should only enable it if you have\n            multiple GPUs (and experiment with enabling/disabling it).\n    \"\"\"\n    # Default to empty\n    if data is None:\n        data = []\n\n    # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n    # between processes. Otherwise, just use a normal list.\n    if multiprocessing_enabled:\n        self._pool = ProcessPoolExecutor(max_workers=max_workers)\n        manager = Manager()\n        self._data = manager.list(data)\n        self._data.extend(data)\n        self._pool_exceptions = manager.list()\n    else:\n        self._data = data\n        self._pool_exceptions = []\n\n    self._pool_futures = []\n\n    # Device for storing the activation vectors\n    self._device = device\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the dataset in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Returns:\n        The size of the dataset in bytes.\n    \"\"\"\n    # The list of tensors is really a list of pointers to tensors, so we need to account for\n    # this as well as the size of the tensors themselves.\n    list_of_pointers_size = self._data.__sizeof__()\n\n    # Handle 0 items\n    if len(self._data) == 0:\n        return list_of_pointers_size\n\n    # Otherwise, get the size of the first tensor\n    first_tensor = self._data[0]\n    first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n    num_tensors = len(self._data)\n    total_tensors_size = first_tensor_size * num_tensors\n\n    return total_tensors_size + list_of_pointers_size\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.append","title":"<code>append(item)</code>","text":"<p>Append a single item to the dataset.</p> <p>Note append is blocking. For better performance use extend instead with batches.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>InputOutputActivationVector</code> <p>The item to append to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n    \"\"\"Append a single item to the dataset.\n\n    Note **append is blocking**. For better performance use extend instead with batches.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    Args:\n        item: The item to append to the dataset.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    self._data.append(item.to(self._device))\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the dataset.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>store.empty() len(store) 0</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the dataset.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; len(store)\n    0\n    \"\"\"\n    self.wait_for_writes_to_complete()\n\n    # Clearing a list like this works for both standard and multiprocessing lists\n    self._data[:] = []\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Extend the dataset with multiple items (non-blocking).</p> Example <p>import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SourceModelActivations</code> <p>A batch of items to add to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n    \"\"\"Extend the dataset with multiple items (non-blocking).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; async_result = store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        10\n\n    Args:\n        batch: A batch of items to add to the dataset.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    # Schedule _extend to run in a separate process\n    if self._pool:\n        future = self._pool.submit(self._extend, batch)\n        self._pool_futures.append(future)\n\n    # Fallback to synchronous execution if not multiprocessing\n    self._extend(batch)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.append(torch.tensor([3.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; len(store)\n    3\n\n    \"\"\"\n    self.wait_for_writes_to_complete()\n    random.shuffle(self._data)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.ListActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>Wait for any non-blocking writes (e.g. calls to :meth:<code>append</code>) to complete.</p> Example <p>import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any exceptions occurred in the background workers.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n        &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; len(store)\n        3\n\n    Raises:\n        RuntimeError: If any exceptions occurred in the background workers.\n    \"\"\"\n    # Restart the pool\n    if self._pool:\n        for _future in as_completed(self._pool_futures):\n            pass\n        self._pool_futures.clear()\n\n    time.sleep(1)\n\n    if self._pool_exceptions:\n        exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n        msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n        raise RuntimeError(msg)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer","title":"<code>LossReducer</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Loss reducer.</p> <p>Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential.</p> Example <p>from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ...     L2ReconstructionLoss(), ...     LearnedActivationsL1Loss(0.001), ... ) LossReducer(   (0): L2ReconstructionLoss()   (1): LearnedActivationsL1Loss(l1_coefficient=0.001) )</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>@final\nclass LossReducer(AbstractLoss):\n    \"\"\"Loss reducer.\n\n    Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to\n    nn.Sequential.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss\n        &gt;&gt;&gt; from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss\n        &gt;&gt;&gt; LossReducer(\n        ...     L2ReconstructionLoss(),\n        ...     LearnedActivationsL1Loss(0.001),\n        ... )\n        LossReducer(\n          (0): L2ReconstructionLoss()\n          (1): LearnedActivationsL1Loss(l1_coefficient=0.001)\n        )\n\n    \"\"\"\n\n    _modules: dict[str, \"AbstractLoss\"]\n    \"\"\"Children loss modules.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"total_loss\"\n\n    def __init__(\n        self,\n        *loss_modules: AbstractLoss,\n    ):\n        \"\"\"Initialize the loss reducer.\n\n        Args:\n            *loss_modules: Loss modules to reduce.\n\n        Raises:\n            ValueError: If the loss reducer has no loss modules.\n        \"\"\"\n        super().__init__()\n\n        for idx, loss_module in enumerate(loss_modules):\n            self._modules[str(idx)] = loss_module\n\n        if len(self) == 0:\n            error_message = \"Loss reducer must have at least one loss module.\"\n            raise ValueError(error_message)\n\n    def forward(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n    ) -&gt; TrainBatchStatistic:\n        \"\"\"Reduce loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Mean loss across the batch, summed across the loss modules.\n        \"\"\"\n        all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n            [\n                loss_module.forward(source_activations, learned_activations, decoded_activations)\n                for loss_module in self._modules.values()\n            ]\n        )\n\n        return all_modules_loss.sum(dim=0)\n\n    def __dir__(self) -&gt; list[str]:\n        \"\"\"Dir dunder method.\"\"\"\n        return list(self._modules.__dir__())\n\n    def __getitem__(self, idx: int) -&gt; AbstractLoss:\n        \"\"\"Get item dunder method.\"\"\"\n        return self._modules[str(idx)]\n\n    def __iter__(self) -&gt; Iterator[AbstractLoss]:\n        \"\"\"Iterator dunder method.\"\"\"\n        return iter(self._modules.values())\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length dunder method.\"\"\"\n        return len(self._modules)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__dir__","title":"<code>__dir__()</code>","text":"<p>Dir dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __dir__(self) -&gt; list[str]:\n    \"\"\"Dir dunder method.\"\"\"\n    return list(self._modules.__dir__())\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get item dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; AbstractLoss:\n    \"\"\"Get item dunder method.\"\"\"\n    return self._modules[str(idx)]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__init__","title":"<code>__init__(*loss_modules)</code>","text":"<p>Initialize the loss reducer.</p> <p>Parameters:</p> Name Type Description Default <code>*loss_modules</code> <code>AbstractLoss</code> <p>Loss modules to reduce.</p> <code>()</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the loss reducer has no loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __init__(\n    self,\n    *loss_modules: AbstractLoss,\n):\n    \"\"\"Initialize the loss reducer.\n\n    Args:\n        *loss_modules: Loss modules to reduce.\n\n    Raises:\n        ValueError: If the loss reducer has no loss modules.\n    \"\"\"\n    super().__init__()\n\n    for idx, loss_module in enumerate(loss_modules):\n        self._modules[str(idx)] = loss_module\n\n    if len(self) == 0:\n        error_message = \"Loss reducer must have at least one loss module.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterator dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __iter__(self) -&gt; Iterator[AbstractLoss]:\n    \"\"\"Iterator dunder method.\"\"\"\n    return iter(self._modules.values())\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.__len__","title":"<code>__len__()</code>","text":"<p>Length dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length dunder method.\"\"\"\n    return len(self._modules)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Reduce loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>Mean loss across the batch, summed across the loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def forward(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n) -&gt; TrainBatchStatistic:\n    \"\"\"Reduce loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Mean loss across the batch, summed across the loss modules.\n    \"\"\"\n    all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n        [\n            loss_module.forward(source_activations, learned_activations, decoded_activations)\n            for loss_module in self._modules.values()\n        ]\n    )\n\n    return all_modules_loss.sum(dim=0)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReducer.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"total_loss\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReductionType","title":"<code>LossReductionType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Loss reduction type (across batch items).</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>class LossReductionType(LowercaseStrEnum):\n    \"\"\"Loss reduction type (across batch items).\"\"\"\n\n    MEAN = \"mean\"\n    \"\"\"Mean loss across batch items.\"\"\"\n\n    SUM = \"sum\"\n    \"\"\"Sum the loss from all batch items.\"\"\"\n</code></pre>"},{"location":"reference/#sparse_autoencoder.LossReductionType.MEAN","title":"<code>MEAN = 'mean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean loss across batch items.</p>"},{"location":"reference/#sparse_autoencoder.LossReductionType.SUM","title":"<code>SUM = 'sum'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sum the loss from all batch items.</p>"},{"location":"reference/#sparse_autoencoder.NeuronActivityMetric","title":"<code>NeuronActivityMetric</code>","text":"<p>             Bases: <code>AbstractResampleMetric</code></p> <p>Neuron activity metric.</p> Source code in <code>sparse_autoencoder/metrics/resample/neuron_activity_metric.py</code> <pre><code>class NeuronActivityMetric(AbstractResampleMetric):\n    \"\"\"Neuron activity metric.\"\"\"\n\n    def calculate(self, data: ResampleMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the neuron activity metrics.\n\n        Args:\n            data: Resample metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n        neuron_activity = data.neuron_activity\n\n        # Histogram of neuron activity\n        numpy_neuron_activity: NDArray[np.float_] = neuron_activity.detach().cpu().numpy()\n        bins, values = np.histogram(numpy_neuron_activity, bins=50)\n        histogram = wandb.Histogram(np_histogram=(bins, values))\n\n        return {\n            \"resample_alive_neuron_count\": (neuron_activity &gt; 0).sum().item(),\n            \"resample_dead_neuron_count\": (neuron_activity == 0).sum().item(),\n            \"resample_neuron_activity_histogram\": histogram,\n        }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.NeuronActivityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the neuron activity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ResampleMetricData</code> <p>Resample metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/resample/neuron_activity_metric.py</code> <pre><code>def calculate(self, data: ResampleMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the neuron activity metrics.\n\n    Args:\n        data: Resample metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n    neuron_activity = data.neuron_activity\n\n    # Histogram of neuron activity\n    numpy_neuron_activity: NDArray[np.float_] = neuron_activity.detach().cpu().numpy()\n    bins, values = np.histogram(numpy_neuron_activity, bins=50)\n    histogram = wandb.Histogram(np_histogram=(bins, values))\n\n    return {\n        \"resample_alive_neuron_count\": (neuron_activity &gt; 0).sum().item(),\n        \"resample_dead_neuron_count\": (neuron_activity == 0).sum().item(),\n        \"resample_neuron_activity_histogram\": histogram,\n    }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p> <p>Includes all the key functionality to train a sparse autoencoder, with a specific set of     hyperparameters.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations.\n\n    Includes all the key functionality to train a sparse autoencoder, with a specific set of\n        hyperparameters.\n    \"\"\"\n\n    activation_resampler: AbstractActivationResampler | None\n    \"\"\"Activation resampler to use.\"\"\"\n\n    autoencoder: SparseAutoencoder\n    \"\"\"Sparse autoencoder to train.\"\"\"\n\n    cache_name: str\n    \"\"\"Name of the cache to use in the source model (hook point).\"\"\"\n\n    layer: int\n    \"\"\"Layer to get activations from with the source model.\"\"\"\n\n    log_frequency: int\n    \"\"\"Frequency at which to log metrics (in steps).\"\"\"\n\n    loss: AbstractLoss\n    \"\"\"Loss function to use.\"\"\"\n\n    metrics: MetricsContainer\n    \"\"\"Metrics to use.\"\"\"\n\n    optimizer: AbstractOptimizerWithReset\n    \"\"\"Optimizer to use.\"\"\"\n\n    progress_bar: tqdm | None\n    \"\"\"Progress bar for the pipeline.\"\"\"\n\n    source_data: Iterable[TorchTokenizedPrompts]\n    \"\"\"Iterable over the source data.\"\"\"\n\n    source_dataset: SourceDataset\n    \"\"\"Source dataset to generate activation data from (tokenized prompts).\"\"\"\n\n    source_model: HookedTransformer\n    \"\"\"Source model to get activations from.\"\"\"\n\n    total_training_steps: int = 1\n    \"\"\"Total number of training steps state.\"\"\"\n\n    @final\n    def __init__(  # noqa: PLR0913\n        self,\n        activation_resampler: AbstractActivationResampler | None,\n        autoencoder: SparseAutoencoder,\n        cache_name: str,\n        layer: int,\n        loss: AbstractLoss,\n        optimizer: AbstractOptimizerWithReset,\n        source_dataset: SourceDataset,\n        source_model: HookedTransformer,\n        checkpoint_directory: Path | None = None,\n        log_frequency: int = 100,\n        metrics: MetricsContainer = default_metrics,\n        source_data_batch_size: int = 12,\n    ) -&gt; None:\n        \"\"\"Initialize the pipeline.\n\n        Args:\n            activation_resampler: Activation resampler to use.\n            autoencoder: Sparse autoencoder to train.\n            cache_name: Name of the cache to use in the source model (hook point).\n            layer: Layer to get activations from with the source model.\n            loss: Loss function to use.\n            optimizer: Optimizer to use.\n            source_dataset: Source dataset to get data from.\n            source_model: Source model to get activations from.\n            checkpoint_directory: Directory to save checkpoints to.\n            log_frequency: Frequency at which to log metrics (in steps)\n            metrics: Metrics to use.\n            source_data_batch_size: Batch size for the source data.\n        \"\"\"\n        self.activation_resampler = activation_resampler\n        self.autoencoder = autoencoder\n        self.cache_name = cache_name\n        self.checkpoint_directory = checkpoint_directory\n        self.layer = layer\n        self.log_frequency = log_frequency\n        self.loss = loss\n        self.metrics = metrics\n        self.optimizer = optimizer\n        self.source_data_batch_size = source_data_batch_size\n        self.source_dataset = source_dataset\n        self.source_model = source_model\n\n        source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n        self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n\n    def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n        \"\"\"Generate activations.\n\n        Args:\n            store_size: Number of activations to generate.\n\n        Returns:\n            Activation store for the train section.\n\n        Raises:\n            ValueError: If the store size is not positive or is not divisible by the batch size.\n        \"\"\"\n        # Check the store size is positive and divisible by the batch size\n        if store_size &lt;= 0:\n            error_message = f\"Store size must be positive, got {store_size}\"\n            raise ValueError(error_message)\n        if store_size % self.source_data_batch_size != 0:\n            error_message = (\n                f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n                f\"got {store_size}\"\n            )\n            raise ValueError(error_message)\n\n        # Setup the store\n        num_neurons: int = self.autoencoder.n_input_features\n        source_model_device: torch.device = get_model_device(self.source_model)\n        store = TensorActivationStore(store_size, num_neurons)\n\n        # Add the hook to the model (will automatically store the activations every time the model\n        # runs)\n        self.source_model.remove_all_hook_fns()\n        hook = partial(store_activations_hook, store=store)\n        self.source_model.add_hook(self.cache_name, hook)\n\n        # Loop through the dataloader until the store reaches the desired size\n        with torch.no_grad():\n            for batch in self.source_data:\n                input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n                self.source_model.forward(\n                    input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n                )  # type: ignore (TLens is typed incorrectly)\n\n                if len(store) &gt;= store_size:\n                    break\n\n        self.source_model.remove_all_hook_fns()\n        store.shuffle()\n\n        return store\n\n    def train_autoencoder(\n        self, activation_store: TensorActivationStore, train_batch_size: int\n    ) -&gt; NeuronActivity:\n        \"\"\"Train the sparse autoencoder.\n\n        Args:\n            activation_store: Activation store from the generate section.\n            train_batch_size: Train batch size.\n\n        Returns:\n            Number of times each neuron fired.\n        \"\"\"\n        autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n        activations_dataloader = DataLoader(\n            activation_store,\n            batch_size=train_batch_size,\n        )\n\n        learned_activations_fired_count: NeuronActivity = torch.zeros(\n            self.autoencoder.n_learned_features, dtype=torch.int32, device=autoencoder_device\n        )\n\n        for store_batch in activations_dataloader:\n            # Zero the gradients\n            self.optimizer.zero_grad()\n\n            # Move the batch to the device (in place)\n            batch = store_batch.detach().to(autoencoder_device)\n\n            # Forward pass\n            learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n            # Get loss &amp; metrics\n            metrics = {}\n            total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n                batch, learned_activations, reconstructed_activations\n            )\n            metrics.update(loss_metrics)\n\n            with torch.no_grad():\n                for metric in self.metrics.train_metrics:\n                    calculated = metric.calculate(\n                        TrainMetricData(batch, learned_activations, reconstructed_activations)\n                    )\n                    metrics.update(calculated)\n\n            # Store count of how many neurons have fired\n            with torch.no_grad():\n                fired = learned_activations &gt; 0\n                learned_activations_fired_count.add_(fired.sum(dim=0))\n\n            # Backwards pass\n            total_loss.backward()\n            self.optimizer.step()\n            self.autoencoder.decoder.constrain_weights_unit_norm()\n\n            # Log\n            if wandb.run is not None and self.total_training_steps % self.log_frequency == 0:\n                wandb.log(\n                    data={**metrics, **loss_metrics}, step=self.total_training_steps, commit=True\n                )\n            self.total_training_steps += 1\n\n        return learned_activations_fired_count\n\n    def resample_neurons(\n        self,\n        activation_store: TensorActivationStore,\n        neuron_activity_sample_size: int,\n        neuron_activity: NeuronActivity,\n        train_batch_size: int,\n    ) -&gt; None:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            activation_store: Activation store.\n            neuron_activity_sample_size: Sample size for resampling.\n            neuron_activity: Number of times each neuron fired.\n            train_batch_size: Train batch size (also used for resampling).\n        \"\"\"\n        if self.activation_resampler is not None:\n            # Get the updates\n            parameter_updates = self.activation_resampler.resample_dead_neurons(\n                activation_store=activation_store,\n                autoencoder=self.autoencoder,\n                loss_fn=self.loss,\n                neuron_activity=neuron_activity,\n                train_batch_size=train_batch_size,\n                neuron_activity_sample_size=neuron_activity_sample_size,\n            )\n\n            # Update the weights and biases\n            self.autoencoder.encoder.update_dictionary_vectors(\n                parameter_updates.dead_neuron_indices,\n                parameter_updates.dead_encoder_weight_updates,\n            )\n            self.autoencoder.encoder.update_bias(\n                parameter_updates.dead_neuron_indices,\n                parameter_updates.dead_encoder_bias_updates,\n            )\n            self.autoencoder.decoder.update_dictionary_vectors(\n                parameter_updates.dead_neuron_indices,\n                parameter_updates.dead_decoder_weight_updates,\n            )\n\n            # Log any metrics\n            with torch.no_grad():\n                metrics = {}\n                if wandb.run is not None:\n                    for metric in self.metrics.resample_metrics:\n                        calculated = metric.calculate(\n                            ResampleMetricData(\n                                neuron_activity=neuron_activity,\n                            )\n                        )\n                        metrics.update(calculated)\n                    wandb.log(metrics, commit=False)\n\n            # Reset the optimizer (TODO: Consider resetting just the relevant parameters)\n            self.optimizer.reset_state_all_parameters()\n\n    def validate_sae(self, validation_number_activations: int) -&gt; None:\n        \"\"\"Get validation metrics.\n\n        Args:\n            validation_number_activations: Number of activations to use for validation.\n        \"\"\"\n        losses: list[float] = []\n        losses_with_reconstruction: list[float] = []\n        losses_with_zero_ablation: list[float] = []\n        source_model_device: torch.device = get_model_device(self.source_model)\n\n        for batch in self.source_data:\n            input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n\n            # Run a forward pass with and without the replaced activations\n            self.source_model.remove_all_hook_fns()\n            replacement_hook = partial(\n                replace_activations_hook, sparse_autoencoder=self.autoencoder\n            )\n\n            loss = self.source_model.forward(input_ids, return_type=\"loss\")\n            loss_with_reconstruction = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[\n                    (\n                        self.cache_name,\n                        replacement_hook,\n                    )\n                ],\n            )\n            loss_with_zero_ablation = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n            )\n\n            losses.append(loss.sum().item())\n            losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n            losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n            if len(losses) &gt;= validation_number_activations:\n                break\n\n        # Log\n        validation_data = ValidationMetricData(\n            source_model_loss=torch.tensor(losses),\n            source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n            source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n        )\n        for metric in self.metrics.validation_metrics:\n            calculated = metric.calculate(validation_data)\n            wandb.log(data=calculated, step=self.total_training_steps, commit=False)\n\n    @final\n    def save_checkpoint(self) -&gt; None:\n        \"\"\"Save the model as a checkpoint.\"\"\"\n        if self.checkpoint_directory:\n            file_path: Path = (\n                self.checkpoint_directory / f\"sae_state_dict-{self.total_training_steps}.pt\"\n            )\n            torch.save(self.autoencoder.state_dict(), file_path)\n\n    def run_pipeline(\n        self,\n        train_batch_size: int,\n        max_store_size: int,\n        max_activations: int,\n        resample_frequency: int,\n        validation_number_activations: int = 1024,\n        validate_frequency: int | None = None,\n        checkpoint_frequency: int | None = None,\n    ) -&gt; None:\n        \"\"\"Run the full training pipeline.\n\n        Args:\n            train_batch_size: Train batch size.\n            max_store_size: Maximum size of the activation store.\n            max_activations: Maximum total number of activations to train on (the original paper\n                used 8bn, although others have had success with 100m+).\n            resample_frequency: Frequency at which to resample dead neurons (the original paper used\n                every 200m).\n            validation_number_activations: Number of activations to use for validation.\n            validate_frequency: Frequency at which to get validation metrics.\n            checkpoint_frequency: Frequency at which to save a checkpoint.\n        \"\"\"\n        last_resampled: int = 0\n        neuron_activity_sample_size: int = 0\n        last_validated: int = 0\n        last_checkpoint: int = 0\n        total_activations: int = 0\n        neuron_activity: NeuronActivity = torch.zeros(self.autoencoder.n_learned_features)\n\n        self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n        # Get the store size\n        store_size: int = max_store_size - max_store_size % (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n\n        with tqdm(\n            desc=\"Activations trained on\",\n            total=max_activations,\n        ) as progress_bar:\n            for _ in range(0, max_activations, store_size):\n                # Generate\n                progress_bar.set_postfix({\"stage\": \"generate\"})\n                activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n                # Update the counters\n                num_activation_vectors_in_store = len(activation_store)\n                last_resampled += num_activation_vectors_in_store\n                last_validated += num_activation_vectors_in_store\n                last_checkpoint += num_activation_vectors_in_store\n                total_activations += num_activation_vectors_in_store\n                if wandb.run is not None:\n                    wandb.log({\"activations_generated\": total_activations}, commit=False)\n\n                # Train\n                progress_bar.set_postfix({\"stage\": \"train\"})\n                batch_neuron_activity: NeuronActivity = self.train_autoencoder(\n                    activation_store, train_batch_size=train_batch_size\n                )\n                detached_neuron_activity = batch_neuron_activity.detach().cpu()\n                is_second_half_resample: bool = last_resampled &gt; resample_frequency / 2\n                if is_second_half_resample:\n                    neuron_activity_sample_size += num_activation_vectors_in_store\n                    neuron_activity.add_(detached_neuron_activity)\n\n                # Resample dead neurons (if needed)\n                progress_bar.set_postfix({\"stage\": \"resample\"})\n                if last_resampled &gt;= resample_frequency and self.activation_resampler is not None:\n                    self.resample_neurons(\n                        activation_store=activation_store,\n                        neuron_activity_sample_size=neuron_activity_sample_size,\n                        neuron_activity=neuron_activity,\n                        train_batch_size=train_batch_size,\n                    )\n\n                    # Reset\n                    last_resampled = 0\n                    neuron_activity_sample_size = 0\n                    neuron_activity.zero_()\n\n                # Get validation metrics (if needed)\n                progress_bar.set_postfix({\"stage\": \"validate\"})\n                if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                    self.validate_sae(validation_number_activations)\n                    last_validated = 0\n\n                # Checkpoint (if needed)\n                progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n                if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                    last_checkpoint = 0\n                    self.save_checkpoint()\n\n                # Update the progress bar\n                progress_bar.update(store_size)\n\n    @staticmethod\n    def stateful_dataloader_iterable(\n        dataloader: DataLoader[TorchTokenizedPrompts]\n    ) -&gt; Iterable[TorchTokenizedPrompts]:\n        \"\"\"Create a stateful dataloader iterable.\n\n        Create an iterable that maintains it's position in the dataloader between loops.\n\n        Examples:\n            Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n            (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n            position from where the first loop left off.\n\n            &gt;&gt;&gt; from datasets import Dataset\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; def gen():\n            ...     yield {\"int\": 0}\n            ...     yield {\"int\": 1}\n            &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n            &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n            (tensor([0]), tensor([0]))\n\n            By contrast if you create a stateful iterable from the dataloader, each loop will get\n            different data.\n\n            &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n            &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n            (tensor([0]), tensor([1]))\n\n        Args:\n            dataloader: PyTorch DataLoader.\n\n        Returns:\n            Stateful iterable over the data in the dataloader.\n\n        Yields:\n            Data from the dataloader.\n        \"\"\"\n        yield from dataloader\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.activation_resampler","title":"<code>activation_resampler: AbstractActivationResampler | None = activation_resampler</code>  <code>instance-attribute</code>","text":"<p>Activation resampler to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.autoencoder","title":"<code>autoencoder: SparseAutoencoder = autoencoder</code>  <code>instance-attribute</code>","text":"<p>Sparse autoencoder to train.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.cache_name","title":"<code>cache_name: str = cache_name</code>  <code>instance-attribute</code>","text":"<p>Name of the cache to use in the source model (hook point).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.layer","title":"<code>layer: int = layer</code>  <code>instance-attribute</code>","text":"<p>Layer to get activations from with the source model.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.log_frequency","title":"<code>log_frequency: int = log_frequency</code>  <code>instance-attribute</code>","text":"<p>Frequency at which to log metrics (in steps).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.loss","title":"<code>loss: AbstractLoss = loss</code>  <code>instance-attribute</code>","text":"<p>Loss function to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.metrics","title":"<code>metrics: MetricsContainer = metrics</code>  <code>instance-attribute</code>","text":"<p>Metrics to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.optimizer","title":"<code>optimizer: AbstractOptimizerWithReset = optimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer to use.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.progress_bar","title":"<code>progress_bar: tqdm | None</code>  <code>instance-attribute</code>","text":"<p>Progress bar for the pipeline.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_data","title":"<code>source_data: Iterable[TorchTokenizedPrompts] = self.stateful_dataloader_iterable(source_dataloader)</code>  <code>instance-attribute</code>","text":"<p>Iterable over the source data.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_dataset","title":"<code>source_dataset: SourceDataset = source_dataset</code>  <code>instance-attribute</code>","text":"<p>Source dataset to generate activation data from (tokenized prompts).</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.source_model","title":"<code>source_model: HookedTransformer = source_model</code>  <code>instance-attribute</code>","text":"<p>Source model to get activations from.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.total_training_steps","title":"<code>total_training_steps: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Total number of training steps state.</p>"},{"location":"reference/#sparse_autoencoder.Pipeline.__init__","title":"<code>__init__(activation_resampler, autoencoder, cache_name, layer, loss, optimizer, source_dataset, source_model, checkpoint_directory=None, log_frequency=100, metrics=default_metrics, source_data_batch_size=12)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>activation_resampler</code> <code>AbstractActivationResampler | None</code> <p>Activation resampler to use.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder to train.</p> required <code>cache_name</code> <code>str</code> <p>Name of the cache to use in the source model (hook point).</p> required <code>layer</code> <code>int</code> <p>Layer to get activations from with the source model.</p> required <code>loss</code> <code>AbstractLoss</code> <p>Loss function to use.</p> required <code>optimizer</code> <code>AbstractOptimizerWithReset</code> <p>Optimizer to use.</p> required <code>source_dataset</code> <code>SourceDataset</code> <p>Source dataset to get data from.</p> required <code>source_model</code> <code>HookedTransformer</code> <p>Source model to get activations from.</p> required <code>checkpoint_directory</code> <code>Path | None</code> <p>Directory to save checkpoints to.</p> <code>None</code> <code>log_frequency</code> <code>int</code> <p>Frequency at which to log metrics (in steps)</p> <code>100</code> <code>metrics</code> <code>MetricsContainer</code> <p>Metrics to use.</p> <code>default_metrics</code> <code>source_data_batch_size</code> <code>int</code> <p>Batch size for the source data.</p> <code>12</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef __init__(  # noqa: PLR0913\n    self,\n    activation_resampler: AbstractActivationResampler | None,\n    autoencoder: SparseAutoencoder,\n    cache_name: str,\n    layer: int,\n    loss: AbstractLoss,\n    optimizer: AbstractOptimizerWithReset,\n    source_dataset: SourceDataset,\n    source_model: HookedTransformer,\n    checkpoint_directory: Path | None = None,\n    log_frequency: int = 100,\n    metrics: MetricsContainer = default_metrics,\n    source_data_batch_size: int = 12,\n) -&gt; None:\n    \"\"\"Initialize the pipeline.\n\n    Args:\n        activation_resampler: Activation resampler to use.\n        autoencoder: Sparse autoencoder to train.\n        cache_name: Name of the cache to use in the source model (hook point).\n        layer: Layer to get activations from with the source model.\n        loss: Loss function to use.\n        optimizer: Optimizer to use.\n        source_dataset: Source dataset to get data from.\n        source_model: Source model to get activations from.\n        checkpoint_directory: Directory to save checkpoints to.\n        log_frequency: Frequency at which to log metrics (in steps)\n        metrics: Metrics to use.\n        source_data_batch_size: Batch size for the source data.\n    \"\"\"\n    self.activation_resampler = activation_resampler\n    self.autoencoder = autoencoder\n    self.cache_name = cache_name\n    self.checkpoint_directory = checkpoint_directory\n    self.layer = layer\n    self.log_frequency = log_frequency\n    self.loss = loss\n    self.metrics = metrics\n    self.optimizer = optimizer\n    self.source_data_batch_size = source_data_batch_size\n    self.source_dataset = source_dataset\n    self.source_model = source_model\n\n    source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n    self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.generate_activations","title":"<code>generate_activations(store_size)</code>","text":"<p>Generate activations.</p> <p>Parameters:</p> Name Type Description Default <code>store_size</code> <code>int</code> <p>Number of activations to generate.</p> required <p>Returns:</p> Type Description <code>TensorActivationStore</code> <p>Activation store for the train section.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the store size is not positive or is not divisible by the batch size.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n    \"\"\"Generate activations.\n\n    Args:\n        store_size: Number of activations to generate.\n\n    Returns:\n        Activation store for the train section.\n\n    Raises:\n        ValueError: If the store size is not positive or is not divisible by the batch size.\n    \"\"\"\n    # Check the store size is positive and divisible by the batch size\n    if store_size &lt;= 0:\n        error_message = f\"Store size must be positive, got {store_size}\"\n        raise ValueError(error_message)\n    if store_size % self.source_data_batch_size != 0:\n        error_message = (\n            f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n            f\"got {store_size}\"\n        )\n        raise ValueError(error_message)\n\n    # Setup the store\n    num_neurons: int = self.autoencoder.n_input_features\n    source_model_device: torch.device = get_model_device(self.source_model)\n    store = TensorActivationStore(store_size, num_neurons)\n\n    # Add the hook to the model (will automatically store the activations every time the model\n    # runs)\n    self.source_model.remove_all_hook_fns()\n    hook = partial(store_activations_hook, store=store)\n    self.source_model.add_hook(self.cache_name, hook)\n\n    # Loop through the dataloader until the store reaches the desired size\n    with torch.no_grad():\n        for batch in self.source_data:\n            input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n            self.source_model.forward(\n                input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n            )  # type: ignore (TLens is typed incorrectly)\n\n            if len(store) &gt;= store_size:\n                break\n\n    self.source_model.remove_all_hook_fns()\n    store.shuffle()\n\n    return store\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.resample_neurons","title":"<code>resample_neurons(activation_store, neuron_activity_sample_size, neuron_activity, train_batch_size)</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store.</p> required <code>neuron_activity_sample_size</code> <code>int</code> <p>Sample size for resampling.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Number of times each neuron fired.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def resample_neurons(\n    self,\n    activation_store: TensorActivationStore,\n    neuron_activity_sample_size: int,\n    neuron_activity: NeuronActivity,\n    train_batch_size: int,\n) -&gt; None:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        activation_store: Activation store.\n        neuron_activity_sample_size: Sample size for resampling.\n        neuron_activity: Number of times each neuron fired.\n        train_batch_size: Train batch size (also used for resampling).\n    \"\"\"\n    if self.activation_resampler is not None:\n        # Get the updates\n        parameter_updates = self.activation_resampler.resample_dead_neurons(\n            activation_store=activation_store,\n            autoencoder=self.autoencoder,\n            loss_fn=self.loss,\n            neuron_activity=neuron_activity,\n            train_batch_size=train_batch_size,\n            neuron_activity_sample_size=neuron_activity_sample_size,\n        )\n\n        # Update the weights and biases\n        self.autoencoder.encoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_weight_updates,\n        )\n        self.autoencoder.encoder.update_bias(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_bias_updates,\n        )\n        self.autoencoder.decoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_decoder_weight_updates,\n        )\n\n        # Log any metrics\n        with torch.no_grad():\n            metrics = {}\n            if wandb.run is not None:\n                for metric in self.metrics.resample_metrics:\n                    calculated = metric.calculate(\n                        ResampleMetricData(\n                            neuron_activity=neuron_activity,\n                        )\n                    )\n                    metrics.update(calculated)\n                wandb.log(metrics, commit=False)\n\n        # Reset the optimizer (TODO: Consider resetting just the relevant parameters)\n        self.optimizer.reset_state_all_parameters()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.run_pipeline","title":"<code>run_pipeline(train_batch_size, max_store_size, max_activations, resample_frequency, validation_number_activations=1024, validate_frequency=None, checkpoint_frequency=None)</code>","text":"<p>Run the full training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <code>max_store_size</code> <code>int</code> <p>Maximum size of the activation store.</p> required <code>max_activations</code> <code>int</code> <p>Maximum total number of activations to train on (the original paper used 8bn, although others have had success with 100m+).</p> required <code>resample_frequency</code> <code>int</code> <p>Frequency at which to resample dead neurons (the original paper used every 200m).</p> required <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> <code>1024</code> <code>validate_frequency</code> <code>int | None</code> <p>Frequency at which to get validation metrics.</p> <code>None</code> <code>checkpoint_frequency</code> <code>int | None</code> <p>Frequency at which to save a checkpoint.</p> <code>None</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def run_pipeline(\n    self,\n    train_batch_size: int,\n    max_store_size: int,\n    max_activations: int,\n    resample_frequency: int,\n    validation_number_activations: int = 1024,\n    validate_frequency: int | None = None,\n    checkpoint_frequency: int | None = None,\n) -&gt; None:\n    \"\"\"Run the full training pipeline.\n\n    Args:\n        train_batch_size: Train batch size.\n        max_store_size: Maximum size of the activation store.\n        max_activations: Maximum total number of activations to train on (the original paper\n            used 8bn, although others have had success with 100m+).\n        resample_frequency: Frequency at which to resample dead neurons (the original paper used\n            every 200m).\n        validation_number_activations: Number of activations to use for validation.\n        validate_frequency: Frequency at which to get validation metrics.\n        checkpoint_frequency: Frequency at which to save a checkpoint.\n    \"\"\"\n    last_resampled: int = 0\n    neuron_activity_sample_size: int = 0\n    last_validated: int = 0\n    last_checkpoint: int = 0\n    total_activations: int = 0\n    neuron_activity: NeuronActivity = torch.zeros(self.autoencoder.n_learned_features)\n\n    self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n    # Get the store size\n    store_size: int = max_store_size - max_store_size % (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n\n    with tqdm(\n        desc=\"Activations trained on\",\n        total=max_activations,\n    ) as progress_bar:\n        for _ in range(0, max_activations, store_size):\n            # Generate\n            progress_bar.set_postfix({\"stage\": \"generate\"})\n            activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n            # Update the counters\n            num_activation_vectors_in_store = len(activation_store)\n            last_resampled += num_activation_vectors_in_store\n            last_validated += num_activation_vectors_in_store\n            last_checkpoint += num_activation_vectors_in_store\n            total_activations += num_activation_vectors_in_store\n            if wandb.run is not None:\n                wandb.log({\"activations_generated\": total_activations}, commit=False)\n\n            # Train\n            progress_bar.set_postfix({\"stage\": \"train\"})\n            batch_neuron_activity: NeuronActivity = self.train_autoencoder(\n                activation_store, train_batch_size=train_batch_size\n            )\n            detached_neuron_activity = batch_neuron_activity.detach().cpu()\n            is_second_half_resample: bool = last_resampled &gt; resample_frequency / 2\n            if is_second_half_resample:\n                neuron_activity_sample_size += num_activation_vectors_in_store\n                neuron_activity.add_(detached_neuron_activity)\n\n            # Resample dead neurons (if needed)\n            progress_bar.set_postfix({\"stage\": \"resample\"})\n            if last_resampled &gt;= resample_frequency and self.activation_resampler is not None:\n                self.resample_neurons(\n                    activation_store=activation_store,\n                    neuron_activity_sample_size=neuron_activity_sample_size,\n                    neuron_activity=neuron_activity,\n                    train_batch_size=train_batch_size,\n                )\n\n                # Reset\n                last_resampled = 0\n                neuron_activity_sample_size = 0\n                neuron_activity.zero_()\n\n            # Get validation metrics (if needed)\n            progress_bar.set_postfix({\"stage\": \"validate\"})\n            if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                self.validate_sae(validation_number_activations)\n                last_validated = 0\n\n            # Checkpoint (if needed)\n            progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n            if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                last_checkpoint = 0\n                self.save_checkpoint()\n\n            # Update the progress bar\n            progress_bar.update(store_size)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.save_checkpoint","title":"<code>save_checkpoint()</code>","text":"<p>Save the model as a checkpoint.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef save_checkpoint(self) -&gt; None:\n    \"\"\"Save the model as a checkpoint.\"\"\"\n    if self.checkpoint_directory:\n        file_path: Path = (\n            self.checkpoint_directory / f\"sae_state_dict-{self.total_training_steps}.pt\"\n        )\n        torch.save(self.autoencoder.state_dict(), file_path)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.stateful_dataloader_iterable","title":"<code>stateful_dataloader_iterable(dataloader)</code>  <code>staticmethod</code>","text":"<p>Create a stateful dataloader iterable.</p> <p>Create an iterable that maintains it's position in the dataloader between loops.</p> <p>Examples:</p> <p>Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off.</p> <pre><code>&gt;&gt;&gt; from datasets import Dataset\n&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; def gen():\n...     yield {\"int\": 0}\n...     yield {\"int\": 1}\n&gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n&gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n(tensor([0]), tensor([0]))\n</code></pre> <p>By contrast if you create a stateful iterable from the dataloader, each loop will get different data.</p> <pre><code>&gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n&gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n(tensor([0]), tensor([1]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader[TorchTokenizedPrompts]</code> <p>PyTorch DataLoader.</p> required <p>Returns:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Stateful iterable over the data in the dataloader.</p> <p>Yields:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Data from the dataloader.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@staticmethod\ndef stateful_dataloader_iterable(\n    dataloader: DataLoader[TorchTokenizedPrompts]\n) -&gt; Iterable[TorchTokenizedPrompts]:\n    \"\"\"Create a stateful dataloader iterable.\n\n    Create an iterable that maintains it's position in the dataloader between loops.\n\n    Examples:\n        Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n        (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n        position from where the first loop left off.\n\n        &gt;&gt;&gt; from datasets import Dataset\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; def gen():\n        ...     yield {\"int\": 0}\n        ...     yield {\"int\": 1}\n        &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n        &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n        (tensor([0]), tensor([0]))\n\n        By contrast if you create a stateful iterable from the dataloader, each loop will get\n        different data.\n\n        &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n        &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n        (tensor([0]), tensor([1]))\n\n    Args:\n        dataloader: PyTorch DataLoader.\n\n    Returns:\n        Stateful iterable over the data in the dataloader.\n\n    Yields:\n        Data from the dataloader.\n    \"\"\"\n    yield from dataloader\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.train_autoencoder","title":"<code>train_autoencoder(activation_store, train_batch_size)</code>","text":"<p>Train the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store from the generate section.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <p>Returns:</p> Type Description <code>NeuronActivity</code> <p>Number of times each neuron fired.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def train_autoencoder(\n    self, activation_store: TensorActivationStore, train_batch_size: int\n) -&gt; NeuronActivity:\n    \"\"\"Train the sparse autoencoder.\n\n    Args:\n        activation_store: Activation store from the generate section.\n        train_batch_size: Train batch size.\n\n    Returns:\n        Number of times each neuron fired.\n    \"\"\"\n    autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n    activations_dataloader = DataLoader(\n        activation_store,\n        batch_size=train_batch_size,\n    )\n\n    learned_activations_fired_count: NeuronActivity = torch.zeros(\n        self.autoencoder.n_learned_features, dtype=torch.int32, device=autoencoder_device\n    )\n\n    for store_batch in activations_dataloader:\n        # Zero the gradients\n        self.optimizer.zero_grad()\n\n        # Move the batch to the device (in place)\n        batch = store_batch.detach().to(autoencoder_device)\n\n        # Forward pass\n        learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n        # Get loss &amp; metrics\n        metrics = {}\n        total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n            batch, learned_activations, reconstructed_activations\n        )\n        metrics.update(loss_metrics)\n\n        with torch.no_grad():\n            for metric in self.metrics.train_metrics:\n                calculated = metric.calculate(\n                    TrainMetricData(batch, learned_activations, reconstructed_activations)\n                )\n                metrics.update(calculated)\n\n        # Store count of how many neurons have fired\n        with torch.no_grad():\n            fired = learned_activations &gt; 0\n            learned_activations_fired_count.add_(fired.sum(dim=0))\n\n        # Backwards pass\n        total_loss.backward()\n        self.optimizer.step()\n        self.autoencoder.decoder.constrain_weights_unit_norm()\n\n        # Log\n        if wandb.run is not None and self.total_training_steps % self.log_frequency == 0:\n            wandb.log(\n                data={**metrics, **loss_metrics}, step=self.total_training_steps, commit=True\n            )\n        self.total_training_steps += 1\n\n    return learned_activations_fired_count\n</code></pre>"},{"location":"reference/#sparse_autoencoder.Pipeline.validate_sae","title":"<code>validate_sae(validation_number_activations)</code>","text":"<p>Get validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def validate_sae(self, validation_number_activations: int) -&gt; None:\n    \"\"\"Get validation metrics.\n\n    Args:\n        validation_number_activations: Number of activations to use for validation.\n    \"\"\"\n    losses: list[float] = []\n    losses_with_reconstruction: list[float] = []\n    losses_with_zero_ablation: list[float] = []\n    source_model_device: torch.device = get_model_device(self.source_model)\n\n    for batch in self.source_data:\n        input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n\n        # Run a forward pass with and without the replaced activations\n        self.source_model.remove_all_hook_fns()\n        replacement_hook = partial(\n            replace_activations_hook, sparse_autoencoder=self.autoencoder\n        )\n\n        loss = self.source_model.forward(input_ids, return_type=\"loss\")\n        loss_with_reconstruction = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[\n                (\n                    self.cache_name,\n                    replacement_hook,\n                )\n            ],\n        )\n        loss_with_zero_ablation = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n        )\n\n        losses.append(loss.sum().item())\n        losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n        losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n        if len(losses) &gt;= validation_number_activations:\n            break\n\n    # Log\n    validation_data = ValidationMetricData(\n        source_model_loss=torch.tensor(losses),\n        source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n        source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n    )\n    for metric in self.metrics.validation_metrics:\n        calculated = metric.calculate(validation_data)\n        wandb.log(data=calculated, step=self.total_training_steps, commit=False)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset","title":"<code>PreTokenizedDataset</code>","text":"<p>             Bases: <code>SourceDataset[PreTokenizedDataBatch]</code></p> <p>General Pre-Tokenized Dataset from Hugging Face.</p> <p>Can be used for various datasets available on Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@final\nclass PreTokenizedDataset(SourceDataset[PreTokenizedDataBatch]):\n    \"\"\"General Pre-Tokenized Dataset from Hugging Face.\n\n    Can be used for various datasets available on Hugging Face.\n    \"\"\"\n\n    def preprocess(\n        self,\n        source_batch: PreTokenizedDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        The method splits each pre-tokenized item based on the context size.\n\n        Args:\n            source_batch: A batch of source data.\n            context_size: The context size to use for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n        # Chunk each tokenized prompt into blocks of context_size,\n        # discarding the last block if too small.\n        context_size_prompts = []\n        for encoding in tokenized_prompts:\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        dataset_path: str,\n        context_size: int = 256,\n        buffer_size: int = 1000,\n        preprocess_batch_size: int = 1000,\n        dataset_split: str = \"train\",\n    ):\n        \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face.\n            context_size: The context size for tokenized prompts.\n            buffer_size: Buffer size for shuffling the dataset.\n            preprocess_batch_size: Batch size for preprocessing.\n            dataset_split: Dataset split (e.g., `train`).\n        \"\"\"\n        super().__init__(\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            context_size=context_size,\n            buffer_size=buffer_size,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset.__init__","title":"<code>__init__(dataset_path, context_size=256, buffer_size=1000, preprocess_batch_size=1000, dataset_split='train')</code>","text":"<p>Initialize a pre-tokenized dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> required <code>context_size</code> <code>int</code> <p>The context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>int</code> <p>Buffer size for shuffling the dataset.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>int</code> <p>Batch size for preprocessing.</p> <code>1000</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., <code>train</code>).</p> <code>'train'</code> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_path: str,\n    context_size: int = 256,\n    buffer_size: int = 1000,\n    preprocess_batch_size: int = 1000,\n    dataset_split: str = \"train\",\n):\n    \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face.\n        context_size: The context size for tokenized prompts.\n        buffer_size: Buffer size for shuffling the dataset.\n        preprocess_batch_size: Batch size for preprocessing.\n        dataset_split: Dataset split (e.g., `train`).\n    \"\"\"\n    super().__init__(\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        context_size=context_size,\n        buffer_size=buffer_size,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.PreTokenizedDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>The method splits each pre-tokenized item based on the context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>PreTokenizedDataBatch</code> <p>A batch of source data.</p> required <code>context_size</code> <code>int</code> <p>The context size to use for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: PreTokenizedDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    The method splits each pre-tokenized item based on the context size.\n\n    Args:\n        source_batch: A batch of source data.\n        context_size: The context size to use for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n    # Chunk each tokenized prompt into blocks of context_size,\n    # discarding the last block if too small.\n    context_size_prompts = []\n    for encoding in tokenized_prompts:\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>AbstractAutoencoder</code></p> <p>Sparse Autoencoder Model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@final\nclass SparseAutoencoder(AbstractAutoencoder):\n    \"\"\"Sparse Autoencoder Model.\"\"\"\n\n    geometric_median_dataset: InputOutputActivationVector\n    \"\"\"Estimated Geometric Median of the Dataset.\n\n    Used for initialising :attr:`tied_bias`.\n    \"\"\"\n\n    tied_bias: InputOutputActivationBatch\n    \"\"\"Tied Bias Parameter.\n\n    The same bias is used pre-encoder and post-decoder.\n    \"\"\"\n\n    n_input_features: int\n    \"\"\"Number of Input Features.\"\"\"\n\n    n_learned_features: int\n    \"\"\"Number of Learned Features.\"\"\"\n\n    _pre_encoder_bias: TiedBias\n\n    _encoder: LinearEncoder\n\n    _decoder: UnitNormDecoder\n\n    _post_decoder_bias: TiedBias\n\n    @property\n    def pre_encoder_bias(self) -&gt; TiedBias:\n        \"\"\"Pre-encoder bias.\"\"\"\n        return self._pre_encoder_bias\n\n    @property\n    def encoder(self) -&gt; LinearEncoder:\n        \"\"\"Encoder.\"\"\"\n        return self._encoder\n\n    @property\n    def decoder(self) -&gt; UnitNormDecoder:\n        \"\"\"Decoder.\"\"\"\n        return self._decoder\n\n    @property\n    def post_decoder_bias(self) -&gt; TiedBias:\n        \"\"\"Post-decoder bias.\"\"\"\n        return self._post_decoder_bias\n\n    def __init__(\n        self,\n        n_input_features: int,\n        n_learned_features: int,\n        geometric_median_dataset: InputOutputActivationVector | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Sparse Autoencoder Model.\n\n        Args:\n            n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n                from TransformerLens).\n            n_learned_features: Number of learned features. The initial paper experimented with 1 to\n                256 times the number of input features, and primarily used a multiple of 8.\n            geometric_median_dataset: Estimated geometric median of the dataset.\n        \"\"\"\n        super().__init__()\n\n        self.n_input_features = n_input_features\n        self.n_learned_features = n_learned_features\n\n        # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n        # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n        if geometric_median_dataset is not None:\n            self.geometric_median_dataset = geometric_median_dataset.clone()\n            self.geometric_median_dataset.requires_grad = False\n        else:\n            self.geometric_median_dataset = torch.zeros(n_input_features)\n            self.geometric_median_dataset.requires_grad = False\n\n        # Initialize the tied bias\n        self.tied_bias = Parameter(torch.empty(n_input_features))\n        self.initialize_tied_parameters()\n\n        self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n        self._encoder = LinearEncoder(\n            input_features=n_input_features, learnt_features=n_learned_features\n        )\n\n        self._decoder = UnitNormDecoder(\n            learnt_features=n_learned_features, decoded_features=n_input_features\n        )\n\n        self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n\n    def forward(\n        self,\n        x: InputOutputActivationBatch,\n    ) -&gt; tuple[\n        LearnedActivationBatch,\n        InputOutputActivationBatch,\n    ]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n        x = self._pre_encoder_bias(x)\n        learned_activations = self._encoder(x)\n        x = self._decoder(learned_activations)\n        decoded_activations = self._post_decoder_bias(x)\n        return learned_activations, decoded_activations\n\n    def initialize_tied_parameters(self) -&gt; None:\n        \"\"\"Initialize the tied parameters.\"\"\"\n        # The tied bias is initialised as the geometric median of the dataset\n        self.tied_bias.data = self.geometric_median_dataset\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n        self.initialize_tied_parameters()\n        for module in self.network:\n            if \"reset_parameters\" in dir(module):\n                module.reset_parameters()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.decoder","title":"<code>decoder: UnitNormDecoder</code>  <code>property</code>","text":"<p>Decoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.encoder","title":"<code>encoder: LinearEncoder</code>  <code>property</code>","text":"<p>Encoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.geometric_median_dataset","title":"<code>geometric_median_dataset: InputOutputActivationVector</code>  <code>instance-attribute</code>","text":"<p>Estimated Geometric Median of the Dataset.</p> <p>Used for initialising :attr:<code>tied_bias</code>.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.n_input_features","title":"<code>n_input_features: int = n_input_features</code>  <code>instance-attribute</code>","text":"<p>Number of Input Features.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.n_learned_features","title":"<code>n_learned_features: int = n_learned_features</code>  <code>instance-attribute</code>","text":"<p>Number of Learned Features.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Post-decoder bias.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Pre-encoder bias.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.tied_bias","title":"<code>tied_bias: InputOutputActivationBatch = Parameter(torch.empty(n_input_features))</code>  <code>instance-attribute</code>","text":"<p>Tied Bias Parameter.</p> <p>The same bias is used pre-encoder and post-decoder.</p>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.__init__","title":"<code>__init__(n_input_features, n_learned_features, geometric_median_dataset=None)</code>","text":"<p>Initialize the Sparse Autoencoder Model.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_features</code> <code>int</code> <p>Number of input features (e.g. <code>d_mlp</code> if training on MLP activations from TransformerLens).</p> required <code>n_learned_features</code> <code>int</code> <p>Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8.</p> required <code>geometric_median_dataset</code> <code>InputOutputActivationVector | None</code> <p>Estimated geometric median of the dataset.</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def __init__(\n    self,\n    n_input_features: int,\n    n_learned_features: int,\n    geometric_median_dataset: InputOutputActivationVector | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Sparse Autoencoder Model.\n\n    Args:\n        n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n            from TransformerLens).\n        n_learned_features: Number of learned features. The initial paper experimented with 1 to\n            256 times the number of input features, and primarily used a multiple of 8.\n        geometric_median_dataset: Estimated geometric median of the dataset.\n    \"\"\"\n    super().__init__()\n\n    self.n_input_features = n_input_features\n    self.n_learned_features = n_learned_features\n\n    # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n    # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n    if geometric_median_dataset is not None:\n        self.geometric_median_dataset = geometric_median_dataset.clone()\n        self.geometric_median_dataset.requires_grad = False\n    else:\n        self.geometric_median_dataset = torch.zeros(n_input_features)\n        self.geometric_median_dataset.requires_grad = False\n\n    # Initialize the tied bias\n    self.tied_bias = Parameter(torch.empty(n_input_features))\n    self.initialize_tied_parameters()\n\n    self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n    self._encoder = LinearEncoder(\n        input_features=n_input_features, learnt_features=n_learned_features\n    )\n\n    self._decoder = UnitNormDecoder(\n        learnt_features=n_learned_features, decoded_features=n_input_features\n    )\n\n    self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>tuple[LearnedActivationBatch, InputOutputActivationBatch]</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def forward(\n    self,\n    x: InputOutputActivationBatch,\n) -&gt; tuple[\n    LearnedActivationBatch,\n    InputOutputActivationBatch,\n]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n    x = self._pre_encoder_bias(x)\n    learned_activations = self._encoder(x)\n    x = self._decoder(learned_activations)\n    decoded_activations = self._post_decoder_bias(x)\n    return learned_activations, decoded_activations\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.initialize_tied_parameters","title":"<code>initialize_tied_parameters()</code>","text":"<p>Initialize the tied parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def initialize_tied_parameters(self) -&gt; None:\n    \"\"\"Initialize the tied parameters.\"\"\"\n    # The tied bias is initialised as the geometric median of the dataset\n    self.tied_bias.data = self.geometric_median_dataset\n</code></pre>"},{"location":"reference/#sparse_autoencoder.SparseAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n    self.initialize_tied_parameters()\n    for module in self.network:\n        if \"reset_parameters\" in dir(module):\n            module.reset_parameters()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore","title":"<code>TensorActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Tensor Activation Store.</p> <p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n</code></pre> <p>Add a single activation vector to the dataset:</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a [batch, pos, neurons] activation tensor to the dataset:</p> <pre><code>&gt;&gt;&gt; store.empty()\n&gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n100\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>class TensorActivationStore(ActivationStore):\n    \"\"\"Tensor Activation Store.\n\n    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation\n    vectors to be stored to be known in advance. Multiprocess safe.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n\n    Add a single activation vector to the dataset:\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a [batch, pos, neurons] activation tensor to the dataset:\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        100\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: StoreActivations\n    \"\"\"Underlying Tensor Data Store.\"\"\"\n\n    items_stored: int = 0\n    \"\"\"Number of items stored.\"\"\"\n\n    max_items: int\n    \"\"\"Maximum Number of Items to Store.\"\"\"\n\n    def __init__(\n        self,\n        max_items: int,\n        num_neurons: int,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the Tensor Activation Store.\n\n        Args:\n            max_items: Maximum number of items to store (individual activation vectors)\n            num_neurons: Number of neurons in each activation vector.\n            device: Device to store the activation vectors on.\n        \"\"\"\n        self._data = torch.empty((max_items, num_neurons), device=device)\n        self._max_items = max_items\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return self.items_stored\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n            800\n\n        Returns:\n            The size of the underlying tensor in bytes.\n        \"\"\"\n        return self._data.element_size() * self._data.nelement()\n\n    def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        # Check in range\n        if index &gt;= self.items_stored:\n            msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n            raise IndexError(msg)\n\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n        &gt;&gt;&gt; store.append(torch.tensor([0.]))\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; [store[i].item() for i in range(3)]\n        [0.0, 2.0, 1.0]\n        \"\"\"\n        # Generate a permutation of the indices for the active data\n        perm = torch.randperm(self.items_stored)\n\n        # Use this permutation to shuffle the active data in-place\n        self._data[: self.items_stored] = self._data[perm]\n\n    def append(self, item: InputOutputActivationVector) -&gt; None:\n        \"\"\"Add a single item to the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            item: The item to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        if self.items_stored + 1 &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[self.items_stored] = item.to(\n            self._data.device,\n        )\n        self.items_stored += 1\n\n    def extend(self, batch: SourceModelActivations) -&gt; None:\n        \"\"\"Add a batch to the store.\n\n        Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n        &gt;&gt;&gt; store.items_stored\n        9\n\n        Args:\n            batch: The batch to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        reshaped: InputOutputActivationBatch = resize_to_single_item_dimension(\n            batch,\n        )\n\n        # Check we have space\n        num_activation_tensors: int = reshaped.shape[0]\n        if self.items_stored + num_activation_tensors &gt; self._max_items:\n            if reshaped.shape[0] &gt; self._max_items:\n                msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                    total maximum in the store of {self._max_items}.\"\n                raise ValueError(msg)\n\n            raise StoreFullError\n\n        self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n            self._data.device\n        )\n        self.items_stored += num_activation_tensors\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; store.items_stored\n        0\n        \"\"\"\n        # We don't need to zero the data, just reset the number of items stored\n        self.items_stored = 0\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.items_stored","title":"<code>items_stored: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of items stored.</p>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.max_items","title":"<code>max_items: int</code>  <code>instance-attribute</code>","text":"<p>Maximum Number of Items to Store.</p>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationVector</code> <p>The activation store item at the given index.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the index is out of range.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n\n    Raises:\n        IndexError: If the index is out of range.\n    \"\"\"\n    # Check in range\n    if index &gt;= self.items_stored:\n        msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n        raise IndexError(msg)\n\n    return self._data[index]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__init__","title":"<code>__init__(max_items, num_neurons, device=None)</code>","text":"<p>Initialise the Tensor Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>max_items</code> <code>int</code> <p>Maximum number of items to store (individual activation vectors)</p> required <code>num_neurons</code> <code>int</code> <p>Number of neurons in each activation vector.</p> required <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __init__(\n    self,\n    max_items: int,\n    num_neurons: int,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"Initialise the Tensor Activation Store.\n\n    Args:\n        max_items: Maximum number of items to store (individual activation vectors)\n        num_neurons: Number of neurons in each activation vector.\n        device: Device to store the activation vectors on.\n    \"\"\"\n    self._data = torch.empty((max_items, num_neurons), device=device)\n    self._max_items = max_items\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return self.items_stored\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> Example <p>import torch store = TensorActivationStore(max_items=2, num_neurons=100) store.sizeof() # Pre-allocated tensor of 2x100 800</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the underlying tensor in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n        800\n\n    Returns:\n        The size of the underlying tensor in bytes.\n    \"\"\"\n    return self._data.element_size() * self._data.nelement()\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a single item to the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>InputOutputActivationVector</code> <p>The item to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def append(self, item: InputOutputActivationVector) -&gt; None:\n    \"\"\"Add a single item to the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        item: The item to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    if self.items_stored + 1 &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[self.items_stored] = item.to(\n        self._data.device,\n    )\n    self.items_stored += 1\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; store.items_stored\n    0\n    \"\"\"\n    # We don't need to zero the data, just reset the number of items stored\n    self.items_stored = 0\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a batch to the store.</p> <p>Examples:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2</p> <p>store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SourceModelActivations</code> <p>The batch to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def extend(self, batch: SourceModelActivations) -&gt; None:\n    \"\"\"Add a batch to the store.\n\n    Examples:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n    &gt;&gt;&gt; store.items_stored\n    9\n\n    Args:\n        batch: The batch to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    reshaped: InputOutputActivationBatch = resize_to_single_item_dimension(\n        batch,\n    )\n\n    # Check we have space\n    num_activation_tensors: int = reshaped.shape[0]\n    if self.items_stored + num_activation_tensors &gt; self._max_items:\n        if reshaped.shape[0] &gt; self._max_items:\n            msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                total maximum in the store of {self._max_items}.\"\n            raise ValueError(msg)\n\n        raise StoreFullError\n\n    self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n        self._data.device\n    )\n    self.items_stored += num_activation_tensors\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TensorActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0]</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n    &gt;&gt;&gt; store.append(torch.tensor([0.]))\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; [store[i].item() for i in range(3)]\n    [0.0, 2.0, 1.0]\n    \"\"\"\n    # Generate a permutation of the indices for the active data\n    perm = torch.randperm(self.items_stored)\n\n    # Use this permutation to shuffle the active data in-place\n    self._data[: self.items_stored] = self._data[perm]\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset","title":"<code>TextDataset</code>","text":"<p>             Bases: <code>SourceDataset[GenericTextDataBatch]</code></p> <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@final\nclass TextDataset(SourceDataset[GenericTextDataBatch]):\n    \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n\n    def preprocess(\n        self,\n        source_batch: GenericTextDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n        Args:\n            source_batch: A batch of source data, including 'text' with a list of strings.\n            context_size: Context size for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        prompts: list[str] = source_batch[\"text\"]\n\n        tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n        # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n        context_size_prompts = []\n        for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizerBase,\n        context_size: int = 256,\n        buffer_size: int = 1000,\n        preprocess_batch_size: int = 1000,\n        dataset_path: str = \"monology/pile-uncopyrighted\",\n        dataset_split: str = \"train\",\n    ):\n        \"\"\"Initialize a generic text dataset from Hugging Face.\n\n        Args:\n            tokenizer: Tokenizer to process text data.\n            context_size: Context size for tokenized prompts.\n            buffer_size: Buffer size for shuffling the dataset.\n            preprocess_batch_size: Batch size for preprocessing.\n            dataset_path: Path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g., 'train').\n        \"\"\"\n        self.tokenizer = tokenizer\n\n        super().__init__(\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            context_size=context_size,\n            buffer_size=buffer_size,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.__init__","title":"<code>__init__(tokenizer, context_size=256, buffer_size=1000, preprocess_batch_size=1000, dataset_path='monology/pile-uncopyrighted', dataset_split='train')</code>","text":"<p>Initialize a generic text dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer to process text data.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>int</code> <p>Buffer size for shuffling the dataset.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>int</code> <p>Batch size for preprocessing.</p> <code>1000</code> <code>dataset_path</code> <code>str</code> <p>Path to the dataset on Hugging Face.</p> <code>'monology/pile-uncopyrighted'</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., 'train').</p> <code>'train'</code> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizerBase,\n    context_size: int = 256,\n    buffer_size: int = 1000,\n    preprocess_batch_size: int = 1000,\n    dataset_path: str = \"monology/pile-uncopyrighted\",\n    dataset_split: str = \"train\",\n):\n    \"\"\"Initialize a generic text dataset from Hugging Face.\n\n    Args:\n        tokenizer: Tokenizer to process text data.\n        context_size: Context size for tokenized prompts.\n        buffer_size: Buffer size for shuffling the dataset.\n        preprocess_batch_size: Batch size for preprocessing.\n        dataset_path: Path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g., 'train').\n    \"\"\"\n    self.tokenizer = tokenizer\n\n    super().__init__(\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        context_size=context_size,\n        buffer_size=buffer_size,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TextDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>GenericTextDataBatch</code> <p>A batch of source data, including 'text' with a list of strings.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: GenericTextDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n    Args:\n        source_batch: A batch of source data, including 'text' with a list of strings.\n        context_size: Context size for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    prompts: list[str] = source_batch[\"text\"]\n\n    tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n    # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n    context_size_prompts = []\n    for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric","title":"<code>TrainBatchFeatureDensityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Train batch feature density.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a training batch.</p> <p>Generally we want a small number of features to be active in each batch, so average feature density should be low. By contrast if the average feature density is high, it means that the features are not sparse enough.</p> Warning <p>This is not the same as the feature density of the entire training set. It's main use is tracking the progress of training.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>class TrainBatchFeatureDensityMetric(AbstractTrainMetric):\n    \"\"\"Train batch feature density.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a\n    training batch.\n\n    Generally we want a small number of features to be active in each batch, so average feature\n    density should be low. By contrast if the average feature density is high, it means that the\n    features are not sparse enough.\n\n    Warning:\n        This is not the same as the feature density of the entire training set. It's main use is\n        tracking the progress of training.\n    \"\"\"\n\n    threshold: float\n\n    def __init__(self, threshold: float = 0.0) -&gt; None:\n        \"\"\"Initialise the train batch feature density metric.\n\n        Args:\n            threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n                This should be close to zero.\n        \"\"\"\n        super().__init__()\n        self.threshold = threshold\n\n    def feature_density(self, activations: LearnedActivationBatch) -&gt; LearntActivationVector:\n        \"\"\"Count how many times each feature was active.\n\n        Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n            &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n            [1.0, 0.5, 0.0]\n\n        Args:\n            activations: Sample of cached activations (the Autoencoder's learned features).\n\n        Returns:\n            Number of times each feature was active in a sample.\n        \"\"\"\n        has_fired: LearnedActivationBatch = torch.gt(activations, self.threshold).to(\n            dtype=torch.float  # Move to float so it can be averaged\n        )\n\n        return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n\n    @staticmethod\n    def wandb_feature_density_histogram(\n        feature_density: LearntActivationVector,\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the feature density.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n        wandb_feature_density_histogram(feature_density)})`.\n\n        Args:\n            feature_density: Number of times each feature was active in a sample. Can be calculated\n                using :func:`feature_activity_count`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_feature_density, bins=50)\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the train batch feature density metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary with the train batch feature density metric, and a histogram of the feature\n            density.\n        \"\"\"\n        train_batch_feature_density: LearntActivationVector = self.feature_density(\n            data.learned_activations\n        )\n\n        train_batch_feature_density_histogram: wandb.Histogram = (\n            self.wandb_feature_density_histogram(train_batch_feature_density)\n        )\n\n        return {\n            \"train_batch_feature_density_histogram\": train_batch_feature_density_histogram,\n        }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.__init__","title":"<code>__init__(threshold=0.0)</code>","text":"<p>Initialise the train batch feature density metric.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero.</p> <code>0.0</code> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def __init__(self, threshold: float = 0.0) -&gt; None:\n    \"\"\"Initialise the train batch feature density metric.\n\n    Args:\n        threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n            This should be close to zero.\n    \"\"\"\n    super().__init__()\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the train batch feature density metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with the train batch feature density metric, and a histogram of the feature</p> <code>dict[str, Any]</code> <p>density.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the train batch feature density metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary with the train batch feature density metric, and a histogram of the feature\n        density.\n    \"\"\"\n    train_batch_feature_density: LearntActivationVector = self.feature_density(\n        data.learned_activations\n    )\n\n    train_batch_feature_density_histogram: wandb.Histogram = (\n        self.wandb_feature_density_histogram(train_batch_feature_density)\n    )\n\n    return {\n        \"train_batch_feature_density_histogram\": train_batch_feature_density_histogram,\n    }\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.feature_density","title":"<code>feature_density(activations)</code>","text":"<p>Count how many times each feature was active.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").</p> Example <p>import torch activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist() [1.0, 0.5, 0.0]</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>LearnedActivationBatch</code> <p>Sample of cached activations (the Autoencoder's learned features).</p> required <p>Returns:</p> Type Description <code>LearntActivationVector</code> <p>Number of times each feature was active in a sample.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def feature_density(self, activations: LearnedActivationBatch) -&gt; LearntActivationVector:\n    \"\"\"Count how many times each feature was active.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n        &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n        [1.0, 0.5, 0.0]\n\n    Args:\n        activations: Sample of cached activations (the Autoencoder's learned features).\n\n    Returns:\n        Number of times each feature was active in a sample.\n    \"\"\"\n    has_fired: LearnedActivationBatch = torch.gt(activations, self.threshold).to(\n        dtype=torch.float  # Move to float so it can be averaged\n    )\n\n    return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n</code></pre>"},{"location":"reference/#sparse_autoencoder.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram","title":"<code>wandb_feature_density_histogram(feature_density)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the feature density.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature_density</code> <code>LearntActivationVector</code> <p>Number of times each feature was active in a sample. Can be calculated using :func:<code>feature_activity_count</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>@staticmethod\ndef wandb_feature_density_histogram(\n    feature_density: LearntActivationVector,\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the feature density.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n    wandb_feature_density_histogram(feature_density)})`.\n\n    Args:\n        feature_density: Number of times each feature was active in a sample. Can be calculated\n            using :func:`feature_activity_count`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_feature_density, bins=50)\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>activation_resampler<ul> <li>abstract_activation_resampler</li> <li>activation_resampler</li> </ul> </li> <li>activation_store<ul> <li>base_store</li> <li>disk_store</li> <li>list_store</li> <li>tensor_store</li> <li>utils<ul> <li>extend_resize</li> </ul> </li> </ul> </li> <li>autoencoder<ul> <li>abstract_autoencoder</li> <li>components<ul> <li>abstract_decoder</li> <li>abstract_encoder</li> <li>abstract_outer_bias</li> <li>linear_encoder</li> <li>tied_bias</li> <li>unit_norm_decoder</li> </ul> </li> <li>model</li> </ul> </li> <li>loss<ul> <li>abstract_loss</li> <li>decoded_activations_l2</li> <li>learned_activations_l1</li> <li>reducer</li> </ul> </li> <li>metrics<ul> <li>generate<ul> <li>abstract_generate_metric</li> </ul> </li> <li>metrics_container</li> <li>resample<ul> <li>abstract_resample_metric</li> <li>neuron_activity_metric</li> </ul> </li> <li>train<ul> <li>abstract_train_metric</li> <li>capacity</li> <li>feature_density</li> <li>l0_norm_metric</li> </ul> </li> <li>validate<ul> <li>abstract_validate_metric</li> <li>model_reconstruction_score</li> </ul> </li> </ul> </li> <li>optimizer<ul> <li>abstract_optimizer</li> <li>adam_with_reset</li> </ul> </li> <li>source_data<ul> <li>abstract_dataset</li> <li>mock_dataset</li> <li>pretokenized_dataset</li> <li>text_dataset</li> </ul> </li> <li>source_model<ul> <li>replace_activations_hook</li> <li>store_activations_hook</li> <li>zero_ablate_hook</li> </ul> </li> <li>tensor_types</li> <li>train<ul> <li>pipeline</li> <li>sweep_config</li> <li>utils<ul> <li>get_model_device</li> <li>wandb_sweep_types</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/tensor_types/","title":"Tensor Types","text":"<p>Tensor Types.</p> <p>Tensor types with axis labels. Note that this uses the <code>jaxtyping</code> library, which works with PyTorch tensors as well. Note also that shape sizes are included in the docstrings as well as in the types as this is needed for IDEs such as VSCode to provide code hints.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.AliveEncoderWeights","title":"<code>AliveEncoderWeights: TypeAlias = Float[Tensor, Axis.dims(Axis.LEARNT_FEATURE, Axis.ALIVE_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Alive encoder weights.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.BatchTokenizedPrompts","title":"<code>BatchTokenizedPrompts: TypeAlias = Int[Tensor, Axis.dims(Axis.SOURCE_DATA_BATCH, Axis.POSITION)]</code>  <code>module-attribute</code>","text":"<p>Batch of tokenized prompts.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DeadDecoderNeuronWeightUpdates","title":"<code>DeadDecoderNeuronWeightUpdates: TypeAlias = Float[Tensor, Axis.dims(Axis.INPUT_OUTPUT_FEATURE, Axis.DEAD_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Dead decoder neuron weight updates.</p> <p>Shape (dead_feature, learnt_feature)</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DeadEncoderNeuronBiasUpdates","title":"<code>DeadEncoderNeuronBiasUpdates: TypeAlias = Float[Tensor, Axis.DEAD_FEATURE]</code>  <code>module-attribute</code>","text":"<p>Dead encoder neuron bias updates.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DeadEncoderNeuronWeightUpdates","title":"<code>DeadEncoderNeuronWeightUpdates: TypeAlias = Float[Tensor, Axis.dims(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Dead encoder neuron weight updates.</p> <p>Shape (learnt_feature, dead_feature)</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.DecoderWeights","title":"<code>DecoderWeights: TypeAlias = Float[Tensor, Axis.dims(Axis.INPUT_OUTPUT_FEATURE, Axis.LEARNT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Decoder weights.</p> <p>These weights form the decoder part of the autoencoder, which aims to reconstruct the original input data from the decompressed representation created by the encoder.</p> <p>Viewing the dictionary vectors in the context of reconstruction, they can be thought of as rows in this weight matrix.</p> <p>Shape: (input_output_feature_dim, learnt_feature_dim)</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.EncoderWeights","title":"<code>EncoderWeights: TypeAlias = Float[Tensor, Axis.dims(Axis.LEARNT_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Encoder weights.</p> <p>These weights are part of the encoder module of the autoencoder, responsible for decompressing the input data (activations from a source model) into a higher-dimensional representation.</p> <p>The dictionary vectors (basis vectors in the learnt feature space), they can be thought of as columns of this weight matrix, where each column corresponds to a particular feature in the lower-dimensional space. The sparsity constraint (hopefully) enforces that they respond relatively strongly to only a small portion of possible input vectors.</p> <p>Shape: (learnt_feature_dim, input_output_feature_dim)</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationBatch","title":"<code>InputOutputActivationBatch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Input/output activation batch.</p> <p>This is either a batch of input activation vectors from the source model, or a batch of decoded activation vectors from the autoencoder.</p> <p>Shape (batch, input_output_feature)</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.InputOutputActivationVector","title":"<code>InputOutputActivationVector: TypeAlias = Float[Tensor, Axis.INPUT_OUTPUT_FEATURE]</code>  <code>module-attribute</code>","text":"<p>Input/output activation vector.</p> <p>This is either a input activation vector from the source model, or a decoded activation vector from the autoencoder.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.InputOutputNeuronIndices","title":"<code>InputOutputNeuronIndices: TypeAlias = Int[Tensor, Axis.INPUT_OUTPUT_FEATURE]</code>  <code>module-attribute</code>","text":"<p>Input/output neuron indices.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.ItemTensor","title":"<code>ItemTensor: TypeAlias = Float[Tensor, Axis.SINGLE_ITEM]</code>  <code>module-attribute</code>","text":"<p>Single element item tensor.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.LearnedActivationBatch","title":"<code>LearnedActivationBatch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.LEARNT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Learned activation batch.</p> <p>This is a batch of activation vectors from the hidden (learnt) layer of the autoencoder. Typically the feature dimension is larger than the input/output activation vector.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.LearntActivationVector","title":"<code>LearntActivationVector: TypeAlias = Float[Tensor, Axis.LEARNT_FEATURE]</code>  <code>module-attribute</code>","text":"<p>Learned activation vector.</p> <p>Activation vector from the hidden (learnt) layer of the autoencoder. Typically this is larger than the input/output activation vector.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.LearntNeuronIndices","title":"<code>LearntNeuronIndices: TypeAlias = Int[Tensor, Axis.LEARNT_FEATURE_IDX]</code>  <code>module-attribute</code>","text":"<p>Learnt neuron indices.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.NeuronActivity","title":"<code>NeuronActivity: TypeAlias = Int[Tensor, Axis.LEARNT_FEATURE]</code>  <code>module-attribute</code>","text":"<p>Neuron activity.</p> <p>Number of times each neuron has fired (since the last reset).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.SampledDeadNeuronInputs","title":"<code>SampledDeadNeuronInputs: TypeAlias = Float[Tensor, Axis.dims(Axis.DEAD_FEATURE, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Sampled dead neuron inputs.</p> <p>Shape: (dead_feature, input_output_feature)</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.SourceModelActivations","title":"<code>SourceModelActivations: TypeAlias = Float[Tensor, Axis.dims(Axis.ANY, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Source model activations.</p> <p>Can have any number of proceeding dimensions (e.g. an attention head may generate activations of shape (batch_size, num_heads, seq_len, feature_dim).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.StoreActivations","title":"<code>StoreActivations: TypeAlias = Float[Tensor, Axis.dims(Axis.ITEMS, Axis.INPUT_OUTPUT_FEATURE)]</code>  <code>module-attribute</code>","text":"<p>Store of activation vectors.</p> <p>This is used to store large numbers of activation vectors from the source model.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.TrainBatchStatistic","title":"<code>TrainBatchStatistic: TypeAlias = Float[Tensor, Axis.BATCH]</code>  <code>module-attribute</code>","text":"<p>Train batch statistic.</p> <p>Contains one scalar value per item in the batch.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.ValidationStatistics","title":"<code>ValidationStatistics: TypeAlias = Float[Tensor, Axis.ITEMS]</code>  <code>module-attribute</code>","text":"<p>Validation statistics.</p> <p>Contains one scalar value per item in the validation set.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis","title":"<code>Axis</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Tensor axis names.</p> <p>Used to annotate tensor types.</p> Example <p>When used directly it prints a string:</p> <p>print(Axis.INPUT_OUTPUT_FEATURE) input_output_feature</p> <p>The primary use is to annotate tensor types:</p> <p>from jaxtyping import Float from torch import Tensor from typing import TypeAlias batch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)] print(batch)  <p>You can also join multiple axis together to represent the dimensions of a tensor:</p> <p>print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature</p> Source code in <code>sparse_autoencoder/tensor_types.py</code> <pre><code>class Axis(LowercaseStrEnum):\n    \"\"\"Tensor axis names.\n\n    Used to annotate tensor types.\n\n    Example:\n        When used directly it prints a string:\n\n        &gt;&gt;&gt; print(Axis.INPUT_OUTPUT_FEATURE)\n        input_output_feature\n\n        The primary use is to annotate tensor types:\n\n        &gt;&gt;&gt; from jaxtyping import Float\n        &gt;&gt;&gt; from torch import Tensor\n        &gt;&gt;&gt; from typing import TypeAlias\n        &gt;&gt;&gt; batch: TypeAlias = Float[Tensor, Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)]\n        &gt;&gt;&gt; print(batch)\n        &lt;class 'jaxtyping.Float[Tensor, 'batch input_output_feature']'&gt;\n\n        You can also join multiple axis together to represent the dimensions of a tensor:\n\n        &gt;&gt;&gt; print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n        batch input_output_feature\n    \"\"\"\n\n    # Batches\n    SOURCE_DATA_BATCH = auto()\n    \"\"\"Batch of prompts used to generate source model activations.\"\"\"\n\n    BATCH = auto()\n    \"\"\"Batch of items that the SAE is being trained on.\"\"\"\n\n    ITEMS = auto()\n    \"\"\"Arbitrary number of items.\"\"\"\n\n    # Features\n    INPUT_OUTPUT_FEATURE = auto()\n    \"\"\"Input or output feature (e.g. feature in activation vector from source model).\"\"\"\n\n    LEARNT_FEATURE = auto()\n    \"\"\"Learn feature (e.g. feature in learnt activation vector).\"\"\"\n\n    DEAD_FEATURE = auto()\n    \"\"\"Dead feature.\"\"\"\n\n    ALIVE_FEATURE = auto()\n    \"\"\"Alive feature.\"\"\"\n\n    # Feature indices\n    LEARNT_FEATURE_IDX = auto()\n\n    # Other\n    POSITION = auto()\n    \"\"\"Token position.\"\"\"\n\n    SINGLE_ITEM = \"\"\n    \"\"\"Single item axis.\"\"\"\n\n    ANY = \"*any\"\n    \"\"\"Any number of axis.\"\"\"\n\n    @staticmethod\n    def dims(*axis: \"Axis\") -&gt; str:\n        \"\"\"Join multiple axis together, to represent the dimensions of a tensor.\n\n        Example:\n            &gt;&gt;&gt; print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n            batch input_output_feature\n\n        Args:\n            *axis: Axis to join.\n\n        Returns:\n            Joined axis string.\n        \"\"\"\n        return \" \".join(axis)\n</code></pre>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ALIVE_FEATURE","title":"<code>ALIVE_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Alive feature.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ANY","title":"<code>ANY = '*any'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Any number of axis.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.BATCH","title":"<code>BATCH = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch of items that the SAE is being trained on.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.DEAD_FEATURE","title":"<code>DEAD_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dead feature.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.INPUT_OUTPUT_FEATURE","title":"<code>INPUT_OUTPUT_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Input or output feature (e.g. feature in activation vector from source model).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.ITEMS","title":"<code>ITEMS = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arbitrary number of items.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.LEARNT_FEATURE","title":"<code>LEARNT_FEATURE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learn feature (e.g. feature in learnt activation vector).</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.POSITION","title":"<code>POSITION = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Token position.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SINGLE_ITEM","title":"<code>SINGLE_ITEM = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Single item axis.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.SOURCE_DATA_BATCH","title":"<code>SOURCE_DATA_BATCH = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch of prompts used to generate source model activations.</p>"},{"location":"reference/tensor_types/#sparse_autoencoder.tensor_types.Axis.dims","title":"<code>dims(*axis)</code>  <code>staticmethod</code>","text":"<p>Join multiple axis together, to represent the dimensions of a tensor.</p> Example <p>print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE)) batch input_output_feature</p> <p>Parameters:</p> Name Type Description Default <code>*axis</code> <code>Axis</code> <p>Axis to join.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>Joined axis string.</p> Source code in <code>sparse_autoencoder/tensor_types.py</code> <pre><code>@staticmethod\ndef dims(*axis: \"Axis\") -&gt; str:\n    \"\"\"Join multiple axis together, to represent the dimensions of a tensor.\n\n    Example:\n        &gt;&gt;&gt; print(Axis.dims(Axis.BATCH, Axis.INPUT_OUTPUT_FEATURE))\n        batch input_output_feature\n\n    Args:\n        *axis: Axis to join.\n\n    Returns:\n        Joined axis string.\n    \"\"\"\n    return \" \".join(axis)\n</code></pre>"},{"location":"reference/activation_resampler/","title":"Activation Resampler","text":"<p>Activation Resampler.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/","title":"Abstract activation resampler","text":"<p>Abstract activation resampler.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler","title":"<code>AbstractActivationResampler</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract activation resampler.</p> Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code> <pre><code>class AbstractActivationResampler(ABC):\n    \"\"\"Abstract activation resampler.\"\"\"\n\n    _resample_dataset_size: int | None = None\n    \"\"\"Resample dataset size.\n\n    If none, will use the train dataset size.\n    \"\"\"\n\n    @final\n    def __init__(self, resample_dataset_size: int | None = None) -&gt; None:\n        \"\"\"Initialize the abstract activation resampler.\n\n        Args:\n            resample_dataset_size: Resample dataset size. If none, will use the train dataset size.\n        \"\"\"\n        self._resample_dataset_size = resample_dataset_size\n\n    @abstractmethod\n    def resample_dead_neurons(\n        self,\n        activation_store: TensorActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        neuron_activity_sample_size: int,\n        neuron_activity: NeuronActivity,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            neuron_activity_sample_size: Sample size for resampling.\n            neuron_activity: Number of times each neuron fired.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n        \"\"\"\n</code></pre>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler.__init__","title":"<code>__init__(resample_dataset_size=None)</code>","text":"<p>Initialize the abstract activation resampler.</p> <p>Parameters:</p> Name Type Description Default <code>resample_dataset_size</code> <code>int | None</code> <p>Resample dataset size. If none, will use the train dataset size.</p> <code>None</code> Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code> <pre><code>@final\ndef __init__(self, resample_dataset_size: int | None = None) -&gt; None:\n    \"\"\"Initialize the abstract activation resampler.\n\n    Args:\n        resample_dataset_size: Resample dataset size. If none, will use the train dataset size.\n    \"\"\"\n    self._resample_dataset_size = resample_dataset_size\n</code></pre>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.AbstractActivationResampler.resample_dead_neurons","title":"<code>resample_dead_neurons(activation_store, autoencoder, loss_fn, neuron_activity_sample_size, neuron_activity, train_batch_size)</code>  <code>abstractmethod</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>neuron_activity_sample_size</code> <code>int</code> <p>Sample size for resampling.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Number of times each neuron fired.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults</code> <p>Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</p> Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code> <pre><code>@abstractmethod\ndef resample_dead_neurons(\n    self,\n    activation_store: TensorActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    neuron_activity_sample_size: int,\n    neuron_activity: NeuronActivity,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        neuron_activity_sample_size: Sample size for resampling.\n        neuron_activity: Number of times each neuron fired.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n    \"\"\"\n</code></pre>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults","title":"<code>ParameterUpdateResults</code>  <code>dataclass</code>","text":"<p>Parameter update results from resampling dead neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/abstract_activation_resampler.py</code> <pre><code>@dataclass\nclass ParameterUpdateResults:\n    \"\"\"Parameter update results from resampling dead neurons.\"\"\"\n\n    dead_neuron_indices: LearntNeuronIndices\n    \"\"\"Dead neuron indices.\"\"\"\n\n    dead_encoder_weight_updates: DeadEncoderNeuronWeightUpdates\n    \"\"\"Dead encoder weight updates.\"\"\"\n\n    dead_encoder_bias_updates: DeadEncoderNeuronBiasUpdates\n    \"\"\"Dead encoder bias updates.\"\"\"\n\n    dead_decoder_weight_updates: DeadDecoderNeuronWeightUpdates\n    \"\"\"Dead decoder weight updates.\"\"\"\n</code></pre>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_decoder_weight_updates","title":"<code>dead_decoder_weight_updates: DeadDecoderNeuronWeightUpdates</code>  <code>instance-attribute</code>","text":"<p>Dead decoder weight updates.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_encoder_bias_updates","title":"<code>dead_encoder_bias_updates: DeadEncoderNeuronBiasUpdates</code>  <code>instance-attribute</code>","text":"<p>Dead encoder bias updates.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_encoder_weight_updates","title":"<code>dead_encoder_weight_updates: DeadEncoderNeuronWeightUpdates</code>  <code>instance-attribute</code>","text":"<p>Dead encoder weight updates.</p>"},{"location":"reference/activation_resampler/abstract_activation_resampler/#sparse_autoencoder.activation_resampler.abstract_activation_resampler.ParameterUpdateResults.dead_neuron_indices","title":"<code>dead_neuron_indices: LearntNeuronIndices</code>  <code>instance-attribute</code>","text":"<p>Dead neuron indices.</p>"},{"location":"reference/activation_resampler/activation_resampler/","title":"Activation resampler","text":"<p>Activation resampler.</p>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler","title":"<code>ActivationResampler</code>","text":"<p>             Bases: <code>AbstractActivationResampler</code></p> <p>Activation resampler.</p> <p>Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. The authors of Towards Monosemanticity: Decomposing Language Models With Dictionary Learning found that \u201cresampling\u201d these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.</p> <p>An interesting nuance around dead neurons involves the ultralow density cluster. They found that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone.</p> <p>This approach is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. This was done to minimize interference with the rest of the network.</p> Warning <p>The optimizer should be reset after applying this function, as the Adam state will be incorrect for the modified weights and biases.</p> <p>Note this approach is also known to create sudden loss spikes, and resampling too frequently causes training to diverge.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>class ActivationResampler(AbstractActivationResampler):\n    \"\"\"Activation resampler.\n\n    Over the course of training, a subset of autoencoder neurons will have zero activity across\n    a large number of datapoints. The authors of *Towards Monosemanticity: Decomposing Language\n    Models With Dictionary Learning* found that \u201cresampling\u201d these dead neurons during training\n    improves the number of likely-interpretable features (i.e., those in the high density cluster)\n    and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and\n    increase the number of chances the network has to find promising feature directions.\n\n    An interesting nuance around dead neurons involves the ultralow density cluster. They found that\n    if we increase the number of training steps then networks will kill off more of these ultralow\n    density neurons. This reinforces the use of the high density cluster as a useful metric because\n    there can exist neurons that are de facto dead but will not appear to be when looking at the\n    number of dead neurons alone.\n\n    This approach is designed to seed new features to fit inputs where the current autoencoder\n    performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled\n    neuron will only fire weakly for inputs similar to the one used for its reinitialization. This\n    was done to minimize interference with the rest of the network.\n\n    Warning:\n        The optimizer should be reset after applying this function, as the Adam state will be\n        incorrect for the modified weights and biases.\n\n        Note this approach is also known to create sudden loss spikes, and resampling too frequently\n        causes training to diverge.\n    \"\"\"\n\n    @staticmethod\n    def get_dead_neuron_indices(\n        neuron_activity_sample_size: int,\n        neuron_activity: NeuronActivity,\n        threshold: float = 0,\n    ) -&gt; LearntNeuronIndices:\n        \"\"\"Identify the indices of neurons that have zero activity.\n\n        Example:\n            &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 1])\n            &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(\n            ...     neuron_activity_sample_size=10,\n            ...     neuron_activity=neuron_activity,\n            ...     threshold=0.2\n            ... )\n            &gt;&gt;&gt; dead_neuron_indices.tolist()\n            [0, 1, 4]\n\n        Args:\n            neuron_activity_sample_size: Sample size for resampling.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            threshold: Threshold for determining if a neuron is dead (has fired less than this\n                portion of the sample size).\n\n        Returns:\n            A tensor containing the indices of neurons that are 'dead' (zero activity).\n        \"\"\"\n        threshold_activity: int = int(neuron_activity_sample_size * threshold)\n        return torch.where(neuron_activity &lt;= threshold_activity)[0]\n\n    def compute_loss_and_get_activations(\n        self,\n        store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        train_batch_size: int,\n    ) -&gt; tuple[TrainBatchStatistic, InputOutputActivationBatch]:\n        \"\"\"Compute the loss on a random subset of inputs.\n\n        Computes the loss and also stores the input activations (for use in resampling neurons).\n\n        Args:\n            store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            A tuple containing the loss per item, and all input activations.\n\n        Raises:\n            ValueError: If the number of items in the store is less than the number of inputs\n        \"\"\"\n        with torch.no_grad():\n            loss_batches: list[TrainBatchStatistic] = []\n            input_activations_batches: list[InputOutputActivationBatch] = []\n            dataloader = DataLoader(store, batch_size=train_batch_size)\n            num_inputs = self._resample_dataset_size or len(store)\n            batches: int = num_inputs // train_batch_size\n            model_device: torch.device = get_model_device(autoencoder)\n\n            for batch_idx, batch in enumerate(iter(dataloader)):\n                input_activations_batches.append(batch)\n                source_activations = batch.to(model_device)\n                learned_activations, reconstructed_activations = autoencoder(source_activations)\n                loss_batches.append(\n                    loss_fn.forward(\n                        source_activations, learned_activations, reconstructed_activations\n                    )\n                )\n                if batch_idx &gt;= batches:\n                    break\n\n            loss_result = torch.cat(loss_batches).to(model_device)\n            input_activations = torch.cat(input_activations_batches).to(model_device)\n\n            # Check we generated enough data\n            if len(loss_result) &lt; num_inputs:\n                error_message = (\n                    f\"Cannot get {num_inputs} items from the store, \"\n                    f\"as only {len(loss_result)} were available.\"\n                )\n                raise ValueError(error_message)\n\n            return loss_result, input_activations\n\n    @staticmethod\n    def assign_sampling_probabilities(loss: TrainBatchStatistic) -&gt; Tensor:\n        \"\"\"Assign the sampling probabilities for each input activations vector.\n\n        Assign each input vector a probability of being picked that is proportional to the square of\n        the autoencoder's loss on that input.\n\n        Example:\n            &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n            tensor([0.1000, 0.3000, 0.6000])\n\n        Args:\n            loss: Loss per item.\n\n        Returns:\n            A tensor of probabilities for each item.\n        \"\"\"\n        square_loss = loss.pow(2)\n        return square_loss / square_loss.sum()\n\n    @staticmethod\n    def sample_input(\n        probabilities: TrainBatchStatistic,\n        input_activations: InputOutputActivationBatch,\n        num_samples: int,\n    ) -&gt; SampledDeadNeuronInputs:\n        \"\"\"Sample an input vector based on the provided probabilities.\n\n        Example:\n            &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n            &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n            ...     probabilities, input_activations, 2\n            ... )\n            &gt;&gt;&gt; sampled_input.tolist()\n            [[5.0, 6.0], [3.0, 4.0]]\n\n        Args:\n            probabilities: Probabilities for each input.\n            input_activations: Input activation vectors.\n            num_samples: Number of samples to take (number of dead neurons).\n\n        Returns:\n            Sampled input activation vector.\n\n        Raises:\n            ValueError: If the number of samples is greater than the number of input activations.\n        \"\"\"\n        if num_samples &gt; len(input_activations):\n            exception_message = (\n                f\"Cannot sample {num_samples} inputs from \"\n                f\"{len(input_activations)} input activations.\"\n            )\n            raise ValueError(exception_message)\n\n        if num_samples == 0:\n            return torch.empty(\n                (0, input_activations.shape[-1]),\n                dtype=input_activations.dtype,\n                device=input_activations.device,\n            ).to(input_activations.device)\n\n        sample_indices: LearntNeuronIndices = torch.multinomial(\n            probabilities, num_samples=num_samples\n        )\n        return input_activations[sample_indices, :]\n\n    @staticmethod\n    def renormalize_and_scale(\n        sampled_input: SampledDeadNeuronInputs,\n        neuron_activity: NeuronActivity,\n        encoder_weight: EncoderWeights,\n    ) -&gt; DeadEncoderNeuronWeightUpdates:\n        \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n        Renormalize the input vector to equal the average norm of the encoder weights for alive\n        neurons times 0.2.\n\n        Example:\n            &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n            &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n            &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n            &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))\n            &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n            ...     sampled_input,\n            ...     neuron_activity,\n            ...     encoder_weight\n            ... )\n            &gt;&gt;&gt; rescaled_input.round(decimals=1)\n            tensor([[0.2000, 0.2000]])\n\n        Args:\n            sampled_input: Tensor of the sampled input activation.\n            neuron_activity: Tensor representing the number of times each neuron fired.\n            encoder_weight: Tensor of encoder weights.\n\n        Returns:\n            Rescaled sampled input.\n\n        Raises:\n            ValueError: If there are no alive neurons.\n        \"\"\"\n        alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n        # Check there is at least one alive neuron\n        if not torch.any(alive_neuron_mask):\n            error_message = \"No alive neurons found.\"\n            raise ValueError(error_message)\n\n        # Handle all alive neurons\n        if torch.all(alive_neuron_mask):\n            return torch.empty(\n                (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n            )\n\n        # Calculate the average norm of the encoder weights for alive neurons.\n        alive_encoder_weights: AliveEncoderWeights = encoder_weight[alive_neuron_mask, :]\n        average_alive_norm: ItemTensor = alive_encoder_weights.norm(dim=-1).mean()\n\n        # Renormalize the input vector to equal the average norm of the encoder weights for alive\n        # neurons times 0.2.\n        renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n            sampled_input, dim=-1\n        )\n        return renormalized_input * (average_alive_norm * 0.2)\n\n    def resample_dead_neurons(\n        self,\n        activation_store: ActivationStore,\n        autoencoder: SparseAutoencoder,\n        loss_fn: AbstractLoss,\n        neuron_activity_sample_size: int,\n        neuron_activity: NeuronActivity,\n        train_batch_size: int,\n    ) -&gt; ParameterUpdateResults:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            activation_store: Activation store.\n            autoencoder: Sparse autoencoder model.\n            loss_fn: Loss function.\n            neuron_activity_sample_size: Sample size for resampling.\n            neuron_activity: Number of times each neuron fired.\n            train_batch_size: Train batch size (also used for resampling).\n\n        Returns:\n            Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n        \"\"\"\n        with torch.no_grad():\n            dead_neuron_indices = self.get_dead_neuron_indices(\n                neuron_activity=neuron_activity,\n                neuron_activity_sample_size=neuron_activity_sample_size,\n            )\n\n            # Compute the loss for the current model on a random subset of inputs and get the\n            # activations.\n            loss, input_activations = self.compute_loss_and_get_activations(\n                store=activation_store,\n                autoencoder=autoencoder,\n                loss_fn=loss_fn,\n                train_batch_size=train_batch_size,\n            )\n\n            # Assign each input vector a probability of being picked that is proportional to the\n            # square of the autoencoder's loss on that input.\n            sample_probabilities: TrainBatchStatistic = self.assign_sampling_probabilities(loss)\n\n            # Get references to the encoder and decoder parameters\n            encoder_weight: EncoderWeights = autoencoder.encoder.weight\n\n            # For each dead neuron sample an input according to these probabilities.\n            sampled_input: SampledDeadNeuronInputs = self.sample_input(\n                sample_probabilities, input_activations, len(dead_neuron_indices)\n            )\n\n            # Renormalize the input vector to have unit L2 norm and set this to be the dictionary\n            # vector for the dead autoencoder neuron.\n            renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n                sampled_input, dim=-1\n            )\n\n            dead_decoder_weight_updates = rearrange(\n                renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n            )\n\n            # For the corresponding encoder vector, renormalize the input vector to equal the\n            # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n            # encoder bias element to zero.\n            rescaled_sampled_input = self.renormalize_and_scale(\n                sampled_input, neuron_activity, encoder_weight\n            )\n            dead_encoder_bias_updates = torch.zeros_like(\n                dead_neuron_indices,\n                dtype=dead_decoder_weight_updates.dtype,\n                device=dead_decoder_weight_updates.device,\n            )\n\n            return ParameterUpdateResults(\n                dead_neuron_indices=dead_neuron_indices,\n                dead_encoder_weight_updates=rescaled_sampled_input,\n                dead_encoder_bias_updates=dead_encoder_bias_updates,\n                dead_decoder_weight_updates=dead_decoder_weight_updates,\n            )\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.assign_sampling_probabilities","title":"<code>assign_sampling_probabilities(loss)</code>  <code>staticmethod</code>","text":"<p>Assign the sampling probabilities for each input activations vector.</p> <p>Assign each input vector a probability of being picked that is proportional to the square of the autoencoder's loss on that input.</p> Example <p>loss = torch.tensor([1.0, 2.0, 3.0]) ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1) tensor([0.1000, 0.3000, 0.6000])</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>TrainBatchStatistic</code> <p>Loss per item.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of probabilities for each item.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef assign_sampling_probabilities(loss: TrainBatchStatistic) -&gt; Tensor:\n    \"\"\"Assign the sampling probabilities for each input activations vector.\n\n    Assign each input vector a probability of being picked that is proportional to the square of\n    the autoencoder's loss on that input.\n\n    Example:\n        &gt;&gt;&gt; loss = torch.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; ActivationResampler.assign_sampling_probabilities(loss).round(decimals=1)\n        tensor([0.1000, 0.3000, 0.6000])\n\n    Args:\n        loss: Loss per item.\n\n    Returns:\n        A tensor of probabilities for each item.\n    \"\"\"\n    square_loss = loss.pow(2)\n    return square_loss / square_loss.sum()\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.compute_loss_and_get_activations","title":"<code>compute_loss_and_get_activations(store, autoencoder, loss_fn, train_batch_size)</code>","text":"<p>Compute the loss on a random subset of inputs.</p> <p>Computes the loss and also stores the input activations (for use in resampling neurons).</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>tuple[TrainBatchStatistic, InputOutputActivationBatch]</code> <p>A tuple containing the loss per item, and all input activations.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of items in the store is less than the number of inputs</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def compute_loss_and_get_activations(\n    self,\n    store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    train_batch_size: int,\n) -&gt; tuple[TrainBatchStatistic, InputOutputActivationBatch]:\n    \"\"\"Compute the loss on a random subset of inputs.\n\n    Computes the loss and also stores the input activations (for use in resampling neurons).\n\n    Args:\n        store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        A tuple containing the loss per item, and all input activations.\n\n    Raises:\n        ValueError: If the number of items in the store is less than the number of inputs\n    \"\"\"\n    with torch.no_grad():\n        loss_batches: list[TrainBatchStatistic] = []\n        input_activations_batches: list[InputOutputActivationBatch] = []\n        dataloader = DataLoader(store, batch_size=train_batch_size)\n        num_inputs = self._resample_dataset_size or len(store)\n        batches: int = num_inputs // train_batch_size\n        model_device: torch.device = get_model_device(autoencoder)\n\n        for batch_idx, batch in enumerate(iter(dataloader)):\n            input_activations_batches.append(batch)\n            source_activations = batch.to(model_device)\n            learned_activations, reconstructed_activations = autoencoder(source_activations)\n            loss_batches.append(\n                loss_fn.forward(\n                    source_activations, learned_activations, reconstructed_activations\n                )\n            )\n            if batch_idx &gt;= batches:\n                break\n\n        loss_result = torch.cat(loss_batches).to(model_device)\n        input_activations = torch.cat(input_activations_batches).to(model_device)\n\n        # Check we generated enough data\n        if len(loss_result) &lt; num_inputs:\n            error_message = (\n                f\"Cannot get {num_inputs} items from the store, \"\n                f\"as only {len(loss_result)} were available.\"\n            )\n            raise ValueError(error_message)\n\n        return loss_result, input_activations\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.get_dead_neuron_indices","title":"<code>get_dead_neuron_indices(neuron_activity_sample_size, neuron_activity, threshold=0)</code>  <code>staticmethod</code>","text":"<p>Identify the indices of neurons that have zero activity.</p> Example <p>neuron_activity = torch.tensor([0, 0, 3, 10, 1]) dead_neuron_indices = ActivationResampler.get_dead_neuron_indices( ...     neuron_activity_sample_size=10, ...     neuron_activity=neuron_activity, ...     threshold=0.2 ... ) dead_neuron_indices.tolist() [0, 1, 4]</p> <p>Parameters:</p> Name Type Description Default <code>neuron_activity_sample_size</code> <code>int</code> <p>Sample size for resampling.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>threshold</code> <code>float</code> <p>Threshold for determining if a neuron is dead (has fired less than this portion of the sample size).</p> <code>0</code> <p>Returns:</p> Type Description <code>LearntNeuronIndices</code> <p>A tensor containing the indices of neurons that are 'dead' (zero activity).</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef get_dead_neuron_indices(\n    neuron_activity_sample_size: int,\n    neuron_activity: NeuronActivity,\n    threshold: float = 0,\n) -&gt; LearntNeuronIndices:\n    \"\"\"Identify the indices of neurons that have zero activity.\n\n    Example:\n        &gt;&gt;&gt; neuron_activity = torch.tensor([0, 0, 3, 10, 1])\n        &gt;&gt;&gt; dead_neuron_indices = ActivationResampler.get_dead_neuron_indices(\n        ...     neuron_activity_sample_size=10,\n        ...     neuron_activity=neuron_activity,\n        ...     threshold=0.2\n        ... )\n        &gt;&gt;&gt; dead_neuron_indices.tolist()\n        [0, 1, 4]\n\n    Args:\n        neuron_activity_sample_size: Sample size for resampling.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        threshold: Threshold for determining if a neuron is dead (has fired less than this\n            portion of the sample size).\n\n    Returns:\n        A tensor containing the indices of neurons that are 'dead' (zero activity).\n    \"\"\"\n    threshold_activity: int = int(neuron_activity_sample_size * threshold)\n    return torch.where(neuron_activity &lt;= threshold_activity)[0]\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.renormalize_and_scale","title":"<code>renormalize_and_scale(sampled_input, neuron_activity, encoder_weight)</code>  <code>staticmethod</code>","text":"<p>Renormalize and scale the resampled dictionary vectors.</p> <p>Renormalize the input vector to equal the average norm of the encoder weights for alive neurons times 0.2.</p> Example <p>_seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = torch.tensor([[3.0, 4.0]]) neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3]) encoder_weight = torch.ones((6, 2)) rescaled_input = ActivationResampler.renormalize_and_scale( ...     sampled_input, ...     neuron_activity, ...     encoder_weight ... ) rescaled_input.round(decimals=1) tensor([[0.2000, 0.2000]])</p> <p>Parameters:</p> Name Type Description Default <code>sampled_input</code> <code>SampledDeadNeuronInputs</code> <p>Tensor of the sampled input activation.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Tensor representing the number of times each neuron fired.</p> required <code>encoder_weight</code> <code>EncoderWeights</code> <p>Tensor of encoder weights.</p> required <p>Returns:</p> Type Description <code>DeadEncoderNeuronWeightUpdates</code> <p>Rescaled sampled input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are no alive neurons.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef renormalize_and_scale(\n    sampled_input: SampledDeadNeuronInputs,\n    neuron_activity: NeuronActivity,\n    encoder_weight: EncoderWeights,\n) -&gt; DeadEncoderNeuronWeightUpdates:\n    \"\"\"Renormalize and scale the resampled dictionary vectors.\n\n    Renormalize the input vector to equal the average norm of the encoder weights for alive\n    neurons times 0.2.\n\n    Example:\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = torch.tensor([[3.0, 4.0]])\n        &gt;&gt;&gt; neuron_activity = torch.tensor([3, 0, 5, 0, 1, 3])\n        &gt;&gt;&gt; encoder_weight = torch.ones((6, 2))\n        &gt;&gt;&gt; rescaled_input = ActivationResampler.renormalize_and_scale(\n        ...     sampled_input,\n        ...     neuron_activity,\n        ...     encoder_weight\n        ... )\n        &gt;&gt;&gt; rescaled_input.round(decimals=1)\n        tensor([[0.2000, 0.2000]])\n\n    Args:\n        sampled_input: Tensor of the sampled input activation.\n        neuron_activity: Tensor representing the number of times each neuron fired.\n        encoder_weight: Tensor of encoder weights.\n\n    Returns:\n        Rescaled sampled input.\n\n    Raises:\n        ValueError: If there are no alive neurons.\n    \"\"\"\n    alive_neuron_mask: Bool[Tensor, \" learned_features\"] = neuron_activity &gt; 0\n\n    # Check there is at least one alive neuron\n    if not torch.any(alive_neuron_mask):\n        error_message = \"No alive neurons found.\"\n        raise ValueError(error_message)\n\n    # Handle all alive neurons\n    if torch.all(alive_neuron_mask):\n        return torch.empty(\n            (0, sampled_input.shape[-1]), dtype=sampled_input.dtype, device=sampled_input.device\n        )\n\n    # Calculate the average norm of the encoder weights for alive neurons.\n    alive_encoder_weights: AliveEncoderWeights = encoder_weight[alive_neuron_mask, :]\n    average_alive_norm: ItemTensor = alive_encoder_weights.norm(dim=-1).mean()\n\n    # Renormalize the input vector to equal the average norm of the encoder weights for alive\n    # neurons times 0.2.\n    renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n        sampled_input, dim=-1\n    )\n    return renormalized_input * (average_alive_norm * 0.2)\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.resample_dead_neurons","title":"<code>resample_dead_neurons(activation_store, autoencoder, loss_fn, neuron_activity_sample_size, neuron_activity, train_batch_size)</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>ActivationStore</code> <p>Activation store.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder model.</p> required <code>loss_fn</code> <code>AbstractLoss</code> <p>Loss function.</p> required <code>neuron_activity_sample_size</code> <code>int</code> <p>Sample size for resampling.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Number of times each neuron fired.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required <p>Returns:</p> Type Description <code>ParameterUpdateResults</code> <p>Indices of dead neurons, and the updates for the encoder and decoder weights and biases.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>def resample_dead_neurons(\n    self,\n    activation_store: ActivationStore,\n    autoencoder: SparseAutoencoder,\n    loss_fn: AbstractLoss,\n    neuron_activity_sample_size: int,\n    neuron_activity: NeuronActivity,\n    train_batch_size: int,\n) -&gt; ParameterUpdateResults:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        activation_store: Activation store.\n        autoencoder: Sparse autoencoder model.\n        loss_fn: Loss function.\n        neuron_activity_sample_size: Sample size for resampling.\n        neuron_activity: Number of times each neuron fired.\n        train_batch_size: Train batch size (also used for resampling).\n\n    Returns:\n        Indices of dead neurons, and the updates for the encoder and decoder weights and biases.\n    \"\"\"\n    with torch.no_grad():\n        dead_neuron_indices = self.get_dead_neuron_indices(\n            neuron_activity=neuron_activity,\n            neuron_activity_sample_size=neuron_activity_sample_size,\n        )\n\n        # Compute the loss for the current model on a random subset of inputs and get the\n        # activations.\n        loss, input_activations = self.compute_loss_and_get_activations(\n            store=activation_store,\n            autoencoder=autoencoder,\n            loss_fn=loss_fn,\n            train_batch_size=train_batch_size,\n        )\n\n        # Assign each input vector a probability of being picked that is proportional to the\n        # square of the autoencoder's loss on that input.\n        sample_probabilities: TrainBatchStatistic = self.assign_sampling_probabilities(loss)\n\n        # Get references to the encoder and decoder parameters\n        encoder_weight: EncoderWeights = autoencoder.encoder.weight\n\n        # For each dead neuron sample an input according to these probabilities.\n        sampled_input: SampledDeadNeuronInputs = self.sample_input(\n            sample_probabilities, input_activations, len(dead_neuron_indices)\n        )\n\n        # Renormalize the input vector to have unit L2 norm and set this to be the dictionary\n        # vector for the dead autoencoder neuron.\n        renormalized_input: SampledDeadNeuronInputs = torch.nn.functional.normalize(\n            sampled_input, dim=-1\n        )\n\n        dead_decoder_weight_updates = rearrange(\n            renormalized_input, \"dead_neuron input_feature -&gt; input_feature dead_neuron\"\n        )\n\n        # For the corresponding encoder vector, renormalize the input vector to equal the\n        # average norm of the encoder weights for alive neurons times 0.2. Set the corresponding\n        # encoder bias element to zero.\n        rescaled_sampled_input = self.renormalize_and_scale(\n            sampled_input, neuron_activity, encoder_weight\n        )\n        dead_encoder_bias_updates = torch.zeros_like(\n            dead_neuron_indices,\n            dtype=dead_decoder_weight_updates.dtype,\n            device=dead_decoder_weight_updates.device,\n        )\n\n        return ParameterUpdateResults(\n            dead_neuron_indices=dead_neuron_indices,\n            dead_encoder_weight_updates=rescaled_sampled_input,\n            dead_encoder_bias_updates=dead_encoder_bias_updates,\n            dead_decoder_weight_updates=dead_decoder_weight_updates,\n        )\n</code></pre>"},{"location":"reference/activation_resampler/activation_resampler/#sparse_autoencoder.activation_resampler.activation_resampler.ActivationResampler.sample_input","title":"<code>sample_input(probabilities, input_activations, num_samples)</code>  <code>staticmethod</code>","text":"<p>Sample an input vector based on the provided probabilities.</p> Example <p>probabilities = torch.tensor([0.1, 0.2, 0.7]) input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) _seed = torch.manual_seed(0)  # For reproducibility in example sampled_input = ActivationResampler.sample_input( ...     probabilities, input_activations, 2 ... ) sampled_input.tolist() [[5.0, 6.0], [3.0, 4.0]]</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>TrainBatchStatistic</code> <p>Probabilities for each input.</p> required <code>input_activations</code> <code>InputOutputActivationBatch</code> <p>Input activation vectors.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to take (number of dead neurons).</p> required <p>Returns:</p> Type Description <code>SampledDeadNeuronInputs</code> <p>Sampled input activation vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of samples is greater than the number of input activations.</p> Source code in <code>sparse_autoencoder/activation_resampler/activation_resampler.py</code> <pre><code>@staticmethod\ndef sample_input(\n    probabilities: TrainBatchStatistic,\n    input_activations: InputOutputActivationBatch,\n    num_samples: int,\n) -&gt; SampledDeadNeuronInputs:\n    \"\"\"Sample an input vector based on the provided probabilities.\n\n    Example:\n        &gt;&gt;&gt; probabilities = torch.tensor([0.1, 0.2, 0.7])\n        &gt;&gt;&gt; input_activations = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n        &gt;&gt;&gt; _seed = torch.manual_seed(0)  # For reproducibility in example\n        &gt;&gt;&gt; sampled_input = ActivationResampler.sample_input(\n        ...     probabilities, input_activations, 2\n        ... )\n        &gt;&gt;&gt; sampled_input.tolist()\n        [[5.0, 6.0], [3.0, 4.0]]\n\n    Args:\n        probabilities: Probabilities for each input.\n        input_activations: Input activation vectors.\n        num_samples: Number of samples to take (number of dead neurons).\n\n    Returns:\n        Sampled input activation vector.\n\n    Raises:\n        ValueError: If the number of samples is greater than the number of input activations.\n    \"\"\"\n    if num_samples &gt; len(input_activations):\n        exception_message = (\n            f\"Cannot sample {num_samples} inputs from \"\n            f\"{len(input_activations)} input activations.\"\n        )\n        raise ValueError(exception_message)\n\n    if num_samples == 0:\n        return torch.empty(\n            (0, input_activations.shape[-1]),\n            dtype=input_activations.dtype,\n            device=input_activations.device,\n        ).to(input_activations.device)\n\n    sample_indices: LearntNeuronIndices = torch.multinomial(\n        probabilities, num_samples=num_samples\n    )\n    return input_activations[sample_indices, :]\n</code></pre>"},{"location":"reference/activation_store/","title":"Activation Stores","text":"<p>Activation Stores.</p>"},{"location":"reference/activation_store/base_store/","title":"Activation Store Base Class","text":"<p>Activation Store Base Class.</p>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore","title":"<code>ActivationStore</code>","text":"<p>             Bases: <code>Dataset[InputOutputActivationVector]</code>, <code>ABC</code></p> <p>Activation Store Abstract Class.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide an activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which should typically be non-blocking). The resulting activation store can be used with a <code>torch.utils.data.DataLoader</code> to iterate over the dataset.</p> <p>Extend this class if you want to create a new activation store (noting you also need to create <code>__getitem__</code> and <code>__len__</code> methods from the underlying <code>torch.utils.data.Dataset</code> class).</p> <p>Example:</p> <p>import torch class MyActivationStore(ActivationStore): ...     def init(self): ...         super().init() ...         self._data = [] # In this example, we just store in a list ... ...     def append(self, item) -&gt; None: ...         self._data.append(item) ... ...     def extend(self, batch): ...         self._data.extend(batch) ... ...     def empty(self): ...         self._data = [] ... ...     def getitem(self, index: int): ...         return self._data[index] ... ...     def len(self) -&gt; int: ...         return len(self._data) ... store = MyActivationStore() store.append(torch.randn(100)) print(len(store)) 1</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>class ActivationStore(Dataset[InputOutputActivationVector], ABC):\n    \"\"\"Activation Store Abstract Class.\n\n    Extends the `torch.utils.data.Dataset` class to provide an activation store, with additional\n    :meth:`append` and :meth:`extend` methods (the latter of which should typically be\n    non-blocking). The resulting activation store can be used with a `torch.utils.data.DataLoader`\n    to iterate over the dataset.\n\n    Extend this class if you want to create a new activation store (noting you also need to create\n    `__getitem__` and `__len__` methods from the underlying `torch.utils.data.Dataset` class).\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; class MyActivationStore(ActivationStore):\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self._data = [] # In this example, we just store in a list\n    ...\n    ...     def append(self, item) -&gt; None:\n    ...         self._data.append(item)\n    ...\n    ...     def extend(self, batch):\n    ...         self._data.extend(batch)\n    ...\n    ...     def empty(self):\n    ...         self._data = []\n    ...\n    ...     def __getitem__(self, index: int):\n    ...         return self._data[index]\n    ...\n    ...     def __len__(self) -&gt; int:\n    ...         return len(self._data)\n    ...\n    &gt;&gt;&gt; store = MyActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; print(len(store))\n    1\n    \"\"\"\n\n    @abstractmethod\n    def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n        \"\"\"Add a Single Item to the Store.\"\"\"\n\n    @abstractmethod\n    def extend(self, batch: InputOutputActivationBatch) -&gt; Future | None:\n        \"\"\"Add a Batch to the Store.\"\"\"\n\n    @abstractmethod\n    def empty(self) -&gt; None:\n        \"\"\"Empty the Store.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Get the Length of the Store.\"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n        \"\"\"Get an Item from the Store.\"\"\"\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Optional shuffle method.\"\"\"\n\n    @final\n    def fill_with_test_data(\n        self, num_batches: int = 16, batch_size: int = 16, input_features: int = 256\n    ) -&gt; None:\n        \"\"\"Fill the store with test data.\n\n        For use when testing your code, to ensure it works with a real activation store.\n\n        Warning:\n            You may want to use `torch.seed(0)` to make the random data deterministic, if your test\n            requires inspecting the data itself.\n\n        Example:\n            &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=16*16, num_neurons=256)\n            &gt;&gt;&gt; store.fill_with_test_data()\n            &gt;&gt;&gt; len(store)\n            256\n            &gt;&gt;&gt; store[0].shape\n            torch.Size([256])\n\n        Args:\n            num_batches: Number of batches to fill the store with.\n            batch_size: Number of items per batch.\n            input_features: Number of input features per item.\n        \"\"\"\n        for _ in range(num_batches):\n            sample = torch.rand((batch_size, input_features))\n            self.extend(sample)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__getitem__","title":"<code>__getitem__(index)</code>  <code>abstractmethod</code>","text":"<p>Get an Item from the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n    \"\"\"Get an Item from the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Get the Length of the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Get the Length of the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.append","title":"<code>append(item)</code>  <code>abstractmethod</code>","text":"<p>Add a Single Item to the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef append(self, item: InputOutputActivationVector) -&gt; Future | None:\n    \"\"\"Add a Single Item to the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.empty","title":"<code>empty()</code>  <code>abstractmethod</code>","text":"<p>Empty the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef empty(self) -&gt; None:\n    \"\"\"Empty the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.extend","title":"<code>extend(batch)</code>  <code>abstractmethod</code>","text":"<p>Add a Batch to the Store.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@abstractmethod\ndef extend(self, batch: InputOutputActivationBatch) -&gt; Future | None:\n    \"\"\"Add a Batch to the Store.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.fill_with_test_data","title":"<code>fill_with_test_data(num_batches=16, batch_size=16, input_features=256)</code>","text":"<p>Fill the store with test data.</p> <p>For use when testing your code, to ensure it works with a real activation store.</p> Warning <p>You may want to use <code>torch.seed(0)</code> to make the random data deterministic, if your test requires inspecting the data itself.</p> Example <p>from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore store = TensorActivationStore(max_items=16*16, num_neurons=256) store.fill_with_test_data() len(store) 256 store[0].shape torch.Size([256])</p> <p>Parameters:</p> Name Type Description Default <code>num_batches</code> <code>int</code> <p>Number of batches to fill the store with.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>Number of items per batch.</p> <code>16</code> <code>input_features</code> <code>int</code> <p>Number of input features per item.</p> <code>256</code> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>@final\ndef fill_with_test_data(\n    self, num_batches: int = 16, batch_size: int = 16, input_features: int = 256\n) -&gt; None:\n    \"\"\"Fill the store with test data.\n\n    For use when testing your code, to ensure it works with a real activation store.\n\n    Warning:\n        You may want to use `torch.seed(0)` to make the random data deterministic, if your test\n        requires inspecting the data itself.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.activation_store.tensor_store import TensorActivationStore\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=16*16, num_neurons=256)\n        &gt;&gt;&gt; store.fill_with_test_data()\n        &gt;&gt;&gt; len(store)\n        256\n        &gt;&gt;&gt; store[0].shape\n        torch.Size([256])\n\n    Args:\n        num_batches: Number of batches to fill the store with.\n        batch_size: Number of items per batch.\n        input_features: Number of input features per item.\n    \"\"\"\n    for _ in range(num_batches):\n        sample = torch.rand((batch_size, input_features))\n        self.extend(sample)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.ActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Optional shuffle method.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Optional shuffle method.\"\"\"\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError","title":"<code>StoreFullError</code>","text":"<p>             Bases: <code>IndexError</code></p> <p>Exception raised when the activation store is full.</p> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>class StoreFullError(IndexError):\n    \"\"\"Exception raised when the activation store is full.\"\"\"\n\n    def __init__(self, message: str = \"Activation store is full\"):\n        \"\"\"Initialise the exception.\n\n        Args:\n            message: Override the default message.\n        \"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/activation_store/base_store/#sparse_autoencoder.activation_store.base_store.StoreFullError.__init__","title":"<code>__init__(message='Activation store is full')</code>","text":"<p>Initialise the exception.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Override the default message.</p> <code>'Activation store is full'</code> Source code in <code>sparse_autoencoder/activation_store/base_store.py</code> <pre><code>def __init__(self, message: str = \"Activation store is full\"):\n    \"\"\"Initialise the exception.\n\n    Args:\n        message: Override the default message.\n    \"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/activation_store/disk_store/","title":"Disk Activation Store","text":"<p>Disk Activation Store.</p>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore","title":"<code>DiskActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Disk Activation Store.</p> <p>Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up activation vectors and then write them to the disk in batches.</p> <p>Multiprocess safe (supports writing from multiple GPU workers).</p> <p>Warning: Unless you want to keep and use existing .pt files in the storage directory when initialized, set <code>empty_dir</code> to <code>True</code>.</p> <p>Note also that :meth:<code>close</code> must be called to ensure all activation vectors are written to disk after the last batch has been added to the store.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>class DiskActivationStore(ActivationStore):\n    \"\"\"Disk Activation Store.\n\n    Stores activation vectors on disk (hard-drive). Makes use of a queue (buffer) to store up\n    activation vectors and then write them to the disk in batches.\n\n    Multiprocess safe (supports writing from multiple GPU workers).\n\n    Warning:\n    Unless you want to keep and use existing .pt files in the storage directory when initialized,\n    set `empty_dir` to `True`.\n\n    Note also that :meth:`close` must be called to ensure all activation vectors are written to disk\n    after the last batch has been added to the store.\n    \"\"\"\n\n    _storage_path: Path\n    \"\"\"Path to the Directory where the Activation Vectors are Stored.\"\"\"\n\n    _cache: ListProxy\n    \"\"\"Cache for Activation Vectors.\n\n    Activation vectors are buffered in memory until the cache is full, at which point they are\n    written to disk.\n    \"\"\"\n\n    _cache_lock: Lock\n    \"\"\"Lock for the Cache.\"\"\"\n\n    _max_cache_size: int\n    \"\"\"Maximum Number of Activation Vectors to cache in Memory.\"\"\"\n\n    _thread_pool: ThreadPoolExecutor\n    \"\"\"Threadpool for non-blocking writes to the file system.\"\"\"\n\n    _disk_n_activation_vectors: ValueProxy[int]\n    \"\"\"Length of the Store (on disk).\n\n    Minus 1 signifies not calculated yet.\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n        max_cache_size: int = 10_000,\n        num_workers: int = 6,\n        *,\n        empty_dir: bool = False,\n    ):\n        \"\"\"Initialize the Disk Activation Store.\n\n        Args:\n            storage_path: Path to the directory where the activation vectors will be stored.\n            max_cache_size: The maximum number of activation vectors to cache in memory before\n                writing to disk. Note this is only followed approximately.\n            num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n                that the model can keep running whilst it writes the previous activations to disk).\n                This should be less than the number of CPU cores available. You don't need multiple\n                GPUs to take advantage of this feature.\n            empty_dir: Whether to empty the directory before writing. Generally you want to set this\n                to `True` as otherwise the directory may contain stale activation vectors from\n                previous runs.\n        \"\"\"\n        super().__init__()\n\n        # Setup the storage directory\n        self._storage_path = storage_path\n        self._storage_path.mkdir(parents=True, exist_ok=True)\n\n        # Setup the Cache\n        manager = Manager()\n        self._cache = manager.list()\n        self._max_cache_size = max_cache_size\n        self._cache_lock = manager.Lock()\n        self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n        # Empty the directory if needed\n        if empty_dir:\n            self.empty()\n\n        # Create a threadpool for non-blocking writes to the cache\n        self._thread_pool = ThreadPoolExecutor(num_workers)\n\n    def _write_to_disk(self, *, wait_for_max: bool = False) -&gt; None:\n        \"\"\"Write the contents of the queue to disk.\n\n        Args:\n            wait_for_max: Whether to wait until the cache is full before writing to disk.\n        \"\"\"\n        with self._cache_lock:\n            # Check we have enough items\n            if len(self._cache) == 0:\n                return\n\n            size_to_get = min(self._max_cache_size, len(self._cache))\n            if wait_for_max and size_to_get &lt; self._max_cache_size:\n                return\n\n            # Get the activations from the cache and delete them\n            activations = self._cache[0:size_to_get]\n            del self._cache[0:size_to_get]\n\n            # Update the length cache\n            if self._disk_n_activation_vectors.value != -1:\n                self._disk_n_activation_vectors.value += len(activations)\n\n        stacked_activations = torch.stack(activations)\n\n        filename = f\"{self.__len__}.pt\"\n        torch.save(stacked_activations, self._storage_path / filename)\n\n    def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n        \"\"\"Add a Single Item to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        Args:\n            item: Activation vector to add to the store.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        with self._cache_lock:\n            self._cache.append(item)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n        \"\"\"Add a Batch to the Store.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n        &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        10\n\n        Args:\n            batch: Batch of activation vectors to add to the store.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        items: list[InputOutputActivationVector] = resize_to_list_vectors(batch)\n\n        with self._cache_lock:\n            self._cache.extend(items)\n\n            # Write to disk if needed\n            if len(self._cache) &gt;= self._max_cache_size:\n                return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n        return None  # Keep mypy happy\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        This should be called after the last batch has been added to the store. It will wait for\n        all activation vectors to be written to disk.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; print(len(store))\n        1\n        \"\"\"\n        while len(self._cache) &gt; 0:\n            self._write_to_disk()\n\n    @property\n    def _all_filenames(self) -&gt; list[Path]:\n        \"\"\"Return a List of All Activation Vector Filenames.\"\"\"\n        return list(self._storage_path.glob(\"*.pt\"))\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the Store.\n\n        Warning:\n        This will delete all .pt files in the top level of the storage directory.\n\n        Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; future = store.append(torch.randn(100))\n        &gt;&gt;&gt; future.result()\n        &gt;&gt;&gt; print(len(store))\n        1\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; print(len(store))\n        0\n        \"\"\"\n        for file in self._all_filenames:\n            file.unlink()\n        self._disk_n_activation_vectors.value = 0\n\n    def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n        \"\"\"Get Item Dunder Method.\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        # Find the file containing the activation vector\n        file_index = index // self._max_cache_size\n        file = self._storage_path / f\"{file_index}.pt\"\n\n        # Load the file and return the activation vector\n        activation_vectors = torch.load(file)\n        return activation_vectors[index % self._max_cache_size]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n            &gt;&gt;&gt; print(len(store))\n            0\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        # Calculate the length if not cached\n        if self._disk_n_activation_vectors.value == -1:\n            cache_size: int = 0\n            for file in self._all_filenames:\n                cache_size += len(torch.load(file))\n            self._disk_n_activation_vectors.value = cache_size\n\n        return self._disk_n_activation_vectors.value\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        # Shutdown the thread pool after everything is complete\n        self._thread_pool.shutdown(wait=True, cancel_futures=False)\n        self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    # Shutdown the thread pool after everything is complete\n    self._thread_pool.shutdown(wait=True, cancel_futures=False)\n    self.wait_for_writes_to_complete()\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationVector</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n    \"\"\"Get Item Dunder Method.\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    # Find the file containing the activation vector\n    file_index = index // self._max_cache_size\n    file = self._storage_path / f\"{file_index}.pt\"\n\n    # Load the file and return the activation vector\n    activation_vectors = torch.load(file)\n    return activation_vectors[index % self._max_cache_size]\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__init__","title":"<code>__init__(storage_path=DEFAULT_DISK_ACTIVATION_STORE_PATH, max_cache_size=10000, num_workers=6, *, empty_dir=False)</code>","text":"<p>Initialize the Disk Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>storage_path</code> <code>Path</code> <p>Path to the directory where the activation vectors will be stored.</p> <code>DEFAULT_DISK_ACTIVATION_STORE_PATH</code> <code>max_cache_size</code> <code>int</code> <p>The maximum number of activation vectors to cache in memory before writing to disk. Note this is only followed approximately.</p> <code>10000</code> <code>num_workers</code> <code>int</code> <p>Number of CPU workers to use for non-blocking writes to the file system (so that the model can keep running whilst it writes the previous activations to disk). This should be less than the number of CPU cores available. You don't need multiple GPUs to take advantage of this feature.</p> <code>6</code> <code>empty_dir</code> <code>bool</code> <p>Whether to empty the directory before writing. Generally you want to set this to <code>True</code> as otherwise the directory may contain stale activation vectors from previous runs.</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __init__(\n    self,\n    storage_path: Path = DEFAULT_DISK_ACTIVATION_STORE_PATH,\n    max_cache_size: int = 10_000,\n    num_workers: int = 6,\n    *,\n    empty_dir: bool = False,\n):\n    \"\"\"Initialize the Disk Activation Store.\n\n    Args:\n        storage_path: Path to the directory where the activation vectors will be stored.\n        max_cache_size: The maximum number of activation vectors to cache in memory before\n            writing to disk. Note this is only followed approximately.\n        num_workers: Number of CPU workers to use for non-blocking writes to the file system (so\n            that the model can keep running whilst it writes the previous activations to disk).\n            This should be less than the number of CPU cores available. You don't need multiple\n            GPUs to take advantage of this feature.\n        empty_dir: Whether to empty the directory before writing. Generally you want to set this\n            to `True` as otherwise the directory may contain stale activation vectors from\n            previous runs.\n    \"\"\"\n    super().__init__()\n\n    # Setup the storage directory\n    self._storage_path = storage_path\n    self._storage_path.mkdir(parents=True, exist_ok=True)\n\n    # Setup the Cache\n    manager = Manager()\n    self._cache = manager.list()\n    self._max_cache_size = max_cache_size\n    self._cache_lock = manager.Lock()\n    self._disk_n_activation_vectors = manager.Value(\"i\", -1)\n\n    # Empty the directory if needed\n    if empty_dir:\n        self.empty()\n\n    # Create a threadpool for non-blocking writes to the cache\n    self._thread_pool = ThreadPoolExecutor(num_workers)\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> Example <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) print(len(store)) 0</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n        &gt;&gt;&gt; print(len(store))\n        0\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    # Calculate the length if not cached\n    if self._disk_n_activation_vectors.value == -1:\n        cache_size: int = 0\n        for file in self._all_filenames:\n            cache_size += len(torch.load(file))\n        self._disk_n_activation_vectors.value = cache_size\n\n    return self._disk_n_activation_vectors.value\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a Single Item to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>InputOutputActivationVector</code> <p>Activation vector to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n    \"\"\"Add a Single Item to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    Args:\n        item: Activation vector to add to the store.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    with self._cache_lock:\n        self._cache.append(item)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the Store.</p> <p>Warning: This will delete all .pt files in the top level of the storage directory.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) future.result() print(len(store)) 1</p> <p>store.empty() print(len(store)) 0</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the Store.\n\n    Warning:\n    This will delete all .pt files in the top level of the storage directory.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    1\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; print(len(store))\n    0\n    \"\"\"\n    for file in self._all_filenames:\n        file.unlink()\n    self._disk_n_activation_vectors.value = 0\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a Batch to the Store.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=10, empty_dir=True) future = store.extend(torch.randn(10, 100)) future.result() print(len(store)) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SourceModelActivations</code> <p>Batch of activation vectors to add to the store.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n    \"\"\"Add a Batch to the Store.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=10, empty_dir=True)\n    &gt;&gt;&gt; future = store.extend(torch.randn(10, 100))\n    &gt;&gt;&gt; future.result()\n    &gt;&gt;&gt; print(len(store))\n    10\n\n    Args:\n        batch: Batch of activation vectors to add to the store.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    items: list[InputOutputActivationVector] = resize_to_list_vectors(batch)\n\n    with self._cache_lock:\n        self._cache.extend(items)\n\n        # Write to disk if needed\n        if len(self._cache) &gt;= self._max_cache_size:\n            return self._thread_pool.submit(self._write_to_disk, wait_for_max=True)\n\n    return None  # Keep mypy happy\n</code></pre>"},{"location":"reference/activation_store/disk_store/#sparse_autoencoder.activation_store.disk_store.DiskActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>This should be called after the last batch has been added to the store. It will wait for all activation vectors to be written to disk.</p> <p>Example:</p> <p>store = DiskActivationStore(max_cache_size=1, empty_dir=True) future = store.append(torch.randn(100)) store.wait_for_writes_to_complete() print(len(store)) 1</p> Source code in <code>sparse_autoencoder/activation_store/disk_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    This should be called after the last batch has been added to the store. It will wait for\n    all activation vectors to be written to disk.\n\n    Example:\n    &gt;&gt;&gt; store = DiskActivationStore(max_cache_size=1, empty_dir=True)\n    &gt;&gt;&gt; future = store.append(torch.randn(100))\n    &gt;&gt;&gt; store.wait_for_writes_to_complete()\n    &gt;&gt;&gt; print(len(store))\n    1\n    \"\"\"\n    while len(self._cache) &gt; 0:\n        self._write_to_disk()\n</code></pre>"},{"location":"reference/activation_store/list_store/","title":"List Activation Store","text":"<p>List Activation Store.</p>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore","title":"<code>ListActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>List Activation Store.</p> <p>Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick experiments where you don't want to calculate how much memory you need in advance.</p> <p>Multiprocess safe if the <code>multiprocessing_enabled</code> argument is set to <code>True</code>. This works in two ways:</p> <ol> <li>The list of activation vectors is stored in a multiprocessing manager, which allows multiple     processes (typically multiple GPUs) to read/write to the list.</li> <li>The <code>extend</code> method is non-blocking, and uses a threadpool to write to the list in the     background, which allows the main process to continue working even if there is just one GPU.</li> </ol> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Note that the built-in :meth:<code>shuffle</code> method is much faster than using the <code>shuffle</code> argument on <code>torch.utils.data.DataLoader</code>. You should therefore call this method before passing the dataset to the loader and then set the DataLoader <code>shuffle</code> argument to <code>False</code>.</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = ListActivationStore()\n</code></pre> <p>Add a single activation vector to the dataset (this is blocking):</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a batch of activation vectors to the dataset (non-blocking):</p> <pre><code>&gt;&gt;&gt; batch = torch.randn(10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n11\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>class ListActivationStore(ActivationStore):\n    \"\"\"List Activation Store.\n\n    Stores pointers to activation vectors in a list (in-memory). This is primarily of use for quick\n    experiments where you don't want to calculate how much memory you need in advance.\n\n    Multiprocess safe if the `multiprocessing_enabled` argument is set to `True`. This works in two\n    ways:\n\n    1. The list of activation vectors is stored in a multiprocessing manager, which allows multiple\n        processes (typically multiple GPUs) to read/write to the list.\n    2. The `extend` method is non-blocking, and uses a threadpool to write to the list in the\n        background, which allows the main process to continue working even if there is just one GPU.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Note that the built-in :meth:`shuffle` method is much faster than using the `shuffle` argument\n    on `torch.utils.data.DataLoader`. You should therefore call this method before passing the\n    dataset to the loader and then set the DataLoader `shuffle` argument to `False`.\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n\n    Add a single activation vector to the dataset (this is blocking):\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a batch of activation vectors to the dataset (non-blocking):\n\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        11\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: list[InputOutputActivationVector] | ListProxy\n    \"\"\"Underlying List Data Store.\"\"\"\n\n    _device: torch.device | None\n    \"\"\"Device to Store the Activation Vectors On.\"\"\"\n\n    _pool: ProcessPoolExecutor | None = None\n    \"\"\"Multiprocessing Pool.\"\"\"\n\n    _pool_exceptions: ListProxy | list[Exception]\n    \"\"\"Pool Exceptions.\n\n    Used to keep track of exceptions.\n    \"\"\"\n\n    _pool_futures: list[Future]\n    \"\"\"Pool Futures.\n\n    Used to keep track of processes running in the pool.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: list[InputOutputActivationVector] | None = None,\n        device: torch.device | None = None,\n        max_workers: int | None = None,\n        *,\n        multiprocessing_enabled: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the List Activation Store.\n\n        Args:\n            data: Data to initialize the dataset with.\n            device: Device to store the activation vectors on.\n            max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n                Default is the number of cores you have.\n            multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n                workers. This creates significant overhead, so you should only enable it if you have\n                multiple GPUs (and experiment with enabling/disabling it).\n        \"\"\"\n        # Default to empty\n        if data is None:\n            data = []\n\n        # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n        # between processes. Otherwise, just use a normal list.\n        if multiprocessing_enabled:\n            self._pool = ProcessPoolExecutor(max_workers=max_workers)\n            manager = Manager()\n            self._data = manager.list(data)\n            self._data.extend(data)\n            self._pool_exceptions = manager.list()\n        else:\n            self._data = data\n            self._pool_exceptions = []\n\n        self._pool_futures = []\n\n        # Device for storing the activation vectors\n        self._device = device\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return len(self._data)\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Returns:\n            The size of the dataset in bytes.\n        \"\"\"\n        # The list of tensors is really a list of pointers to tensors, so we need to account for\n        # this as well as the size of the tensors themselves.\n        list_of_pointers_size = self._data.__sizeof__()\n\n        # Handle 0 items\n        if len(self._data) == 0:\n            return list_of_pointers_size\n\n        # Otherwise, get the size of the first tensor\n        first_tensor = self._data[0]\n        first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n        num_tensors = len(self._data)\n        total_tensors_size = first_tensor_size * num_tensors\n\n        return total_tensors_size + list_of_pointers_size\n\n    def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n        \"\"\"\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.append(torch.tensor([3.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; len(store)\n        3\n\n        \"\"\"\n        self.wait_for_writes_to_complete()\n        random.shuffle(self._data)\n\n    def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n        \"\"\"Append a single item to the dataset.\n\n        Note **append is blocking**. For better performance use extend instead with batches.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        Args:\n            item: The item to append to the dataset.\n\n        Returns:\n            Future that completes when the activation vector has queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        self._data.append(item.to(self._device))\n\n    def _extend(self, batch: SourceModelActivations) -&gt; None:\n        \"\"\"Extend threadpool method.\n\n        To be called by :meth:`extend`.\n\n        Args:\n            batch: A batch of items to add to the dataset.\n        \"\"\"\n        try:\n            # Unstack to a list of tensors\n            items: list[InputOutputActivationVector] = resize_to_list_vectors(batch)\n\n            self._data.extend(items)\n        except Exception as e:  # noqa: BLE001\n            self._pool_exceptions.append(e)\n\n    def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n        \"\"\"Extend the dataset with multiple items (non-blocking).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore()\n            &gt;&gt;&gt; batch = torch.randn(10, 100)\n            &gt;&gt;&gt; async_result = store.extend(batch)\n            &gt;&gt;&gt; len(store)\n            10\n\n        Args:\n            batch: A batch of items to add to the dataset.\n\n        Returns:\n            Future that completes when the activation vectors have queued to be written to disk, and\n            if needed, written to disk.\n        \"\"\"\n        # Schedule _extend to run in a separate process\n        if self._pool:\n            future = self._pool.submit(self._extend, batch)\n            self._pool_futures.append(future)\n\n        # Fallback to synchronous execution if not multiprocessing\n        self._extend(batch)\n\n    def wait_for_writes_to_complete(self) -&gt; None:\n        \"\"\"Wait for Writes to Complete.\n\n        Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n            &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n            &gt;&gt;&gt; store.wait_for_writes_to_complete()\n            &gt;&gt;&gt; len(store)\n            3\n\n        Raises:\n            RuntimeError: If any exceptions occurred in the background workers.\n        \"\"\"\n        # Restart the pool\n        if self._pool:\n            for _future in as_completed(self._pool_futures):\n                pass\n            self._pool_futures.clear()\n\n        time.sleep(1)\n\n        if self._pool_exceptions:\n            exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n            msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n            raise RuntimeError(msg)\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the dataset.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; len(store)\n        0\n        \"\"\"\n        self.wait_for_writes_to_complete()\n\n        # Clearing a list like this works for both standard and multiprocessing lists\n        self._data[:] = []\n\n    def __del__(self) -&gt; None:\n        \"\"\"Delete Dunder Method.\"\"\"\n        if self._pool:\n            self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__del__","title":"<code>__del__()</code>","text":"<p>Delete Dunder Method.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Delete Dunder Method.\"\"\"\n    if self._pool:\n        self._pool.shutdown(wait=False, cancel_futures=True)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationVector</code> <p>The activation store item at the given index.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n    \"\"\"\n    return self._data[index]\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__init__","title":"<code>__init__(data=None, device=None, max_workers=None, *, multiprocessing_enabled=False)</code>","text":"<p>Initialize the List Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[InputOutputActivationVector] | None</code> <p>Data to initialize the dataset with.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> <code>max_workers</code> <code>int | None</code> <p>Max CPU workers if multiprocessing is enabled, for writing to the list. Default is the number of cores you have.</p> <code>None</code> <code>multiprocessing_enabled</code> <code>bool</code> <p>Support reading/writing to the dataset with multiple GPU workers. This creates significant overhead, so you should only enable it if you have multiple GPUs (and experiment with enabling/disabling it).</p> <code>False</code> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __init__(\n    self,\n    data: list[InputOutputActivationVector] | None = None,\n    device: torch.device | None = None,\n    max_workers: int | None = None,\n    *,\n    multiprocessing_enabled: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the List Activation Store.\n\n    Args:\n        data: Data to initialize the dataset with.\n        device: Device to store the activation vectors on.\n        max_workers: Max CPU workers if multiprocessing is enabled, for writing to the list.\n            Default is the number of cores you have.\n        multiprocessing_enabled: Support reading/writing to the dataset with multiple GPU\n            workers. This creates significant overhead, so you should only enable it if you have\n            multiple GPUs (and experiment with enabling/disabling it).\n    \"\"\"\n    # Default to empty\n    if data is None:\n        data = []\n\n    # If multiprocessing is enabled, use a multiprocessing manager to create a shared list\n    # between processes. Otherwise, just use a normal list.\n    if multiprocessing_enabled:\n        self._pool = ProcessPoolExecutor(max_workers=max_workers)\n        manager = Manager()\n        self._data = manager.list(data)\n        self._data.extend(data)\n        self._pool_exceptions = manager.list()\n    else:\n        self._data = data\n        self._pool_exceptions = []\n\n    self._pool_futures = []\n\n    # Device for storing the activation vectors\n    self._device = device\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the dataset in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Returns:\n        The size of the dataset in bytes.\n    \"\"\"\n    # The list of tensors is really a list of pointers to tensors, so we need to account for\n    # this as well as the size of the tensors themselves.\n    list_of_pointers_size = self._data.__sizeof__()\n\n    # Handle 0 items\n    if len(self._data) == 0:\n        return list_of_pointers_size\n\n    # Otherwise, get the size of the first tensor\n    first_tensor = self._data[0]\n    first_tensor_size = first_tensor.element_size() * first_tensor.nelement()\n    num_tensors = len(self._data)\n    total_tensors_size = first_tensor_size * num_tensors\n\n    return total_tensors_size + list_of_pointers_size\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.append","title":"<code>append(item)</code>","text":"<p>Append a single item to the dataset.</p> <p>Note append is blocking. For better performance use extend instead with batches.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>InputOutputActivationVector</code> <p>The item to append to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vector has queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def append(self, item: InputOutputActivationVector) -&gt; Future | None:\n    \"\"\"Append a single item to the dataset.\n\n    Note **append is blocking**. For better performance use extend instead with batches.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    Args:\n        item: The item to append to the dataset.\n\n    Returns:\n        Future that completes when the activation vector has queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    self._data.append(item.to(self._device))\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the dataset.</p> <p>Example:</p> <p>import torch store = ListActivationStore() store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>store.empty() len(store) 0</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the dataset.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; store.append(torch.randn(100))\n    &gt;&gt;&gt; len(store)\n    2\n\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; len(store)\n    0\n    \"\"\"\n    self.wait_for_writes_to_complete()\n\n    # Clearing a list like this works for both standard and multiprocessing lists\n    self._data[:] = []\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Extend the dataset with multiple items (non-blocking).</p> Example <p>import torch store = ListActivationStore() batch = torch.randn(10, 100) async_result = store.extend(batch) len(store) 10</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SourceModelActivations</code> <p>A batch of items to add to the dataset.</p> required <p>Returns:</p> Type Description <code>Future | None</code> <p>Future that completes when the activation vectors have queued to be written to disk, and</p> <code>Future | None</code> <p>if needed, written to disk.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def extend(self, batch: SourceModelActivations) -&gt; Future | None:\n    \"\"\"Extend the dataset with multiple items (non-blocking).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; batch = torch.randn(10, 100)\n        &gt;&gt;&gt; async_result = store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        10\n\n    Args:\n        batch: A batch of items to add to the dataset.\n\n    Returns:\n        Future that completes when the activation vectors have queued to be written to disk, and\n        if needed, written to disk.\n    \"\"\"\n    # Schedule _extend to run in a separate process\n    if self._pool:\n        future = self._pool.submit(self._extend, batch)\n        self._pool_futures.append(future)\n\n    # Fallback to synchronous execution if not multiprocessing\n    self._extend(batch)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = ListActivationStore() store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.append(torch.tensor([3.])) store.shuffle() len(store) 3</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = ListActivationStore()\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.append(torch.tensor([3.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; len(store)\n    3\n\n    \"\"\"\n    self.wait_for_writes_to_complete()\n    random.shuffle(self._data)\n</code></pre>"},{"location":"reference/activation_store/list_store/#sparse_autoencoder.activation_store.list_store.ListActivationStore.wait_for_writes_to_complete","title":"<code>wait_for_writes_to_complete()</code>","text":"<p>Wait for Writes to Complete.</p> <p>Wait for any non-blocking writes (e.g. calls to :meth:<code>append</code>) to complete.</p> Example <p>import torch store = ListActivationStore(multiprocessing_enabled=True) store.extend(torch.randn(3, 100)) store.wait_for_writes_to_complete() len(store) 3</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If any exceptions occurred in the background workers.</p> Source code in <code>sparse_autoencoder/activation_store/list_store.py</code> <pre><code>def wait_for_writes_to_complete(self) -&gt; None:\n    \"\"\"Wait for Writes to Complete.\n\n    Wait for any non-blocking writes (e.g. calls to :meth:`append`) to complete.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = ListActivationStore(multiprocessing_enabled=True)\n        &gt;&gt;&gt; store.extend(torch.randn(3, 100))\n        &gt;&gt;&gt; store.wait_for_writes_to_complete()\n        &gt;&gt;&gt; len(store)\n        3\n\n    Raises:\n        RuntimeError: If any exceptions occurred in the background workers.\n    \"\"\"\n    # Restart the pool\n    if self._pool:\n        for _future in as_completed(self._pool_futures):\n            pass\n        self._pool_futures.clear()\n\n    time.sleep(1)\n\n    if self._pool_exceptions:\n        exceptions_report = \"\\n\".join([str(e) for e in self._pool_exceptions])\n        msg = f\"Exceptions occurred in background workers:\\n{exceptions_report}\"\n        raise RuntimeError(msg)\n</code></pre>"},{"location":"reference/activation_store/tensor_store/","title":"Tensor Activation Store","text":"<p>Tensor Activation Store.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore","title":"<code>TensorActivationStore</code>","text":"<p>             Bases: <code>ActivationStore</code></p> <p>Tensor Activation Store.</p> <p>Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation vectors to be stored to be known in advance. Multiprocess safe.</p> <p>Extends the <code>torch.utils.data.Dataset</code> class to provide a list-based activation store, with additional :meth:<code>append</code> and :meth:<code>extend</code> methods (the latter of which is non-blocking).</p> <p>Examples: Create an empty activation dataset:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n</code></pre> <p>Add a single activation vector to the dataset:</p> <pre><code>&gt;&gt;&gt; store.append(torch.randn(100))\n&gt;&gt;&gt; len(store)\n1\n</code></pre> <p>Add a [batch, pos, neurons] activation tensor to the dataset:</p> <pre><code>&gt;&gt;&gt; store.empty()\n&gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n&gt;&gt;&gt; store.extend(batch)\n&gt;&gt;&gt; len(store)\n100\n</code></pre> <p>Shuffle the dataset before passing it to the DataLoader:</p> <pre><code>&gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n&gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n</code></pre> <p>Use the dataloader to iterate over the dataset:</p> <pre><code>&gt;&gt;&gt; next_item = next(iter(loader))\n&gt;&gt;&gt; next_item.shape\ntorch.Size([2, 100])\n</code></pre> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>class TensorActivationStore(ActivationStore):\n    \"\"\"Tensor Activation Store.\n\n    Stores tensors in a (large) tensor of shape (item, neuron). Requires the number of activation\n    vectors to be stored to be known in advance. Multiprocess safe.\n\n    Extends the `torch.utils.data.Dataset` class to provide a list-based activation store, with\n    additional :meth:`append` and :meth:`extend` methods (the latter of which is non-blocking).\n\n    Examples:\n    Create an empty activation dataset:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=1000, num_neurons=100)\n\n    Add a single activation vector to the dataset:\n\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        1\n\n    Add a [batch, pos, neurons] activation tensor to the dataset:\n\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; batch = torch.randn(10, 10, 100)\n        &gt;&gt;&gt; store.extend(batch)\n        &gt;&gt;&gt; len(store)\n        100\n\n    Shuffle the dataset **before passing it to the DataLoader**:\n\n        &gt;&gt;&gt; store.shuffle() # Faster than using the DataLoader shuffle argument\n        &gt;&gt;&gt; loader = torch.utils.data.DataLoader(store, shuffle=False, batch_size=2)\n\n    Use the dataloader to iterate over the dataset:\n\n        &gt;&gt;&gt; next_item = next(iter(loader))\n        &gt;&gt;&gt; next_item.shape\n        torch.Size([2, 100])\n    \"\"\"\n\n    _data: StoreActivations\n    \"\"\"Underlying Tensor Data Store.\"\"\"\n\n    items_stored: int = 0\n    \"\"\"Number of items stored.\"\"\"\n\n    max_items: int\n    \"\"\"Maximum Number of Items to Store.\"\"\"\n\n    def __init__(\n        self,\n        max_items: int,\n        num_neurons: int,\n        device: torch.device | None = None,\n    ) -&gt; None:\n        \"\"\"Initialise the Tensor Activation Store.\n\n        Args:\n            max_items: Maximum number of items to store (individual activation vectors)\n            num_neurons: Number of neurons in each activation vector.\n            device: Device to store the activation vectors on.\n        \"\"\"\n        self._data = torch.empty((max_items, num_neurons), device=device)\n        self._max_items = max_items\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length Dunder Method.\n\n        Returns the number of activation vectors in the dataset.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; store.append(torch.randn(100))\n            &gt;&gt;&gt; len(store)\n            2\n\n        Returns:\n            The number of activation vectors in the dataset.\n        \"\"\"\n        return self.items_stored\n\n    def __sizeof__(self) -&gt; int:\n        \"\"\"Sizeof Dunder Method.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n            &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n            800\n\n        Returns:\n            The size of the underlying tensor in bytes.\n        \"\"\"\n        return self._data.element_size() * self._data.nelement()\n\n    def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n        \"\"\"Get Item Dunder Method.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            index: The index of the tensor to fetch.\n\n        Returns:\n            The activation store item at the given index.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        # Check in range\n        if index &gt;= self.items_stored:\n            msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n            raise IndexError(msg)\n\n        return self._data[index]\n\n    def shuffle(self) -&gt; None:\n        \"\"\"Shuffle the Data In-Place.\n\n        This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; _seed = torch.manual_seed(42)\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n        &gt;&gt;&gt; store.append(torch.tensor([0.]))\n        &gt;&gt;&gt; store.append(torch.tensor([1.]))\n        &gt;&gt;&gt; store.append(torch.tensor([2.]))\n        &gt;&gt;&gt; store.shuffle()\n        &gt;&gt;&gt; [store[i].item() for i in range(3)]\n        [0.0, 2.0, 1.0]\n        \"\"\"\n        # Generate a permutation of the indices for the active data\n        perm = torch.randperm(self.items_stored)\n\n        # Use this permutation to shuffle the active data in-place\n        self._data[: self.items_stored] = self._data[perm]\n\n    def append(self, item: InputOutputActivationVector) -&gt; None:\n        \"\"\"Add a single item to the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.append(torch.zeros(5))\n        &gt;&gt;&gt; store.append(torch.ones(5))\n        &gt;&gt;&gt; store[1]\n        tensor([1., 1., 1., 1., 1.])\n\n        Args:\n            item: The item to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        # Check we have space\n        if self.items_stored + 1 &gt; self._max_items:\n            raise StoreFullError\n\n        self._data[self.items_stored] = item.to(\n            self._data.device,\n        )\n        self.items_stored += 1\n\n    def extend(self, batch: SourceModelActivations) -&gt; None:\n        \"\"\"Add a batch to the store.\n\n        Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n        &gt;&gt;&gt; store.items_stored\n        9\n\n        Args:\n            batch: The batch to append to the dataset.\n\n        Raises:\n            IndexError: If there is no space remaining.\n        \"\"\"\n        reshaped: InputOutputActivationBatch = resize_to_single_item_dimension(\n            batch,\n        )\n\n        # Check we have space\n        num_activation_tensors: int = reshaped.shape[0]\n        if self.items_stored + num_activation_tensors &gt; self._max_items:\n            if reshaped.shape[0] &gt; self._max_items:\n                msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                    total maximum in the store of {self._max_items}.\"\n                raise ValueError(msg)\n\n            raise StoreFullError\n\n        self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n            self._data.device\n        )\n        self.items_stored += num_activation_tensors\n\n    def empty(self) -&gt; None:\n        \"\"\"Empty the store.\n\n        Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n        &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n        &gt;&gt;&gt; store.items_stored\n        2\n        &gt;&gt;&gt; store.empty()\n        &gt;&gt;&gt; store.items_stored\n        0\n        \"\"\"\n        # We don't need to zero the data, just reset the number of items stored\n        self.items_stored = 0\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.items_stored","title":"<code>items_stored: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of items stored.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.max_items","title":"<code>max_items: int</code>  <code>instance-attribute</code>","text":"<p>Maximum Number of Items to Store.</p>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item Dunder Method.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=2, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the tensor to fetch.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationVector</code> <p>The activation store item at the given index.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the index is out of range.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __getitem__(self, index: int) -&gt; InputOutputActivationVector:\n    \"\"\"Get Item Dunder Method.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        index: The index of the tensor to fetch.\n\n    Returns:\n        The activation store item at the given index.\n\n    Raises:\n        IndexError: If the index is out of range.\n    \"\"\"\n    # Check in range\n    if index &gt;= self.items_stored:\n        msg = f\"Index {index} out of range (only {self.items_stored} items stored)\"\n        raise IndexError(msg)\n\n    return self._data[index]\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__init__","title":"<code>__init__(max_items, num_neurons, device=None)</code>","text":"<p>Initialise the Tensor Activation Store.</p> <p>Parameters:</p> Name Type Description Default <code>max_items</code> <code>int</code> <p>Maximum number of items to store (individual activation vectors)</p> required <code>num_neurons</code> <code>int</code> <p>Number of neurons in each activation vector.</p> required <code>device</code> <code>device | None</code> <p>Device to store the activation vectors on.</p> <code>None</code> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __init__(\n    self,\n    max_items: int,\n    num_neurons: int,\n    device: torch.device | None = None,\n) -&gt; None:\n    \"\"\"Initialise the Tensor Activation Store.\n\n    Args:\n        max_items: Maximum number of items to store (individual activation vectors)\n        num_neurons: Number of neurons in each activation vector.\n        device: Device to store the activation vectors on.\n    \"\"\"\n    self._data = torch.empty((max_items, num_neurons), device=device)\n    self._max_items = max_items\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__len__","title":"<code>__len__()</code>","text":"<p>Length Dunder Method.</p> <p>Returns the number of activation vectors in the dataset.</p> Example <p>import torch store = TensorActivationStore(max_items=10_000_000, num_neurons=100) store.append(torch.randn(100)) store.append(torch.randn(100)) len(store) 2</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of activation vectors in the dataset.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length Dunder Method.\n\n    Returns the number of activation vectors in the dataset.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=10_000_000, num_neurons=100)\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; store.append(torch.randn(100))\n        &gt;&gt;&gt; len(store)\n        2\n\n    Returns:\n        The number of activation vectors in the dataset.\n    \"\"\"\n    return self.items_stored\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.__sizeof__","title":"<code>__sizeof__()</code>","text":"<p>Sizeof Dunder Method.</p> Example <p>import torch store = TensorActivationStore(max_items=2, num_neurons=100) store.sizeof() # Pre-allocated tensor of 2x100 800</p> <p>Returns:</p> Type Description <code>int</code> <p>The size of the underlying tensor in bytes.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def __sizeof__(self) -&gt; int:\n    \"\"\"Sizeof Dunder Method.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; store = TensorActivationStore(max_items=2, num_neurons=100)\n        &gt;&gt;&gt; store.__sizeof__() # Pre-allocated tensor of 2x100\n        800\n\n    Returns:\n        The size of the underlying tensor in bytes.\n    \"\"\"\n    return self._data.element_size() * self._data.nelement()\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.append","title":"<code>append(item)</code>","text":"<p>Add a single item to the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.append(torch.zeros(5)) store.append(torch.ones(5)) store[1] tensor([1., 1., 1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>InputOutputActivationVector</code> <p>The item to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def append(self, item: InputOutputActivationVector) -&gt; None:\n    \"\"\"Add a single item to the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.append(torch.zeros(5))\n    &gt;&gt;&gt; store.append(torch.ones(5))\n    &gt;&gt;&gt; store[1]\n    tensor([1., 1., 1., 1., 1.])\n\n    Args:\n        item: The item to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    # Check we have space\n    if self.items_stored + 1 &gt; self._max_items:\n        raise StoreFullError\n\n    self._data[self.items_stored] = item.to(\n        self._data.device,\n    )\n    self.items_stored += 1\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.empty","title":"<code>empty()</code>","text":"<p>Empty the store.</p> <p>Example:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2 store.empty() store.items_stored 0</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def empty(self) -&gt; None:\n    \"\"\"Empty the store.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n    &gt;&gt;&gt; store.empty()\n    &gt;&gt;&gt; store.items_stored\n    0\n    \"\"\"\n    # We don't need to zero the data, just reset the number of items stored\n    self.items_stored = 0\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.extend","title":"<code>extend(batch)</code>","text":"<p>Add a batch to the store.</p> <p>Examples:</p> <p>import torch store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(2, 5)) store.items_stored 2</p> <p>store = TensorActivationStore(max_items=10, num_neurons=5) store.extend(torch.zeros(3, 3, 5)) store.items_stored 9</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SourceModelActivations</code> <p>The batch to append to the dataset.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If there is no space remaining.</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def extend(self, batch: SourceModelActivations) -&gt; None:\n    \"\"\"Add a batch to the store.\n\n    Examples:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(2, 5))\n    &gt;&gt;&gt; store.items_stored\n    2\n\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=5)\n    &gt;&gt;&gt; store.extend(torch.zeros(3, 3, 5))\n    &gt;&gt;&gt; store.items_stored\n    9\n\n    Args:\n        batch: The batch to append to the dataset.\n\n    Raises:\n        IndexError: If there is no space remaining.\n    \"\"\"\n    reshaped: InputOutputActivationBatch = resize_to_single_item_dimension(\n        batch,\n    )\n\n    # Check we have space\n    num_activation_tensors: int = reshaped.shape[0]\n    if self.items_stored + num_activation_tensors &gt; self._max_items:\n        if reshaped.shape[0] &gt; self._max_items:\n            msg = f\"Single batch of {num_activation_tensors} activations is larger than the \\\n                total maximum in the store of {self._max_items}.\"\n            raise ValueError(msg)\n\n        raise StoreFullError\n\n    self._data[self.items_stored : self.items_stored + num_activation_tensors] = reshaped.to(\n        self._data.device\n    )\n    self.items_stored += num_activation_tensors\n</code></pre>"},{"location":"reference/activation_store/tensor_store/#sparse_autoencoder.activation_store.tensor_store.TensorActivationStore.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffle the Data In-Place.</p> <p>This is much faster than using the shuffle argument on <code>torch.utils.data.DataLoader</code>.</p> <p>Example:</p> <p>import torch _seed = torch.manual_seed(42) store = TensorActivationStore(max_items=10, num_neurons=1) store.append(torch.tensor([0.])) store.append(torch.tensor([1.])) store.append(torch.tensor([2.])) store.shuffle() [store[i].item() for i in range(3)] [0.0, 2.0, 1.0]</p> Source code in <code>sparse_autoencoder/activation_store/tensor_store.py</code> <pre><code>def shuffle(self) -&gt; None:\n    \"\"\"Shuffle the Data In-Place.\n\n    This is much faster than using the shuffle argument on `torch.utils.data.DataLoader`.\n\n    Example:\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; _seed = torch.manual_seed(42)\n    &gt;&gt;&gt; store = TensorActivationStore(max_items=10, num_neurons=1)\n    &gt;&gt;&gt; store.append(torch.tensor([0.]))\n    &gt;&gt;&gt; store.append(torch.tensor([1.]))\n    &gt;&gt;&gt; store.append(torch.tensor([2.]))\n    &gt;&gt;&gt; store.shuffle()\n    &gt;&gt;&gt; [store[i].item() for i in range(3)]\n    [0.0, 2.0, 1.0]\n    \"\"\"\n    # Generate a permutation of the indices for the active data\n    perm = torch.randperm(self.items_stored)\n\n    # Use this permutation to shuffle the active data in-place\n    self._data[: self.items_stored] = self._data[perm]\n</code></pre>"},{"location":"reference/activation_store/utils/","title":"Activation Store Utils","text":"<p>Activation Store Utils.</p>"},{"location":"reference/activation_store/utils/extend_resize/","title":"Resize Tensors for Extend Methods","text":"<p>Resize Tensors for Extend Methods.</p>"},{"location":"reference/activation_store/utils/extend_resize/#sparse_autoencoder.activation_store.utils.extend_resize.resize_to_list_vectors","title":"<code>resize_to_list_vectors(batched_tensor)</code>","text":"<p>Resize Extend List Vectors.</p> <p>Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a list of vectors each of size [neurons].</p> <p>Examples: With 2 axis (e.g. pos neuron):</p> <p>import torch input = torch.rand(3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '3 items of shape torch.Size([100])'</p> <p>With 3 axis (e.g. batch, pos, neuron):</p> <p>input = torch.randn(3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '9 items of shape torch.Size([100])'</p> <p>With 4 axis (e.g. batch, pos, head_idx, neuron)</p> <p>input = torch.rand(3, 3, 3, 100) res = resize_to_list_vectors(input) f\"{len(res)} items of shape {res[0].shape}\" '27 items of shape torch.Size([100])'</p> <p>Parameters:</p> Name Type Description Default <code>batched_tensor</code> <code>SourceModelActivations</code> <p>Input Activation Store Batch</p> required <p>Returns:</p> Type Description <code>list[InputOutputActivationVector]</code> <p>List of Activation Store Item Vectors</p> Source code in <code>sparse_autoencoder/activation_store/utils/extend_resize.py</code> <pre><code>def resize_to_list_vectors(\n    batched_tensor: SourceModelActivations,\n) -&gt; list[InputOutputActivationVector]:\n    \"\"\"Resize Extend List Vectors.\n\n    Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is\n    the neurons dimension), and returns a list of vectors each of size [neurons].\n\n    Examples:\n    With 2 axis (e.g. pos neuron):\n\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; input = torch.rand(3, 100)\n    &gt;&gt;&gt; res = resize_to_list_vectors(input)\n    &gt;&gt;&gt; f\"{len(res)} items of shape {res[0].shape}\"\n    '3 items of shape torch.Size([100])'\n\n    With 3 axis (e.g. batch, pos, neuron):\n\n    &gt;&gt;&gt; input = torch.randn(3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_list_vectors(input)\n    &gt;&gt;&gt; f\"{len(res)} items of shape {res[0].shape}\"\n    '9 items of shape torch.Size([100])'\n\n    With 4 axis (e.g. batch, pos, head_idx, neuron)\n\n    &gt;&gt;&gt; input = torch.rand(3, 3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_list_vectors(input)\n    &gt;&gt;&gt; f\"{len(res)} items of shape {res[0].shape}\"\n    '27 items of shape torch.Size([100])'\n\n    Args:\n        batched_tensor: Input Activation Store Batch\n\n    Returns:\n        List of Activation Store Item Vectors\n    \"\"\"\n    rearranged: InputOutputActivationBatch = rearrange(\n        batched_tensor,\n        \"... neurons -&gt; (...) neurons\",\n    )\n    res = rearranged.unbind(0)\n    return list(res)\n</code></pre>"},{"location":"reference/activation_store/utils/extend_resize/#sparse_autoencoder.activation_store.utils.extend_resize.resize_to_single_item_dimension","title":"<code>resize_to_single_item_dimension(batch_activations)</code>","text":"<p>Resize Extend Single Item Dimension.</p> <p>Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is the neurons dimension), and returns a single tensor of size [item, neurons].</p> <p>Examples: With 2 axis (e.g. pos neuron):</p> <p>import torch input = torch.rand(3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([3, 100])</p> <p>With 3 axis (e.g. batch, pos, neuron):</p> <p>input = torch.randn(3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([9, 100])</p> <p>With 4 axis (e.g. batch, pos, head_idx, neuron)</p> <p>input = torch.rand(3, 3, 3, 100) res = resize_to_single_item_dimension(input) res.shape torch.Size([27, 100])</p> <p>Parameters:</p> Name Type Description Default <code>batch_activations</code> <code>SourceModelActivations</code> <p>Input Activation Store Batch</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Single Tensor of Activation Store Items</p> Source code in <code>sparse_autoencoder/activation_store/utils/extend_resize.py</code> <pre><code>def resize_to_single_item_dimension(\n    batch_activations: SourceModelActivations,\n) -&gt; InputOutputActivationBatch:\n    \"\"\"Resize Extend Single Item Dimension.\n\n    Takes a tensor of activation vectors, with arbitrary numbers of dimensions (the last of which is\n    the neurons dimension), and returns a single tensor of size [item, neurons].\n\n    Examples:\n    With 2 axis (e.g. pos neuron):\n\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; input = torch.rand(3, 100)\n    &gt;&gt;&gt; res = resize_to_single_item_dimension(input)\n    &gt;&gt;&gt; res.shape\n    torch.Size([3, 100])\n\n    With 3 axis (e.g. batch, pos, neuron):\n\n    &gt;&gt;&gt; input = torch.randn(3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_single_item_dimension(input)\n    &gt;&gt;&gt; res.shape\n    torch.Size([9, 100])\n\n    With 4 axis (e.g. batch, pos, head_idx, neuron)\n\n    &gt;&gt;&gt; input = torch.rand(3, 3, 3, 100)\n    &gt;&gt;&gt; res = resize_to_single_item_dimension(input)\n    &gt;&gt;&gt; res.shape\n    torch.Size([27, 100])\n\n    Args:\n        batch_activations: Input Activation Store Batch\n\n    Returns:\n        Single Tensor of Activation Store Items\n    \"\"\"\n    return rearrange(batch_activations, \"... neurons -&gt; (...) neurons\")\n</code></pre>"},{"location":"reference/autoencoder/","title":"Sparse autoencoder model &amp; components","text":"<p>Sparse autoencoder model &amp; components.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/","title":"Abstract Sparse Autoencoder Model","text":"<p>Abstract Sparse Autoencoder Model.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder","title":"<code>AbstractAutoencoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Sparse Autoencoder Model.</p> Source code in <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code> <pre><code>class AbstractAutoencoder(Module, ABC):\n    \"\"\"Abstract Sparse Autoencoder Model.\"\"\"\n\n    @property\n    @abstractmethod\n    def encoder(self) -&gt; AbstractEncoder:\n        \"\"\"Encoder.\"\"\"\n\n    @property\n    @abstractmethod\n    def decoder(self) -&gt; AbstractDecoder:\n        \"\"\"Decoder.\"\"\"\n\n    @property\n    @abstractmethod\n    def pre_encoder_bias(self) -&gt; AbstractOuterBias:\n        \"\"\"Pre-encoder bias.\"\"\"\n\n    @property\n    @abstractmethod\n    def post_decoder_bias(self) -&gt; AbstractOuterBias:\n        \"\"\"Post-decoder bias.\"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: InputOutputActivationBatch,\n    ) -&gt; tuple[\n        LearnedActivationBatch,\n        InputOutputActivationBatch,\n    ]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n\n    @abstractmethod\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.decoder","title":"<code>decoder: AbstractDecoder</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Decoder.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.encoder","title":"<code>encoder: AbstractEncoder</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Encoder.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: AbstractOuterBias</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Post-decoder bias.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: AbstractOuterBias</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Pre-encoder bias.</p>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>tuple[LearnedActivationBatch, InputOutputActivationBatch]</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: InputOutputActivationBatch,\n) -&gt; tuple[\n    LearnedActivationBatch,\n    InputOutputActivationBatch,\n]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/abstract_autoencoder/#sparse_autoencoder.autoencoder.abstract_autoencoder.AbstractAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/abstract_autoencoder.py</code> <pre><code>@abstractmethod\ndef reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/model/","title":"The Sparse Autoencoder Model","text":"<p>The Sparse Autoencoder Model.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>AbstractAutoencoder</code></p> <p>Sparse Autoencoder Model.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>@final\nclass SparseAutoencoder(AbstractAutoencoder):\n    \"\"\"Sparse Autoencoder Model.\"\"\"\n\n    geometric_median_dataset: InputOutputActivationVector\n    \"\"\"Estimated Geometric Median of the Dataset.\n\n    Used for initialising :attr:`tied_bias`.\n    \"\"\"\n\n    tied_bias: InputOutputActivationBatch\n    \"\"\"Tied Bias Parameter.\n\n    The same bias is used pre-encoder and post-decoder.\n    \"\"\"\n\n    n_input_features: int\n    \"\"\"Number of Input Features.\"\"\"\n\n    n_learned_features: int\n    \"\"\"Number of Learned Features.\"\"\"\n\n    _pre_encoder_bias: TiedBias\n\n    _encoder: LinearEncoder\n\n    _decoder: UnitNormDecoder\n\n    _post_decoder_bias: TiedBias\n\n    @property\n    def pre_encoder_bias(self) -&gt; TiedBias:\n        \"\"\"Pre-encoder bias.\"\"\"\n        return self._pre_encoder_bias\n\n    @property\n    def encoder(self) -&gt; LinearEncoder:\n        \"\"\"Encoder.\"\"\"\n        return self._encoder\n\n    @property\n    def decoder(self) -&gt; UnitNormDecoder:\n        \"\"\"Decoder.\"\"\"\n        return self._decoder\n\n    @property\n    def post_decoder_bias(self) -&gt; TiedBias:\n        \"\"\"Post-decoder bias.\"\"\"\n        return self._post_decoder_bias\n\n    def __init__(\n        self,\n        n_input_features: int,\n        n_learned_features: int,\n        geometric_median_dataset: InputOutputActivationVector | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Sparse Autoencoder Model.\n\n        Args:\n            n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n                from TransformerLens).\n            n_learned_features: Number of learned features. The initial paper experimented with 1 to\n                256 times the number of input features, and primarily used a multiple of 8.\n            geometric_median_dataset: Estimated geometric median of the dataset.\n        \"\"\"\n        super().__init__()\n\n        self.n_input_features = n_input_features\n        self.n_learned_features = n_learned_features\n\n        # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n        # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n        if geometric_median_dataset is not None:\n            self.geometric_median_dataset = geometric_median_dataset.clone()\n            self.geometric_median_dataset.requires_grad = False\n        else:\n            self.geometric_median_dataset = torch.zeros(n_input_features)\n            self.geometric_median_dataset.requires_grad = False\n\n        # Initialize the tied bias\n        self.tied_bias = Parameter(torch.empty(n_input_features))\n        self.initialize_tied_parameters()\n\n        self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n        self._encoder = LinearEncoder(\n            input_features=n_input_features, learnt_features=n_learned_features\n        )\n\n        self._decoder = UnitNormDecoder(\n            learnt_features=n_learned_features, decoded_features=n_input_features\n        )\n\n        self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n\n    def forward(\n        self,\n        x: InputOutputActivationBatch,\n    ) -&gt; tuple[\n        LearnedActivationBatch,\n        InputOutputActivationBatch,\n    ]:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Tuple of learned activations and decoded activations.\n        \"\"\"\n        x = self._pre_encoder_bias(x)\n        learned_activations = self._encoder(x)\n        x = self._decoder(learned_activations)\n        decoded_activations = self._post_decoder_bias(x)\n        return learned_activations, decoded_activations\n\n    def initialize_tied_parameters(self) -&gt; None:\n        \"\"\"Initialize the tied parameters.\"\"\"\n        # The tied bias is initialised as the geometric median of the dataset\n        self.tied_bias.data = self.geometric_median_dataset\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n        self.initialize_tied_parameters()\n        for module in self.network:\n            if \"reset_parameters\" in dir(module):\n                module.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.decoder","title":"<code>decoder: UnitNormDecoder</code>  <code>property</code>","text":"<p>Decoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.encoder","title":"<code>encoder: LinearEncoder</code>  <code>property</code>","text":"<p>Encoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.geometric_median_dataset","title":"<code>geometric_median_dataset: InputOutputActivationVector</code>  <code>instance-attribute</code>","text":"<p>Estimated Geometric Median of the Dataset.</p> <p>Used for initialising :attr:<code>tied_bias</code>.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.n_input_features","title":"<code>n_input_features: int = n_input_features</code>  <code>instance-attribute</code>","text":"<p>Number of Input Features.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.n_learned_features","title":"<code>n_learned_features: int = n_learned_features</code>  <code>instance-attribute</code>","text":"<p>Number of Learned Features.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.post_decoder_bias","title":"<code>post_decoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Post-decoder bias.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.pre_encoder_bias","title":"<code>pre_encoder_bias: TiedBias</code>  <code>property</code>","text":"<p>Pre-encoder bias.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.tied_bias","title":"<code>tied_bias: InputOutputActivationBatch = Parameter(torch.empty(n_input_features))</code>  <code>instance-attribute</code>","text":"<p>Tied Bias Parameter.</p> <p>The same bias is used pre-encoder and post-decoder.</p>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.__init__","title":"<code>__init__(n_input_features, n_learned_features, geometric_median_dataset=None)</code>","text":"<p>Initialize the Sparse Autoencoder Model.</p> <p>Parameters:</p> Name Type Description Default <code>n_input_features</code> <code>int</code> <p>Number of input features (e.g. <code>d_mlp</code> if training on MLP activations from TransformerLens).</p> required <code>n_learned_features</code> <code>int</code> <p>Number of learned features. The initial paper experimented with 1 to 256 times the number of input features, and primarily used a multiple of 8.</p> required <code>geometric_median_dataset</code> <code>InputOutputActivationVector | None</code> <p>Estimated geometric median of the dataset.</p> <code>None</code> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def __init__(\n    self,\n    n_input_features: int,\n    n_learned_features: int,\n    geometric_median_dataset: InputOutputActivationVector | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Sparse Autoencoder Model.\n\n    Args:\n        n_input_features: Number of input features (e.g. `d_mlp` if training on MLP activations\n            from TransformerLens).\n        n_learned_features: Number of learned features. The initial paper experimented with 1 to\n            256 times the number of input features, and primarily used a multiple of 8.\n        geometric_median_dataset: Estimated geometric median of the dataset.\n    \"\"\"\n    super().__init__()\n\n    self.n_input_features = n_input_features\n    self.n_learned_features = n_learned_features\n\n    # Store the geometric median of the dataset (so that we can reset parameters). This is not a\n    # parameter itself (the tied bias parameter is used for that), so gradients are disabled.\n    if geometric_median_dataset is not None:\n        self.geometric_median_dataset = geometric_median_dataset.clone()\n        self.geometric_median_dataset.requires_grad = False\n    else:\n        self.geometric_median_dataset = torch.zeros(n_input_features)\n        self.geometric_median_dataset.requires_grad = False\n\n    # Initialize the tied bias\n    self.tied_bias = Parameter(torch.empty(n_input_features))\n    self.initialize_tied_parameters()\n\n    self._pre_encoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.PRE_ENCODER)\n\n    self._encoder = LinearEncoder(\n        input_features=n_input_features, learnt_features=n_learned_features\n    )\n\n    self._decoder = UnitNormDecoder(\n        learnt_features=n_learned_features, decoded_features=n_input_features\n    )\n\n    self._post_decoder_bias = TiedBias(self.tied_bias, TiedBiasPosition.POST_DECODER)\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>tuple[LearnedActivationBatch, InputOutputActivationBatch]</code> <p>Tuple of learned activations and decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def forward(\n    self,\n    x: InputOutputActivationBatch,\n) -&gt; tuple[\n    LearnedActivationBatch,\n    InputOutputActivationBatch,\n]:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Tuple of learned activations and decoded activations.\n    \"\"\"\n    x = self._pre_encoder_bias(x)\n    learned_activations = self._encoder(x)\n    x = self._decoder(learned_activations)\n    decoded_activations = self._post_decoder_bias(x)\n    return learned_activations, decoded_activations\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.initialize_tied_parameters","title":"<code>initialize_tied_parameters()</code>","text":"<p>Initialize the tied parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def initialize_tied_parameters(self) -&gt; None:\n    \"\"\"Initialize the tied parameters.\"\"\"\n    # The tied bias is initialised as the geometric median of the dataset\n    self.tied_bias.data = self.geometric_median_dataset\n</code></pre>"},{"location":"reference/autoencoder/model/#sparse_autoencoder.autoencoder.model.SparseAutoencoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/model.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n    self.initialize_tied_parameters()\n    for module in self.network:\n        if \"reset_parameters\" in dir(module):\n            module.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/components/","title":"Sparse autoencoder components","text":"<p>Sparse autoencoder components.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder","title":"<code>AbstractDecoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Decoder Module.</p> <p>Typically includes just a :attr:<code>weight</code> parameter.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>class AbstractDecoder(Module, ABC):\n    \"\"\"Abstract Decoder Module.\n\n    Typically includes just a :attr:`weight` parameter.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def weight(self) -&gt; DecoderWeights:\n        \"\"\"Weight.\n\n        Each column in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: LearnedActivationBatch,\n    ) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Learned activations.\n\n        Returns:\n            Decoded activations.\n        \"\"\"\n\n    @abstractmethod\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: LearntNeuronIndices,\n        updated_weights: DeadDecoderNeuronWeightUpdates,\n    ) -&gt; None:\n        \"\"\"Update decoder dictionary vectors.\n\n        Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n        this is used when resampling neurons (dictionary vectors) that have died.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if len(dictionary_vector_indices) == 0:\n            return\n\n        with torch.no_grad():\n            self.weight[:, dictionary_vector_indices] = updated_weights\n\n    @abstractmethod\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.weight","title":"<code>weight: DecoderWeights</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each column in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>  <code>abstractmethod</code>","text":"<p>Constrain the weights to have unit norm.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>LearnedActivationBatch</code> <p>Learned activations.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: LearnedActivationBatch,\n) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Learned activations.\n\n    Returns:\n        Decoded activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.reset_parameters","title":"<code>reset_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractDecoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_weights)</code>","text":"<p>Update decoder dictionary vectors.</p> <p>Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically this is used when resampling neurons (dictionary vectors) that have died.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>LearntNeuronIndices</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_weights</code> <code>DeadDecoderNeuronWeightUpdates</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: LearntNeuronIndices,\n    updated_weights: DeadDecoderNeuronWeightUpdates,\n) -&gt; None:\n    \"\"\"Update decoder dictionary vectors.\n\n    Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n    this is used when resampling neurons (dictionary vectors) that have died.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if len(dictionary_vector_indices) == 0:\n        return\n\n    with torch.no_grad():\n        self.weight[:, dictionary_vector_indices] = updated_weights\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder","title":"<code>AbstractEncoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract encoder module.</p> <p>Typically includes :attr:<code>weights</code> and :attr:<code>bias</code> parameters, as well as an activation function.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>class AbstractEncoder(Module, ABC):\n    \"\"\"Abstract encoder module.\n\n    Typically includes :attr:`weights` and :attr:`bias` parameters, as well as an activation\n    function.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def weight(self) -&gt; EncoderWeights:\n        \"\"\"Weight.\n\n        Each row in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def bias(self) -&gt; LearntActivationVector:\n        \"\"\"Bias.\"\"\"\n\n    @abstractmethod\n    def forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input activations.\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: LearntNeuronIndices,\n        updated_dictionary_weights: DeadEncoderNeuronWeightUpdates,\n    ) -&gt; None:\n        \"\"\"Update encoder dictionary vectors.\n\n        Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_dictionary_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if len(dictionary_vector_indices) == 0:\n            return\n\n        with torch.no_grad():\n            self.weight[dictionary_vector_indices, :] = updated_dictionary_weights\n\n    @final\n    def update_bias(\n        self,\n        update_parameter_indices: InputOutputNeuronIndices,\n        updated_bias_features: LearntActivationVector | float,\n    ) -&gt; None:\n        \"\"\"Update encoder bias.\n\n        Args:\n            update_parameter_indices: Indices of the bias features to update.\n            updated_bias_features: Updated bias features for just these indices.\n        \"\"\"\n        if len(update_parameter_indices) == 0:\n            return\n\n        with torch.no_grad():\n            self.bias[update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.bias","title":"<code>bias: LearntActivationVector</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.weight","title":"<code>weight: EncoderWeights</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each row in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input activations.</p> required <p>Returns:</p> Type Description <code>LearnedActivationBatch</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@abstractmethod\ndef forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input activations.\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.update_bias","title":"<code>update_bias(update_parameter_indices, updated_bias_features)</code>","text":"<p>Update encoder bias.</p> <p>Parameters:</p> Name Type Description Default <code>update_parameter_indices</code> <code>InputOutputNeuronIndices</code> <p>Indices of the bias features to update.</p> required <code>updated_bias_features</code> <code>LearntActivationVector | float</code> <p>Updated bias features for just these indices.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_bias(\n    self,\n    update_parameter_indices: InputOutputNeuronIndices,\n    updated_bias_features: LearntActivationVector | float,\n) -&gt; None:\n    \"\"\"Update encoder bias.\n\n    Args:\n        update_parameter_indices: Indices of the bias features to update.\n        updated_bias_features: Updated bias features for just these indices.\n    \"\"\"\n    if len(update_parameter_indices) == 0:\n        return\n\n    with torch.no_grad():\n        self.bias[update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractEncoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_dictionary_weights)</code>","text":"<p>Update encoder dictionary vectors.</p> <p>Updates the dictionary vectors (columns in the weight matrix) with the given values.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>LearntNeuronIndices</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_dictionary_weights</code> <code>DeadEncoderNeuronWeightUpdates</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: LearntNeuronIndices,\n    updated_dictionary_weights: DeadEncoderNeuronWeightUpdates,\n) -&gt; None:\n    \"\"\"Update encoder dictionary vectors.\n\n    Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_dictionary_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if len(dictionary_vector_indices) == 0:\n        return\n\n    with torch.no_grad():\n        self.weight[dictionary_vector_indices, :] = updated_dictionary_weights\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractOuterBias","title":"<code>AbstractOuterBias</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Pre-Encoder or Post-Decoder Bias Module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>class AbstractOuterBias(Module, ABC):\n    \"\"\"Abstract Pre-Encoder or Post-Decoder Bias Module.\"\"\"\n\n    @property\n    @abstractmethod\n    def bias(self) -&gt; InputOutputActivationVector:\n        \"\"\"Bias.\n\n        May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: InputOutputActivationBatch,\n    ) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractOuterBias.bias","title":"<code>bias: InputOutputActivationVector</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p> <p>May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.AbstractOuterBias.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: InputOutputActivationBatch,\n) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder","title":"<code>LinearEncoder</code>","text":"<p>             Bases: <code>AbstractEncoder</code></p> <p>Linear encoder layer.</p> <p>Linear encoder layer (essentially <code>nn.Linear</code>, with a ReLU activation function). Designed to be used as the encoder in a sparse autoencoder (excluding any outer tied bias).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\     W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\     b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\     f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output} \\end{align*} \\] Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@final\nclass LinearEncoder(AbstractEncoder):\n    r\"\"\"Linear encoder layer.\n\n    Linear encoder layer (essentially `nn.Linear`, with a ReLU activation function). Designed to be\n    used as the encoder in a sparse autoencoder (excluding any outer tied bias).\n\n    $$\n    \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\\n        W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\\n        b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\\n        f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output}\n    \\end{align*}\n    $$\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _input_features: int\n    \"\"\"Number of decoded features (outputs from this layer).\"\"\"\n\n    _weight: EncoderWeights\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    _bias: LearntActivationVector\n    \"\"\"Bias parameter internal state.\"\"\"\n\n    @property\n    def weight(self) -&gt; EncoderWeights:\n        \"\"\"Weight parameter.\n\n        Each row in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    @property\n    def bias(self) -&gt; LearntActivationVector:\n        \"\"\"Bias parameter.\"\"\"\n        return self._bias\n\n    activation_function: ReLU\n    \"\"\"Activation function.\"\"\"\n\n    def __init__(\n        self,\n        input_features: int,\n        learnt_features: int,\n    ):\n        \"\"\"Initialize the linear encoder layer.\"\"\"\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._input_features = input_features\n\n        self._weight = Parameter(\n            torch.empty(\n                (learnt_features, input_features),\n            )\n        )\n        self._bias = Parameter(torch.zeros(learnt_features))\n        self.activation_function = ReLU()\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\"\"\"\n        # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n        # `nonlinerity` must be changed.\n        init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n        # Bias (approach from nn.Linear)\n        fan_in = self._weight.size(1)\n        bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n        init.uniform_(self._bias, -bound, bound)\n\n    def forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        z = functional.linear(x, self.weight, self.bias)\n        return self.activation_function(z)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.activation_function","title":"<code>activation_function: ReLU = ReLU()</code>  <code>instance-attribute</code>","text":"<p>Activation function.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.bias","title":"<code>bias: LearntActivationVector</code>  <code>property</code>","text":"<p>Bias parameter.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.weight","title":"<code>weight: EncoderWeights</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each row in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.__init__","title":"<code>__init__(input_features, learnt_features)</code>","text":"<p>Initialize the linear encoder layer.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def __init__(\n    self,\n    input_features: int,\n    learnt_features: int,\n):\n    \"\"\"Initialize the linear encoder layer.\"\"\"\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._input_features = input_features\n\n    self._weight = Parameter(\n        torch.empty(\n            (learnt_features, input_features),\n        )\n    )\n    self._bias = Parameter(torch.zeros(learnt_features))\n    self.activation_function = ReLU()\n\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>LearnedActivationBatch</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    z = functional.linear(x, self.weight, self.bias)\n    return self.activation_function(z)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.LinearEncoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\"\"\"\n    # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n    # `nonlinerity` must be changed.\n    init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n    # Bias (approach from nn.Linear)\n    fan_in = self._weight.size(1)\n    bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n    init.uniform_(self._bias, -bound, bound)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias","title":"<code>TiedBias</code>","text":"<p>             Bases: <code>AbstractOuterBias</code></p> <p>Tied Bias Layer.</p> <p>The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding.</p> <p>The bias parameter must be initialised in the parent module, and then passed to this layer.</p> <p>https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>@final\nclass TiedBias(AbstractOuterBias):\n    \"\"\"Tied Bias Layer.\n\n    The tied pre-encoder bias is a learned bias term that is subtracted from the input before\n    encoding, and added back after decoding.\n\n    The bias parameter must be initialised in the parent module, and then passed to this layer.\n\n    https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias\n    \"\"\"\n\n    _bias_position: TiedBiasPosition\n\n    _bias_reference: InputOutputActivationVector\n\n    @property\n    def bias(self) -&gt; InputOutputActivationVector:\n        \"\"\"Bias.\"\"\"\n        return self._bias_reference\n\n    def __init__(\n        self,\n        bias_reference: InputOutputActivationVector,\n        position: TiedBiasPosition,\n    ) -&gt; None:\n        \"\"\"Initialize the bias layer.\n\n        Args:\n            bias_reference: Tied bias parameter (initialised in the parent module), used for both\n                the pre-encoder and post-encoder bias. The original paper initialised this using the\n                geometric median of the dataset.\n            position: Whether this is the pre-encoder or post-encoder bias.\n        \"\"\"\n        super().__init__()\n\n        self._bias_reference = bias_reference\n\n        # Support string literals as well as enums\n        self._bias_position = position\n\n    def forward(\n        self,\n        x: InputOutputActivationBatch,\n    ) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        # If this is the pre-encoder bias, we subtract the bias from the input.\n        if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n            return x - self.bias\n\n        # If it's the post-encoder bias, we add the bias to the input.\n        return x + self.bias\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.bias","title":"<code>bias: InputOutputActivationVector</code>  <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.__init__","title":"<code>__init__(bias_reference, position)</code>","text":"<p>Initialize the bias layer.</p> <p>Parameters:</p> Name Type Description Default <code>bias_reference</code> <code>InputOutputActivationVector</code> <p>Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset.</p> required <code>position</code> <code>TiedBiasPosition</code> <p>Whether this is the pre-encoder or post-encoder bias.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def __init__(\n    self,\n    bias_reference: InputOutputActivationVector,\n    position: TiedBiasPosition,\n) -&gt; None:\n    \"\"\"Initialize the bias layer.\n\n    Args:\n        bias_reference: Tied bias parameter (initialised in the parent module), used for both\n            the pre-encoder and post-encoder bias. The original paper initialised this using the\n            geometric median of the dataset.\n        position: Whether this is the pre-encoder or post-encoder bias.\n    \"\"\"\n    super().__init__()\n\n    self._bias_reference = bias_reference\n\n    # Support string literals as well as enums\n    self._bias_position = position\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBias.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def forward(\n    self,\n    x: InputOutputActivationBatch,\n) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    # If this is the pre-encoder bias, we subtract the bias from the input.\n    if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n        return x - self.bias\n\n    # If it's the post-encoder bias, we add the bias to the input.\n    return x + self.bias\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.TiedBiasPosition","title":"<code>TiedBiasPosition</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Tied Bias Position.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>class TiedBiasPosition(str, Enum):\n    \"\"\"Tied Bias Position.\"\"\"\n\n    PRE_ENCODER = \"pre_encoder\"\n    POST_DECODER = \"post_decoder\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder","title":"<code>UnitNormDecoder</code>","text":"<p>             Bases: <code>AbstractDecoder</code></p> <p>Constrained unit norm linear decoder layer.</p> <p>Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. It also requires <code>constrain_weights_unit_norm</code> to be called after each gradient step, to prevent drift of the dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\     W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\     z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)} \\end{align*} \\] Motivation <p>Normalisation of the columns (dictionary features) prevents the model from reducing the sparsity loss term by increasing the size of the feature vectors in \\(W_d\\).</p> <p>Note that the Towards Monosemanticity: Decomposing Language Models With Dictionary Learning paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>@final\nclass UnitNormDecoder(AbstractDecoder):\n    r\"\"\"Constrained unit norm linear decoder layer.\n\n    Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are\n    constrained to have unit norm. This is done by removing the gradient information parallel to the\n    dictionary vectors before applying the gradient step, using a backward hook. It also requires\n    `constrain_weights_unit_norm` to be called after each gradient step, to prevent drift of the\n    dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the\n    gradient, but instead follow a modified gradient that includes momentum).\n\n    $$ \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\\n        W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\\n        z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)}\n    \\end{align*} $$\n\n    Motivation:\n        Normalisation of the columns (dictionary features) prevents the model from reducing the\n        sparsity loss term by increasing the size of the feature vectors in $W_d$.\n\n        Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary\n        Learning* paper found that removing the gradient information parallel to the dictionary\n        vectors before applying the gradient step, rather than resetting the dictionary vectors to\n        unit norm after each gradient step, results in a small but real reduction in total\n        loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _decoded_features: int\n    \"\"\"Number of decoded features (outputs from this layer).\"\"\"\n\n    _weight: DecoderWeights\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    @property\n    def weight(self) -&gt; DecoderWeights:\n        \"\"\"Weight parameter.\n\n        Each column in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    def __init__(\n        self,\n        learnt_features: int,\n        decoded_features: int,\n        *,\n        enable_gradient_hook: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the constrained unit norm linear layer.\n\n        Args:\n            learnt_features: Number of learnt features in the autoencoder.\n            decoded_features: Number of decoded (output) features in the autoencoder.\n            enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n                applying the gradient step, to maintain unit norm of the dictionary vectors).\n        \"\"\"\n        # Create the linear layer as per the standard PyTorch linear layer\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._decoded_features = decoded_features\n        self._weight = Parameter(\n            torch.empty(\n                (decoded_features, learnt_features),\n            )\n        )\n        self.reset_parameters()\n\n        # Register backward hook to remove any gradient information parallel to the dictionary\n        # vectors (columns of the weight matrix) before applying the gradient step.\n        if enable_gradient_hook:\n            self._weight.register_hook(self._weight_backward_hook)\n\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\n\n        Warning:\n            Note this must be called after each gradient step. This is because optimisers such as\n            Adam don't strictly follow the gradient, but instead follow a modified gradient that\n            includes momentum. This means that the gradient step can change the norm of the\n            dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n            Note this can't be applied directly in the backward hook, as it would interfere with a\n            variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n            with asynchronous operations, etc).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; layer = UnitNormDecoder(3, 3)\n            &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n            &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n            &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0]\n\n        \"\"\"\n        with torch.no_grad():\n            torch.nn.functional.normalize(self._weight, dim=0, out=self._weight)\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n            &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3)\n            &gt;&gt;&gt; layer.reset_parameters()\n            &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n            &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0, 1.0]\n\n        \"\"\"\n        # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n        # normalisation here, since we immediately scale the weights to have unit norm (so the\n        # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n        self._weight: EncoderWeights = init.normal_(self._weight, mean=0, std=1)\n\n        # Scale so that each row has unit norm\n        self.constrain_weights_unit_norm()\n\n    def _weight_backward_hook(\n        self,\n        grad: EncoderWeights,\n    ) -&gt; EncoderWeights:\n        r\"\"\"Unit norm backward hook.\n\n        By subtracting the projection of the gradient onto the dictionary vectors, we remove the\n        component of the gradient that is parallel to the dictionary vectors and just keep the\n        component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere).\n        The result is that the backward pass does not change the norm of the dictionary vectors.\n\n        $$\n        \\begin{align*}\n            W_d &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Decoder weight matrix} \\\\\n            g &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Gradient w.r.t. } W_d\n                \\text{ from the backward pass} \\\\\n            W_{d, \\text{norm}} &amp;= \\frac{W_d}{\\|W_d\\|} = \\text{Normalized decoder weight matrix\n                (over columns)} \\\\\n            g_{\\parallel} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g\n                \\text{ parallel to } W_{d, \\text{norm}} \\\\\n            g_{\\perp} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g \\text{ orthogonal to }\n                W_{d, \\text{norm}} \\\\\n            g_{\\parallel} &amp;= W_{d, \\text{norm}} \\cdot (W_{d, \\text{norm}}^\\top \\cdot g) \\\\\n            g_{\\perp} &amp;= g - g_{\\parallel} =\n                \\text{Adjusted gradient with parallel component removed} \\\\\n        \\end{align*}\n        $$\n\n        Args:\n            grad: Gradient with respect to the weights.\n\n        Returns:\n            Gradient with respect to the weights, with the component parallel to the dictionary\n            vectors removed.\n        \"\"\"\n        # Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can\n        # be thought of as vectors that end on the circumference of a hypersphere. The projection of\n        # the gradient onto the dictionary vectors is the component of the gradient that is parallel\n        # to the dictionary vectors, i.e. the component that moves to or from the center of the\n        # hypersphere.\n        normalized_weight: EncoderWeights = self._weight / torch.norm(\n            self._weight, dim=0, keepdim=True\n        )\n\n        scalar_projections = einops.einsum(\n            grad,\n            normalized_weight,\n            f\"{Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}, \\\n                {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        projection = einops.einsum(\n            scalar_projections,\n            normalized_weight,\n            f\"{Axis.INPUT_OUTPUT_FEATURE}, \\\n                {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        # Subtracting the parallel component from the gradient leaves only the component that is\n        # orthogonal to the dictionary vectors, i.e. the component that moves around the surface of\n        # the hypersphere.\n        return grad - projection\n\n    def forward(self, x: LearnedActivationBatch) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        return torch.nn.functional.linear(x, self._weight)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.weight","title":"<code>weight: DecoderWeights</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each column in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.__init__","title":"<code>__init__(learnt_features, decoded_features, *, enable_gradient_hook=True)</code>","text":"<p>Initialize the constrained unit norm linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>decoded_features</code> <code>int</code> <p>Number of decoded (output) features in the autoencoder.</p> required <code>enable_gradient_hook</code> <code>bool</code> <p>Enable the gradient backwards hook (modify the gradient before applying the gradient step, to maintain unit norm of the dictionary vectors).</p> <code>True</code> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def __init__(\n    self,\n    learnt_features: int,\n    decoded_features: int,\n    *,\n    enable_gradient_hook: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the constrained unit norm linear layer.\n\n    Args:\n        learnt_features: Number of learnt features in the autoencoder.\n        decoded_features: Number of decoded (output) features in the autoencoder.\n        enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n            applying the gradient step, to maintain unit norm of the dictionary vectors).\n    \"\"\"\n    # Create the linear layer as per the standard PyTorch linear layer\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._decoded_features = decoded_features\n    self._weight = Parameter(\n        torch.empty(\n            (decoded_features, learnt_features),\n        )\n    )\n    self.reset_parameters()\n\n    # Register backward hook to remove any gradient information parallel to the dictionary\n    # vectors (columns of the weight matrix) before applying the gradient step.\n    if enable_gradient_hook:\n        self._weight.register_hook(self._weight_backward_hook)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>","text":"<p>Constrain the weights to have unit norm.</p> Warning <p>Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook <code>_weight_backward_hook</code> is applied.</p> <p>Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc).</p> Example <p>import torch layer = UnitNormDecoder(3, 3) layer.weight.data = torch.ones((3, 3)) * 10 layer.constrain_weights_unit_norm() column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0)) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0]</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\n\n    Warning:\n        Note this must be called after each gradient step. This is because optimisers such as\n        Adam don't strictly follow the gradient, but instead follow a modified gradient that\n        includes momentum. This means that the gradient step can change the norm of the\n        dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n        Note this can't be applied directly in the backward hook, as it would interfere with a\n        variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n        with asynchronous operations, etc).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; layer = UnitNormDecoder(3, 3)\n        &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n        &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n        &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0]\n\n    \"\"\"\n    with torch.no_grad():\n        torch.nn.functional.normalize(self._weight, dim=0, out=self._weight)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>LearnedActivationBatch</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def forward(self, x: LearnedActivationBatch) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    return torch.nn.functional.linear(x, self._weight)\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Example <p>import torch</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n        &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3)\n        &gt;&gt;&gt; layer.reset_parameters()\n        &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n        &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0, 1.0]\n\n    \"\"\"\n    # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n    # normalisation here, since we immediately scale the weights to have unit norm (so the\n    # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n    self._weight: EncoderWeights = init.normal_(self._weight, mean=0, std=1)\n\n    # Scale so that each row has unit norm\n    self.constrain_weights_unit_norm()\n</code></pre>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.reset_parameters--create-a-layer-with-4-columns-learnt-features-and-3-rows-decoded-features","title":"Create a layer with 4 columns (learnt features) and 3 rows (decoded features)","text":"<p>layer = UnitNormDecoder(learnt_features=4, decoded_features=3) layer.reset_parameters()</p>"},{"location":"reference/autoencoder/components/#sparse_autoencoder.autoencoder.components.UnitNormDecoder.reset_parameters--get-the-norm-across-the-rows-by-summing-across-the-columns","title":"Get the norm across the rows (by summing across the columns)","text":"<p>column_norms = torch.sum(layer.weight ** 2, dim=0) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0, 1.0]</p>"},{"location":"reference/autoencoder/components/abstract_decoder/","title":"Abstract Sparse Autoencoder Model","text":"<p>Abstract Sparse Autoencoder Model.</p>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder","title":"<code>AbstractDecoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Decoder Module.</p> <p>Typically includes just a :attr:<code>weight</code> parameter.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>class AbstractDecoder(Module, ABC):\n    \"\"\"Abstract Decoder Module.\n\n    Typically includes just a :attr:`weight` parameter.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def weight(self) -&gt; DecoderWeights:\n        \"\"\"Weight.\n\n        Each column in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: LearnedActivationBatch,\n    ) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Learned activations.\n\n        Returns:\n            Decoded activations.\n        \"\"\"\n\n    @abstractmethod\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Reset the parameters.\"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: LearntNeuronIndices,\n        updated_weights: DeadDecoderNeuronWeightUpdates,\n    ) -&gt; None:\n        \"\"\"Update decoder dictionary vectors.\n\n        Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n        this is used when resampling neurons (dictionary vectors) that have died.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if len(dictionary_vector_indices) == 0:\n            return\n\n        with torch.no_grad():\n            self.weight[:, dictionary_vector_indices] = updated_weights\n\n    @abstractmethod\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.weight","title":"<code>weight: DecoderWeights</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each column in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>  <code>abstractmethod</code>","text":"<p>Constrain the weights to have unit norm.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>LearnedActivationBatch</code> <p>Learned activations.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: LearnedActivationBatch,\n) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Learned activations.\n\n    Returns:\n        Decoded activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.reset_parameters","title":"<code>reset_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@abstractmethod\ndef reset_parameters(self) -&gt; None:\n    \"\"\"Reset the parameters.\"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_decoder/#sparse_autoencoder.autoencoder.components.abstract_decoder.AbstractDecoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_weights)</code>","text":"<p>Update decoder dictionary vectors.</p> <p>Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically this is used when resampling neurons (dictionary vectors) that have died.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>LearntNeuronIndices</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_weights</code> <code>DeadDecoderNeuronWeightUpdates</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_decoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: LearntNeuronIndices,\n    updated_weights: DeadDecoderNeuronWeightUpdates,\n) -&gt; None:\n    \"\"\"Update decoder dictionary vectors.\n\n    Updates the dictionary vectors (rows in the weight matrix) with the given values. Typically\n    this is used when resampling neurons (dictionary vectors) that have died.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if len(dictionary_vector_indices) == 0:\n        return\n\n    with torch.no_grad():\n        self.weight[:, dictionary_vector_indices] = updated_weights\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/","title":"Abstract Encoder","text":"<p>Abstract Encoder.</p>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder","title":"<code>AbstractEncoder</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract encoder module.</p> <p>Typically includes :attr:<code>weights</code> and :attr:<code>bias</code> parameters, as well as an activation function.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>class AbstractEncoder(Module, ABC):\n    \"\"\"Abstract encoder module.\n\n    Typically includes :attr:`weights` and :attr:`bias` parameters, as well as an activation\n    function.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def weight(self) -&gt; EncoderWeights:\n        \"\"\"Weight.\n\n        Each row in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def bias(self) -&gt; LearntActivationVector:\n        \"\"\"Bias.\"\"\"\n\n    @abstractmethod\n    def forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input activations.\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n\n    @final\n    def update_dictionary_vectors(\n        self,\n        dictionary_vector_indices: LearntNeuronIndices,\n        updated_dictionary_weights: DeadEncoderNeuronWeightUpdates,\n    ) -&gt; None:\n        \"\"\"Update encoder dictionary vectors.\n\n        Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n        Args:\n            dictionary_vector_indices: Indices of the dictionary vectors to update.\n            updated_dictionary_weights: Updated weights for just these dictionary vectors.\n        \"\"\"\n        if len(dictionary_vector_indices) == 0:\n            return\n\n        with torch.no_grad():\n            self.weight[dictionary_vector_indices, :] = updated_dictionary_weights\n\n    @final\n    def update_bias(\n        self,\n        update_parameter_indices: InputOutputNeuronIndices,\n        updated_bias_features: LearntActivationVector | float,\n    ) -&gt; None:\n        \"\"\"Update encoder bias.\n\n        Args:\n            update_parameter_indices: Indices of the bias features to update.\n            updated_bias_features: Updated bias features for just these indices.\n        \"\"\"\n        if len(update_parameter_indices) == 0:\n            return\n\n        with torch.no_grad():\n            self.bias[update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.bias","title":"<code>bias: LearntActivationVector</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.weight","title":"<code>weight: EncoderWeights</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Weight.</p> <p>Each row in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input activations.</p> required <p>Returns:</p> Type Description <code>LearnedActivationBatch</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@abstractmethod\ndef forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input activations.\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.update_bias","title":"<code>update_bias(update_parameter_indices, updated_bias_features)</code>","text":"<p>Update encoder bias.</p> <p>Parameters:</p> Name Type Description Default <code>update_parameter_indices</code> <code>InputOutputNeuronIndices</code> <p>Indices of the bias features to update.</p> required <code>updated_bias_features</code> <code>LearntActivationVector | float</code> <p>Updated bias features for just these indices.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_bias(\n    self,\n    update_parameter_indices: InputOutputNeuronIndices,\n    updated_bias_features: LearntActivationVector | float,\n) -&gt; None:\n    \"\"\"Update encoder bias.\n\n    Args:\n        update_parameter_indices: Indices of the bias features to update.\n        updated_bias_features: Updated bias features for just these indices.\n    \"\"\"\n    if len(update_parameter_indices) == 0:\n        return\n\n    with torch.no_grad():\n        self.bias[update_parameter_indices] = updated_bias_features\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_encoder/#sparse_autoencoder.autoencoder.components.abstract_encoder.AbstractEncoder.update_dictionary_vectors","title":"<code>update_dictionary_vectors(dictionary_vector_indices, updated_dictionary_weights)</code>","text":"<p>Update encoder dictionary vectors.</p> <p>Updates the dictionary vectors (columns in the weight matrix) with the given values.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_vector_indices</code> <code>LearntNeuronIndices</code> <p>Indices of the dictionary vectors to update.</p> required <code>updated_dictionary_weights</code> <code>DeadEncoderNeuronWeightUpdates</code> <p>Updated weights for just these dictionary vectors.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/abstract_encoder.py</code> <pre><code>@final\ndef update_dictionary_vectors(\n    self,\n    dictionary_vector_indices: LearntNeuronIndices,\n    updated_dictionary_weights: DeadEncoderNeuronWeightUpdates,\n) -&gt; None:\n    \"\"\"Update encoder dictionary vectors.\n\n    Updates the dictionary vectors (columns in the weight matrix) with the given values.\n\n    Args:\n        dictionary_vector_indices: Indices of the dictionary vectors to update.\n        updated_dictionary_weights: Updated weights for just these dictionary vectors.\n    \"\"\"\n    if len(dictionary_vector_indices) == 0:\n        return\n\n    with torch.no_grad():\n        self.weight[dictionary_vector_indices, :] = updated_dictionary_weights\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_outer_bias/","title":"Abstract Outer Bias","text":"<p>Abstract Outer Bias.</p> <p>This can be extended to create e.g. a pre-encoder and post-decoder bias.</p>"},{"location":"reference/autoencoder/components/abstract_outer_bias/#sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias","title":"<code>AbstractOuterBias</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract Pre-Encoder or Post-Decoder Bias Module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>class AbstractOuterBias(Module, ABC):\n    \"\"\"Abstract Pre-Encoder or Post-Decoder Bias Module.\"\"\"\n\n    @property\n    @abstractmethod\n    def bias(self) -&gt; InputOutputActivationVector:\n        \"\"\"Bias.\n\n        May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        x: InputOutputActivationBatch,\n    ) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n        Returns:\n            Resulting activations.\n        \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/abstract_outer_bias/#sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias.bias","title":"<code>bias: InputOutputActivationVector</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Bias.</p> <p>May be a reference to a bias parameter in the parent module, if using e.g. a tied bias.</p>"},{"location":"reference/autoencoder/components/abstract_outer_bias/#sparse_autoencoder.autoencoder.components.abstract_outer_bias.AbstractOuterBias.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input activations (e.g. activations from an MLP layer in a transformer model).</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Resulting activations.</p> Source code in <code>sparse_autoencoder/autoencoder/components/abstract_outer_bias.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    x: InputOutputActivationBatch,\n) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input activations (e.g. activations from an MLP layer in a transformer model).\n\n    Returns:\n        Resulting activations.\n    \"\"\"\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/","title":"Linear encoder layer","text":"<p>Linear encoder layer.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder","title":"<code>LinearEncoder</code>","text":"<p>             Bases: <code>AbstractEncoder</code></p> <p>Linear encoder layer.</p> <p>Linear encoder layer (essentially <code>nn.Linear</code>, with a ReLU activation function). Designed to be used as the encoder in a sparse autoencoder (excluding any outer tied bias).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\     W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\     b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\     f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output} \\end{align*} \\] Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>@final\nclass LinearEncoder(AbstractEncoder):\n    r\"\"\"Linear encoder layer.\n\n    Linear encoder layer (essentially `nn.Linear`, with a ReLU activation function). Designed to be\n    used as the encoder in a sparse autoencoder (excluding any outer tied bias).\n\n    $$\n    \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        \\overline{\\mathbf{x}} \\in \\mathbb{R}^{b \\times n} &amp;= \\text{input after tied bias} \\\\\n        W_e \\in \\mathbb{R}^{m \\times n} &amp;= \\text{weight matrix} \\\\\n        b_e \\in \\mathbb{R}^{m} &amp;= \\text{bias vector} \\\\\n        f &amp;= \\text{ReLU}(\\overline{\\mathbf{x}} W_e^T + b_e) = \\text{LinearEncoder output}\n    \\end{align*}\n    $$\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _input_features: int\n    \"\"\"Number of decoded features (outputs from this layer).\"\"\"\n\n    _weight: EncoderWeights\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    _bias: LearntActivationVector\n    \"\"\"Bias parameter internal state.\"\"\"\n\n    @property\n    def weight(self) -&gt; EncoderWeights:\n        \"\"\"Weight parameter.\n\n        Each row in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    @property\n    def bias(self) -&gt; LearntActivationVector:\n        \"\"\"Bias parameter.\"\"\"\n        return self._bias\n\n    activation_function: ReLU\n    \"\"\"Activation function.\"\"\"\n\n    def __init__(\n        self,\n        input_features: int,\n        learnt_features: int,\n    ):\n        \"\"\"Initialize the linear encoder layer.\"\"\"\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._input_features = input_features\n\n        self._weight = Parameter(\n            torch.empty(\n                (learnt_features, input_features),\n            )\n        )\n        self._bias = Parameter(torch.zeros(learnt_features))\n        self.activation_function = ReLU()\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\"\"\"\n        # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n        # `nonlinerity` must be changed.\n        init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n        # Bias (approach from nn.Linear)\n        fan_in = self._weight.size(1)\n        bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n        init.uniform_(self._bias, -bound, bound)\n\n    def forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        z = functional.linear(x, self.weight, self.bias)\n        return self.activation_function(z)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.activation_function","title":"<code>activation_function: ReLU = ReLU()</code>  <code>instance-attribute</code>","text":"<p>Activation function.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.bias","title":"<code>bias: LearntActivationVector</code>  <code>property</code>","text":"<p>Bias parameter.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.weight","title":"<code>weight: EncoderWeights</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each row in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.__init__","title":"<code>__init__(input_features, learnt_features)</code>","text":"<p>Initialize the linear encoder layer.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def __init__(\n    self,\n    input_features: int,\n    learnt_features: int,\n):\n    \"\"\"Initialize the linear encoder layer.\"\"\"\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._input_features = input_features\n\n    self._weight = Parameter(\n        torch.empty(\n            (learnt_features, input_features),\n        )\n    )\n    self._bias = Parameter(torch.zeros(learnt_features))\n    self.activation_function = ReLU()\n\n    self.reset_parameters()\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._input_features}, out_features={self._learnt_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>LearnedActivationBatch</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def forward(self, x: InputOutputActivationBatch) -&gt; LearnedActivationBatch:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    z = functional.linear(x, self.weight, self.bias)\n    return self.activation_function(z)\n</code></pre>"},{"location":"reference/autoencoder/components/linear_encoder/#sparse_autoencoder.autoencoder.components.linear_encoder.LinearEncoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Source code in <code>sparse_autoencoder/autoencoder/components/linear_encoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\"\"\"\n    # Assumes we are using ReLU activation function (for e.g. leaky ReLU, the `a` parameter and\n    # `nonlinerity` must be changed.\n    init.kaiming_uniform_(self._weight, nonlinearity=\"relu\")\n\n    # Bias (approach from nn.Linear)\n    fan_in = self._weight.size(1)\n    bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n    init.uniform_(self._bias, -bound, bound)\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/","title":"Tied Biases (Pre-Encoder and Post-Decoder)","text":"<p>Tied Biases (Pre-Encoder and Post-Decoder).</p>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias","title":"<code>TiedBias</code>","text":"<p>             Bases: <code>AbstractOuterBias</code></p> <p>Tied Bias Layer.</p> <p>The tied pre-encoder bias is a learned bias term that is subtracted from the input before encoding, and added back after decoding.</p> <p>The bias parameter must be initialised in the parent module, and then passed to this layer.</p> <p>https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>@final\nclass TiedBias(AbstractOuterBias):\n    \"\"\"Tied Bias Layer.\n\n    The tied pre-encoder bias is a learned bias term that is subtracted from the input before\n    encoding, and added back after decoding.\n\n    The bias parameter must be initialised in the parent module, and then passed to this layer.\n\n    https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-bias\n    \"\"\"\n\n    _bias_position: TiedBiasPosition\n\n    _bias_reference: InputOutputActivationVector\n\n    @property\n    def bias(self) -&gt; InputOutputActivationVector:\n        \"\"\"Bias.\"\"\"\n        return self._bias_reference\n\n    def __init__(\n        self,\n        bias_reference: InputOutputActivationVector,\n        position: TiedBiasPosition,\n    ) -&gt; None:\n        \"\"\"Initialize the bias layer.\n\n        Args:\n            bias_reference: Tied bias parameter (initialised in the parent module), used for both\n                the pre-encoder and post-encoder bias. The original paper initialised this using the\n                geometric median of the dataset.\n            position: Whether this is the pre-encoder or post-encoder bias.\n        \"\"\"\n        super().__init__()\n\n        self._bias_reference = bias_reference\n\n        # Support string literals as well as enums\n        self._bias_position = position\n\n    def forward(\n        self,\n        x: InputOutputActivationBatch,\n    ) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward Pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        # If this is the pre-encoder bias, we subtract the bias from the input.\n        if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n            return x - self.bias\n\n        # If it's the post-encoder bias, we add the bias to the input.\n        return x + self.bias\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.bias","title":"<code>bias: InputOutputActivationVector</code>  <code>property</code>","text":"<p>Bias.</p>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.__init__","title":"<code>__init__(bias_reference, position)</code>","text":"<p>Initialize the bias layer.</p> <p>Parameters:</p> Name Type Description Default <code>bias_reference</code> <code>InputOutputActivationVector</code> <p>Tied bias parameter (initialised in the parent module), used for both the pre-encoder and post-encoder bias. The original paper initialised this using the geometric median of the dataset.</p> required <code>position</code> <code>TiedBiasPosition</code> <p>Whether this is the pre-encoder or post-encoder bias.</p> required Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def __init__(\n    self,\n    bias_reference: InputOutputActivationVector,\n    position: TiedBiasPosition,\n) -&gt; None:\n    \"\"\"Initialize the bias layer.\n\n    Args:\n        bias_reference: Tied bias parameter (initialised in the parent module), used for both\n            the pre-encoder and post-encoder bias. The original paper initialised this using the\n            geometric median of the dataset.\n        position: Whether this is the pre-encoder or post-encoder bias.\n    \"\"\"\n    super().__init__()\n\n    self._bias_reference = bias_reference\n\n    # Support string literals as well as enums\n    self._bias_position = position\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"position={self._bias_position.value}\"\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBias.forward","title":"<code>forward(x)</code>","text":"<p>Forward Pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InputOutputActivationBatch</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>def forward(\n    self,\n    x: InputOutputActivationBatch,\n) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward Pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    # If this is the pre-encoder bias, we subtract the bias from the input.\n    if self._bias_position == TiedBiasPosition.PRE_ENCODER:\n        return x - self.bias\n\n    # If it's the post-encoder bias, we add the bias to the input.\n    return x + self.bias\n</code></pre>"},{"location":"reference/autoencoder/components/tied_bias/#sparse_autoencoder.autoencoder.components.tied_bias.TiedBiasPosition","title":"<code>TiedBiasPosition</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Tied Bias Position.</p> Source code in <code>sparse_autoencoder/autoencoder/components/tied_bias.py</code> <pre><code>class TiedBiasPosition(str, Enum):\n    \"\"\"Tied Bias Position.\"\"\"\n\n    PRE_ENCODER = \"pre_encoder\"\n    POST_DECODER = \"post_decoder\"\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/","title":"Linear layer with unit norm weights","text":"<p>Linear layer with unit norm weights.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder","title":"<code>UnitNormDecoder</code>","text":"<p>             Bases: <code>AbstractDecoder</code></p> <p>Constrained unit norm linear decoder layer.</p> <p>Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are constrained to have unit norm. This is done by removing the gradient information parallel to the dictionary vectors before applying the gradient step, using a backward hook. It also requires <code>constrain_weights_unit_norm</code> to be called after each gradient step, to prevent drift of the dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum).</p> \\[ \\begin{align*}     m &amp;= \\text{learned features dimension} \\\\     n &amp;= \\text{input and output dimension} \\\\     b &amp;= \\text{batch items dimension} \\\\     f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\     W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\     z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)} \\end{align*} \\] Motivation <p>Normalisation of the columns (dictionary features) prevents the model from reducing the sparsity loss term by increasing the size of the feature vectors in \\(W_d\\).</p> <p>Note that the Towards Monosemanticity: Decomposing Language Models With Dictionary Learning paper found that removing the gradient information parallel to the dictionary vectors before applying the gradient step, rather than resetting the dictionary vectors to unit norm after each gradient step, results in a small but real reduction in total loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>@final\nclass UnitNormDecoder(AbstractDecoder):\n    r\"\"\"Constrained unit norm linear decoder layer.\n\n    Linear layer decoder, where the dictionary vectors (columns of the weight matrix) are\n    constrained to have unit norm. This is done by removing the gradient information parallel to the\n    dictionary vectors before applying the gradient step, using a backward hook. It also requires\n    `constrain_weights_unit_norm` to be called after each gradient step, to prevent drift of the\n    dictionary vectors away from unit norm (as optimisers such as Adam don't strictly follow the\n    gradient, but instead follow a modified gradient that includes momentum).\n\n    $$ \\begin{align*}\n        m &amp;= \\text{learned features dimension} \\\\\n        n &amp;= \\text{input and output dimension} \\\\\n        b &amp;= \\text{batch items dimension} \\\\\n        f \\in \\mathbb{R}^{b \\times m} &amp;= \\text{encoder output} \\\\\n        W_d \\in \\mathbb{R}^{n \\times m} &amp;= \\text{weight matrix} \\\\\n        z \\in \\mathbb{R}^{b \\times m} &amp;= f W_d^T = \\text{UnitNormDecoder output (pre-tied bias)}\n    \\end{align*} $$\n\n    Motivation:\n        Normalisation of the columns (dictionary features) prevents the model from reducing the\n        sparsity loss term by increasing the size of the feature vectors in $W_d$.\n\n        Note that the *Towards Monosemanticity: Decomposing Language Models With Dictionary\n        Learning* paper found that removing the gradient information parallel to the dictionary\n        vectors before applying the gradient step, rather than resetting the dictionary vectors to\n        unit norm after each gradient step, results in a small but real reduction in total\n        loss](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-optimization).\n    \"\"\"\n\n    _learnt_features: int\n    \"\"\"Number of learnt features (inputs to this layer).\"\"\"\n\n    _decoded_features: int\n    \"\"\"Number of decoded features (outputs from this layer).\"\"\"\n\n    _weight: DecoderWeights\n    \"\"\"Weight parameter internal state.\"\"\"\n\n    @property\n    def weight(self) -&gt; DecoderWeights:\n        \"\"\"Weight parameter.\n\n        Each column in the weights matrix acts as a dictionary vector, representing a single basis\n        element in the learned activation space.\n        \"\"\"\n        return self._weight\n\n    def __init__(\n        self,\n        learnt_features: int,\n        decoded_features: int,\n        *,\n        enable_gradient_hook: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the constrained unit norm linear layer.\n\n        Args:\n            learnt_features: Number of learnt features in the autoencoder.\n            decoded_features: Number of decoded (output) features in the autoencoder.\n            enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n                applying the gradient step, to maintain unit norm of the dictionary vectors).\n        \"\"\"\n        # Create the linear layer as per the standard PyTorch linear layer\n        super().__init__()\n        self._learnt_features = learnt_features\n        self._decoded_features = decoded_features\n        self._weight = Parameter(\n            torch.empty(\n                (decoded_features, learnt_features),\n            )\n        )\n        self.reset_parameters()\n\n        # Register backward hook to remove any gradient information parallel to the dictionary\n        # vectors (columns of the weight matrix) before applying the gradient step.\n        if enable_gradient_hook:\n            self._weight.register_hook(self._weight_backward_hook)\n\n    def constrain_weights_unit_norm(self) -&gt; None:\n        \"\"\"Constrain the weights to have unit norm.\n\n        Warning:\n            Note this must be called after each gradient step. This is because optimisers such as\n            Adam don't strictly follow the gradient, but instead follow a modified gradient that\n            includes momentum. This means that the gradient step can change the norm of the\n            dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n            Note this can't be applied directly in the backward hook, as it would interfere with a\n            variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n            with asynchronous operations, etc).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; layer = UnitNormDecoder(3, 3)\n            &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n            &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n            &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0]\n\n        \"\"\"\n        with torch.no_grad():\n            torch.nn.functional.normalize(self._weight, dim=0, out=self._weight)\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"Initialize or reset the parameters.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n            &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3)\n            &gt;&gt;&gt; layer.reset_parameters()\n            &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n            &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n            &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n            [1.0, 1.0, 1.0, 1.0]\n\n        \"\"\"\n        # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n        # normalisation here, since we immediately scale the weights to have unit norm (so the\n        # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n        self._weight: EncoderWeights = init.normal_(self._weight, mean=0, std=1)\n\n        # Scale so that each row has unit norm\n        self.constrain_weights_unit_norm()\n\n    def _weight_backward_hook(\n        self,\n        grad: EncoderWeights,\n    ) -&gt; EncoderWeights:\n        r\"\"\"Unit norm backward hook.\n\n        By subtracting the projection of the gradient onto the dictionary vectors, we remove the\n        component of the gradient that is parallel to the dictionary vectors and just keep the\n        component that is orthogonal to the dictionary vectors (i.e. moving around the hypersphere).\n        The result is that the backward pass does not change the norm of the dictionary vectors.\n\n        $$\n        \\begin{align*}\n            W_d &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Decoder weight matrix} \\\\\n            g &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Gradient w.r.t. } W_d\n                \\text{ from the backward pass} \\\\\n            W_{d, \\text{norm}} &amp;= \\frac{W_d}{\\|W_d\\|} = \\text{Normalized decoder weight matrix\n                (over columns)} \\\\\n            g_{\\parallel} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g\n                \\text{ parallel to } W_{d, \\text{norm}} \\\\\n            g_{\\perp} &amp;\\in \\mathbb{R}^{n \\times m} = \\text{Component of } g \\text{ orthogonal to }\n                W_{d, \\text{norm}} \\\\\n            g_{\\parallel} &amp;= W_{d, \\text{norm}} \\cdot (W_{d, \\text{norm}}^\\top \\cdot g) \\\\\n            g_{\\perp} &amp;= g - g_{\\parallel} =\n                \\text{Adjusted gradient with parallel component removed} \\\\\n        \\end{align*}\n        $$\n\n        Args:\n            grad: Gradient with respect to the weights.\n\n        Returns:\n            Gradient with respect to the weights, with the component parallel to the dictionary\n            vectors removed.\n        \"\"\"\n        # Project the gradients onto the dictionary vectors. Intuitively the dictionary vectors can\n        # be thought of as vectors that end on the circumference of a hypersphere. The projection of\n        # the gradient onto the dictionary vectors is the component of the gradient that is parallel\n        # to the dictionary vectors, i.e. the component that moves to or from the center of the\n        # hypersphere.\n        normalized_weight: EncoderWeights = self._weight / torch.norm(\n            self._weight, dim=0, keepdim=True\n        )\n\n        scalar_projections = einops.einsum(\n            grad,\n            normalized_weight,\n            f\"{Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}, \\\n                {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        projection = einops.einsum(\n            scalar_projections,\n            normalized_weight,\n            f\"{Axis.INPUT_OUTPUT_FEATURE}, \\\n                {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE} \\\n                -&gt; {Axis.LEARNT_FEATURE} {Axis.INPUT_OUTPUT_FEATURE}\",\n        )\n\n        # Subtracting the parallel component from the gradient leaves only the component that is\n        # orthogonal to the dictionary vectors, i.e. the component that moves around the surface of\n        # the hypersphere.\n        return grad - projection\n\n    def forward(self, x: LearnedActivationBatch) -&gt; InputOutputActivationBatch:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor.\n\n        Returns:\n            Output of the forward pass.\n        \"\"\"\n        return torch.nn.functional.linear(x, self._weight)\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"String extra representation of the module.\"\"\"\n        return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.weight","title":"<code>weight: DecoderWeights</code>  <code>property</code>","text":"<p>Weight parameter.</p> <p>Each column in the weights matrix acts as a dictionary vector, representing a single basis element in the learned activation space.</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.__init__","title":"<code>__init__(learnt_features, decoded_features, *, enable_gradient_hook=True)</code>","text":"<p>Initialize the constrained unit norm linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>learnt_features</code> <code>int</code> <p>Number of learnt features in the autoencoder.</p> required <code>decoded_features</code> <code>int</code> <p>Number of decoded (output) features in the autoencoder.</p> required <code>enable_gradient_hook</code> <code>bool</code> <p>Enable the gradient backwards hook (modify the gradient before applying the gradient step, to maintain unit norm of the dictionary vectors).</p> <code>True</code> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def __init__(\n    self,\n    learnt_features: int,\n    decoded_features: int,\n    *,\n    enable_gradient_hook: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the constrained unit norm linear layer.\n\n    Args:\n        learnt_features: Number of learnt features in the autoencoder.\n        decoded_features: Number of decoded (output) features in the autoencoder.\n        enable_gradient_hook: Enable the gradient backwards hook (modify the gradient before\n            applying the gradient step, to maintain unit norm of the dictionary vectors).\n    \"\"\"\n    # Create the linear layer as per the standard PyTorch linear layer\n    super().__init__()\n    self._learnt_features = learnt_features\n    self._decoded_features = decoded_features\n    self._weight = Parameter(\n        torch.empty(\n            (decoded_features, learnt_features),\n        )\n    )\n    self.reset_parameters()\n\n    # Register backward hook to remove any gradient information parallel to the dictionary\n    # vectors (columns of the weight matrix) before applying the gradient step.\n    if enable_gradient_hook:\n        self._weight.register_hook(self._weight_backward_hook)\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.constrain_weights_unit_norm","title":"<code>constrain_weights_unit_norm()</code>","text":"<p>Constrain the weights to have unit norm.</p> Warning <p>Note this must be called after each gradient step. This is because optimisers such as Adam don't strictly follow the gradient, but instead follow a modified gradient that includes momentum. This means that the gradient step can change the norm of the dictionary vectors, even when the hook <code>_weight_backward_hook</code> is applied.</p> <p>Note this can't be applied directly in the backward hook, as it would interfere with a variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues with asynchronous operations, etc).</p> Example <p>import torch layer = UnitNormDecoder(3, 3) layer.weight.data = torch.ones((3, 3)) * 10 layer.constrain_weights_unit_norm() column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0)) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0]</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def constrain_weights_unit_norm(self) -&gt; None:\n    \"\"\"Constrain the weights to have unit norm.\n\n    Warning:\n        Note this must be called after each gradient step. This is because optimisers such as\n        Adam don't strictly follow the gradient, but instead follow a modified gradient that\n        includes momentum. This means that the gradient step can change the norm of the\n        dictionary vectors, even when the hook `_weight_backward_hook` is applied.\n\n        Note this can't be applied directly in the backward hook, as it would interfere with a\n        variety of use cases (e.g. gradient accumulation across mini-batches, concurrency issues\n        with asynchronous operations, etc).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; layer = UnitNormDecoder(3, 3)\n        &gt;&gt;&gt; layer.weight.data = torch.ones((3, 3)) * 10\n        &gt;&gt;&gt; layer.constrain_weights_unit_norm()\n        &gt;&gt;&gt; column_norms = torch.sqrt(torch.sum(layer.weight ** 2, dim=0))\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0]\n\n    \"\"\"\n    with torch.no_grad():\n        torch.nn.functional.normalize(self._weight, dim=0, out=self._weight)\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.extra_repr","title":"<code>extra_repr()</code>","text":"<p>String extra representation of the module.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"String extra representation of the module.\"\"\"\n    return f\"in_features={self._learnt_features}, out_features={self._decoded_features}\"\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>LearnedActivationBatch</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>InputOutputActivationBatch</code> <p>Output of the forward pass.</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def forward(self, x: LearnedActivationBatch) -&gt; InputOutputActivationBatch:\n    \"\"\"Forward pass.\n\n    Args:\n        x: Input tensor.\n\n    Returns:\n        Output of the forward pass.\n    \"\"\"\n    return torch.nn.functional.linear(x, self._weight)\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Initialize or reset the parameters.</p> Example <p>import torch</p> Source code in <code>sparse_autoencoder/autoencoder/components/unit_norm_decoder.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Initialize or reset the parameters.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; # Create a layer with 4 columns (learnt features) and 3 rows (decoded features)\n        &gt;&gt;&gt; layer = UnitNormDecoder(learnt_features=4, decoded_features=3)\n        &gt;&gt;&gt; layer.reset_parameters()\n        &gt;&gt;&gt; # Get the norm across the rows (by summing across the columns)\n        &gt;&gt;&gt; column_norms = torch.sum(layer.weight ** 2, dim=0)\n        &gt;&gt;&gt; column_norms.round(decimals=3).tolist()\n        [1.0, 1.0, 1.0, 1.0]\n\n    \"\"\"\n    # Initialize the weights with a normal distribution. Note we don't use e.g. kaiming\n    # normalisation here, since we immediately scale the weights to have unit norm (so the\n    # initial standard deviation doesn't matter). Note also that `init.normal_` is in place.\n    self._weight: EncoderWeights = init.normal_(self._weight, mean=0, std=1)\n\n    # Scale so that each row has unit norm\n    self.constrain_weights_unit_norm()\n</code></pre>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters--create-a-layer-with-4-columns-learnt-features-and-3-rows-decoded-features","title":"Create a layer with 4 columns (learnt features) and 3 rows (decoded features)","text":"<p>layer = UnitNormDecoder(learnt_features=4, decoded_features=3) layer.reset_parameters()</p>"},{"location":"reference/autoencoder/components/unit_norm_decoder/#sparse_autoencoder.autoencoder.components.unit_norm_decoder.UnitNormDecoder.reset_parameters--get-the-norm-across-the-rows-by-summing-across-the-columns","title":"Get the norm across the rows (by summing across the columns)","text":"<p>column_norms = torch.sum(layer.weight ** 2, dim=0) column_norms.round(decimals=3).tolist() [1.0, 1.0, 1.0, 1.0]</p>"},{"location":"reference/loss/","title":"Loss Modules","text":"<p>Loss Modules.</p> <p>Loss modules are specialised PyTorch modules that calculate the loss for a Sparse Autoencoder. They all inherit from AbstractLoss, which defines the interface for loss modules and some common methods.</p> <p>If you want to create your own loss function, see :class:<code>AbstractLoss</code>.</p> <p>For combining multiple loss modules into a single loss module, see :class:<code>LossReducer</code>.</p>"},{"location":"reference/loss/abstract_loss/","title":"Abstract loss","text":"<p>Abstract loss.</p>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossLogType","title":"<code>LossLogType: TypeAlias = dict[str, int | float | str]</code>  <code>module-attribute</code>","text":"<p>Loss log dict.</p>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss","title":"<code>AbstractLoss</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract loss interface.</p> <p>Interface for implementing batch itemwise loss functions.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>class AbstractLoss(Module, ABC):\n    \"\"\"Abstract loss interface.\n\n    Interface for implementing batch itemwise loss functions.\n    \"\"\"\n\n    _modules: dict[str, \"AbstractLoss\"]  # type: ignore[assignment] (narrowing)\n    \"\"\"Children loss modules.\"\"\"\n\n    @abstractmethod\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n\n    @abstractmethod\n    def forward(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n    ) -&gt; TrainBatchStatistic:\n        \"\"\"Batch itemwise loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n\n    @final\n    def batch_scalar_loss(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; ItemTensor:\n        \"\"\"Batch scalar loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Loss for the batch.\n        \"\"\"\n        itemwise_loss = self.forward(source_activations, learned_activations, decoded_activations)\n\n        match reduction:\n            case LossReductionType.MEAN:\n                return itemwise_loss.mean().squeeze()\n            case LossReductionType.SUM:\n                return itemwise_loss.sum().squeeze()\n\n    def batch_scalar_loss_with_log(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[ItemTensor, LossLogType]:\n        \"\"\"Batch scalar loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the batch scalar loss and a dict of any properties to log.\n        \"\"\"\n        children_loss_scalars: list[ItemTensor] = []\n        metrics: LossLogType = {}\n\n        # If the loss module has children (e.g. it is a reducer):\n        if len(self._modules) &gt; 0:\n            for loss_module in self._modules.values():\n                child_loss, child_metrics = loss_module.batch_scalar_loss_with_log(\n                    source_activations,\n                    learned_activations,\n                    decoded_activations,\n                    reduction=reduction,\n                )\n                children_loss_scalars.append(child_loss)\n                metrics.update(child_metrics)\n\n            # Get the total loss &amp; metric\n            current_module_loss = torch.stack(children_loss_scalars).sum()\n\n        # Otherwise if it is a leaf loss module:\n        else:\n            current_module_loss = self.batch_scalar_loss(\n                source_activations, learned_activations, decoded_activations, reduction\n            )\n\n        # Add in the current loss module's metric\n        log_name = self.log_name()\n        metrics[log_name] = current_module_loss.detach().cpu().item()\n\n        return current_module_loss, metrics\n\n    @final\n    def __call__(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[ItemTensor, LossLogType]:\n        \"\"\"Batch scalar loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the batch scalar loss and a dict of any properties to log.\n        \"\"\"\n        return self.batch_scalar_loss_with_log(\n            source_activations, learned_activations, decoded_activations, reduction\n        )\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.__call__","title":"<code>__call__(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Batch scalar loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[ItemTensor, LossLogType]</code> <p>Tuple of the batch scalar loss and a dict of any properties to log.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@final\ndef __call__(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[ItemTensor, LossLogType]:\n    \"\"\"Batch scalar loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the batch scalar loss and a dict of any properties to log.\n    \"\"\"\n    return self.batch_scalar_loss_with_log(\n        source_activations, learned_activations, decoded_activations, reduction\n    )\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.batch_scalar_loss","title":"<code>batch_scalar_loss(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Batch scalar loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>ItemTensor</code> <p>Loss for the batch.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@final\ndef batch_scalar_loss(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; ItemTensor:\n    \"\"\"Batch scalar loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Loss for the batch.\n    \"\"\"\n    itemwise_loss = self.forward(source_activations, learned_activations, decoded_activations)\n\n    match reduction:\n        case LossReductionType.MEAN:\n            return itemwise_loss.mean().squeeze()\n        case LossReductionType.SUM:\n            return itemwise_loss.sum().squeeze()\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.batch_scalar_loss_with_log","title":"<code>batch_scalar_loss_with_log(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Batch scalar loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[ItemTensor, LossLogType]</code> <p>Tuple of the batch scalar loss and a dict of any properties to log.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>def batch_scalar_loss_with_log(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[ItemTensor, LossLogType]:\n    \"\"\"Batch scalar loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the batch scalar loss and a dict of any properties to log.\n    \"\"\"\n    children_loss_scalars: list[ItemTensor] = []\n    metrics: LossLogType = {}\n\n    # If the loss module has children (e.g. it is a reducer):\n    if len(self._modules) &gt; 0:\n        for loss_module in self._modules.values():\n            child_loss, child_metrics = loss_module.batch_scalar_loss_with_log(\n                source_activations,\n                learned_activations,\n                decoded_activations,\n                reduction=reduction,\n            )\n            children_loss_scalars.append(child_loss)\n            metrics.update(child_metrics)\n\n        # Get the total loss &amp; metric\n        current_module_loss = torch.stack(children_loss_scalars).sum()\n\n    # Otherwise if it is a leaf loss module:\n    else:\n        current_module_loss = self.batch_scalar_loss(\n            source_activations, learned_activations, decoded_activations, reduction\n        )\n\n    # Add in the current loss module's metric\n    log_name = self.log_name()\n    metrics[log_name] = current_module_loss.detach().cpu().item()\n\n    return current_module_loss, metrics\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>  <code>abstractmethod</code>","text":"<p>Batch itemwise loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n) -&gt; TrainBatchStatistic:\n    \"\"\"Batch itemwise loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.AbstractLoss.log_name","title":"<code>log_name()</code>  <code>abstractmethod</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>@abstractmethod\ndef log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType","title":"<code>LossReductionType</code>","text":"<p>             Bases: <code>LowercaseStrEnum</code></p> <p>Loss reduction type (across batch items).</p> Source code in <code>sparse_autoencoder/loss/abstract_loss.py</code> <pre><code>class LossReductionType(LowercaseStrEnum):\n    \"\"\"Loss reduction type (across batch items).\"\"\"\n\n    MEAN = \"mean\"\n    \"\"\"Mean loss across batch items.\"\"\"\n\n    SUM = \"sum\"\n    \"\"\"Sum the loss from all batch items.\"\"\"\n</code></pre>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.MEAN","title":"<code>MEAN = 'mean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean loss across batch items.</p>"},{"location":"reference/loss/abstract_loss/#sparse_autoencoder.loss.abstract_loss.LossReductionType.SUM","title":"<code>SUM = 'sum'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sum the loss from all batch items.</p>"},{"location":"reference/loss/decoded_activations_l2/","title":"L2 Reconstruction loss","text":"<p>L2 Reconstruction loss.</p>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss","title":"<code>L2ReconstructionLoss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>L2 Reconstruction loss.</p> <p>L2 reconstruction loss is calculated as the sum squared error between each each input vector and it's corresponding decoded vector. The original paper found that models trained with some loss functions such as cross-entropy loss generally prefer to represent features polysemantically, whereas models trained with L2 may achieve the same loss for both polysemantic and monosemantic representations of true features.</p> Example <p>import torch loss = L2ReconstructionLoss() input_activations = torch.tensor([[5.0, 4], [3.0, 4]]) output_activations = torch.tensor([[1.0, 5], [1.0, 5]]) unused_activations = torch.zeros_like(input_activations)</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>@final\nclass L2ReconstructionLoss(AbstractLoss):\n    \"\"\"L2 Reconstruction loss.\n\n    L2 reconstruction loss is calculated as the sum squared error between each each input vector\n    and it's corresponding decoded vector. The original paper found that models trained with some\n    loss functions such as cross-entropy loss generally prefer to represent features\n    polysemantically, whereas models trained with L2 may achieve the same loss for both\n    polysemantic and monosemantic representations of true features.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; loss = L2ReconstructionLoss()\n        &gt;&gt;&gt; input_activations = torch.tensor([[5.0, 4], [3.0, 4]])\n        &gt;&gt;&gt; output_activations = torch.tensor([[1.0, 5], [1.0, 5]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(input_activations)\n        &gt;&gt;&gt; # Outputs both loss and metrics to log\n        &gt;&gt;&gt; loss(input_activations, unused_activations, output_activations)\n        (tensor(11.), {'l2_reconstruction_loss': 11.0})\n    \"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"l2_reconstruction_loss\"\n\n    def forward(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,  # noqa: ARG002\n        decoded_activations: InputOutputActivationBatch,\n    ) -&gt; TrainBatchStatistic:\n        \"\"\"Calculate the L2 reconstruction loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n        # Sum over just the features dimension (i.e. batch itemwise loss). Note this is sum rather\n        # than mean to be consistent with L1 loss (and thus make the l1 coefficient stable to number\n        # of features).\n        return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss--outputs-both-loss-and-metrics-to-log","title":"Outputs both loss and metrics to log","text":"<p>loss(input_activations, unused_activations, output_activations) (tensor(11.), {'l2_reconstruction_loss': 11.0})</p>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Calculate the L2 reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def forward(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,  # noqa: ARG002\n    decoded_activations: InputOutputActivationBatch,\n) -&gt; TrainBatchStatistic:\n    \"\"\"Calculate the L2 reconstruction loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    square_error_loss = mse_loss(source_activations, decoded_activations, reduction=\"none\")\n\n    # Sum over just the features dimension (i.e. batch itemwise loss). Note this is sum rather\n    # than mean to be consistent with L1 loss (and thus make the l1 coefficient stable to number\n    # of features).\n    return square_error_loss.sum(dim=-1)\n</code></pre>"},{"location":"reference/loss/decoded_activations_l2/#sparse_autoencoder.loss.decoded_activations_l2.L2ReconstructionLoss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/decoded_activations_l2.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"l2_reconstruction_loss\"\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/","title":"Learned activations L1 (absolute error) loss","text":"<p>Learned activations L1 (absolute error) loss.</p>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss","title":"<code>LearnedActivationsL1Loss</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Learned activations L1 (absolute error) loss.</p> <p>L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this multiplied by the l1_coefficient (designed to encourage sparsity).</p> Example <p>l1_loss = LearnedActivationsL1Loss(0.1) learned_activations = torch.tensor([[2.0, -3], [2.0, -3]]) unused_activations = torch.zeros_like(learned_activations)</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>@final\nclass LearnedActivationsL1Loss(AbstractLoss):\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    L1 loss penalty is the absolute sum of the learned activations. The L1 penalty is this\n    multiplied by the l1_coefficient (designed to encourage sparsity).\n\n    Example:\n        &gt;&gt;&gt; l1_loss = LearnedActivationsL1Loss(0.1)\n        &gt;&gt;&gt; learned_activations = torch.tensor([[2.0, -3], [2.0, -3]])\n        &gt;&gt;&gt; unused_activations = torch.zeros_like(learned_activations)\n        &gt;&gt;&gt; # Returns loss and metrics to log\n        &gt;&gt;&gt; l1_loss(unused_activations, learned_activations, unused_activations)[0]\n        tensor(0.5000)\n    \"\"\"\n\n    l1_coefficient: float\n    \"\"\"L1 coefficient.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"learned_activations_l1_loss_penalty\"\n\n    def __init__(self, l1_coefficient: float) -&gt; None:\n        \"\"\"Initialize the absolute error loss.\n\n        Args:\n            l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n                [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n                approximate guide if you use e.g. 2x this number of tokens you might consider using\n                0.5x the l1 coefficient.\n        \"\"\"\n        self.l1_coefficient = l1_coefficient\n        super().__init__()\n\n    def _l1_loss(\n        self,\n        source_activations: InputOutputActivationBatch,  # noqa: ARG002\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,  # noqa: ARG002\n    ) -&gt; tuple[TrainBatchStatistic, TrainBatchStatistic]:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Tuple of itemwise absolute loss, and itemwise absolute loss multiplied by the l1\n            coefficient.\n        \"\"\"\n        absolute_loss = torch.abs(learned_activations).sum(dim=-1)\n        absolute_loss_penalty = absolute_loss * self.l1_coefficient\n        return absolute_loss, absolute_loss_penalty\n\n    def forward(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n    ) -&gt; TrainBatchStatistic:\n        \"\"\"Learned activations L1 (absolute error) loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Loss per batch item.\n        \"\"\"\n        return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n\n    # Override to add both the loss and the penalty to the log\n    def batch_scalar_loss_with_log(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n        reduction: LossReductionType = LossReductionType.MEAN,\n    ) -&gt; tuple[ItemTensor, LossLogType]:\n        \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n            reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n                make the loss independent of the batch size.\n\n        Returns:\n            Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n                (loss before and after the l1 coefficient).\n        \"\"\"\n        absolute_loss, absolute_loss_penalty = self._l1_loss(\n            source_activations, learned_activations, decoded_activations\n        )\n\n        match reduction:\n            case LossReductionType.MEAN:\n                batch_scalar_loss = absolute_loss.mean().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n            case LossReductionType.SUM:\n                batch_scalar_loss = absolute_loss.sum().squeeze()\n                batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n        metrics = {\n            \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n            self.log_name(): batch_scalar_loss_penalty.item(),\n        }\n\n        return batch_scalar_loss_penalty, metrics\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Extra representation string.\"\"\"\n        return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss--returns-loss-and-metrics-to-log","title":"Returns loss and metrics to log","text":"<p>l1_loss(unused_activations, learned_activations, unused_activations)[0] tensor(0.5000)</p>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.l1_coefficient","title":"<code>l1_coefficient: float = l1_coefficient</code>  <code>instance-attribute</code>","text":"<p>L1 coefficient.</p>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.__init__","title":"<code>__init__(l1_coefficient)</code>","text":"<p>Initialize the absolute error loss.</p> <p>Parameters:</p> Name Type Description Default <code>l1_coefficient</code> <code>float</code> <p>L1 coefficient. The original paper experimented with L1 coefficients of [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an approximate guide if you use e.g. 2x this number of tokens you might consider using 0.5x the l1 coefficient.</p> required Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def __init__(self, l1_coefficient: float) -&gt; None:\n    \"\"\"Initialize the absolute error loss.\n\n    Args:\n        l1_coefficient: L1 coefficient. The original paper experimented with L1 coefficients of\n            [0.01, 0.008, 0.006, 0.004, 0.001]. They used 250 tokens per prompt, so as an\n            approximate guide if you use e.g. 2x this number of tokens you might consider using\n            0.5x the l1 coefficient.\n    \"\"\"\n    self.l1_coefficient = l1_coefficient\n    super().__init__()\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.batch_scalar_loss_with_log","title":"<code>batch_scalar_loss_with_log(source_activations, learned_activations, decoded_activations, reduction=LossReductionType.MEAN)</code>","text":"<p>Learned activations L1 (absolute error) loss, with log.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <code>reduction</code> <code>LossReductionType</code> <p>Loss reduction type. Typically you would choose LossReductionType.MEAN to make the loss independent of the batch size.</p> <code>MEAN</code> <p>Returns:</p> Type Description <code>tuple[ItemTensor, LossLogType]</code> <p>Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log (loss before and after the l1 coefficient).</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def batch_scalar_loss_with_log(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n    reduction: LossReductionType = LossReductionType.MEAN,\n) -&gt; tuple[ItemTensor, LossLogType]:\n    \"\"\"Learned activations L1 (absolute error) loss, with log.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n        reduction: Loss reduction type. Typically you would choose LossReductionType.MEAN to\n            make the loss independent of the batch size.\n\n    Returns:\n        Tuple of the L1 absolute error batch scalar loss and a dict of the properties to log\n            (loss before and after the l1 coefficient).\n    \"\"\"\n    absolute_loss, absolute_loss_penalty = self._l1_loss(\n        source_activations, learned_activations, decoded_activations\n    )\n\n    match reduction:\n        case LossReductionType.MEAN:\n            batch_scalar_loss = absolute_loss.mean().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.mean().squeeze()\n        case LossReductionType.SUM:\n            batch_scalar_loss = absolute_loss.sum().squeeze()\n            batch_scalar_loss_penalty = absolute_loss_penalty.sum().squeeze()\n\n    metrics = {\n        \"learned_activations_l1_loss\": batch_scalar_loss.item(),\n        self.log_name(): batch_scalar_loss_penalty.item(),\n    }\n\n    return batch_scalar_loss_penalty, metrics\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Extra representation string.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Extra representation string.\"\"\"\n    return f\"l1_coefficient={self.l1_coefficient}\"\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Learned activations L1 (absolute error) loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>Loss per batch item.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def forward(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n) -&gt; TrainBatchStatistic:\n    \"\"\"Learned activations L1 (absolute error) loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Loss per batch item.\n    \"\"\"\n    return self._l1_loss(source_activations, learned_activations, decoded_activations)[1]\n</code></pre>"},{"location":"reference/loss/learned_activations_l1/#sparse_autoencoder.loss.learned_activations_l1.LearnedActivationsL1Loss.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/learned_activations_l1.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"learned_activations_l1_loss_penalty\"\n</code></pre>"},{"location":"reference/loss/reducer/","title":"Loss reducer","text":"<p>Loss reducer.</p>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer","title":"<code>LossReducer</code>","text":"<p>             Bases: <code>AbstractLoss</code></p> <p>Loss reducer.</p> <p>Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to nn.Sequential.</p> Example <p>from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss LossReducer( ...     L2ReconstructionLoss(), ...     LearnedActivationsL1Loss(0.001), ... ) LossReducer(   (0): L2ReconstructionLoss()   (1): LearnedActivationsL1Loss(l1_coefficient=0.001) )</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>@final\nclass LossReducer(AbstractLoss):\n    \"\"\"Loss reducer.\n\n    Reduces multiple loss algorithms into a single loss algorithm (by summing). Analogous to\n    nn.Sequential.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.loss.decoded_activations_l2 import L2ReconstructionLoss\n        &gt;&gt;&gt; from sparse_autoencoder.loss.learned_activations_l1 import LearnedActivationsL1Loss\n        &gt;&gt;&gt; LossReducer(\n        ...     L2ReconstructionLoss(),\n        ...     LearnedActivationsL1Loss(0.001),\n        ... )\n        LossReducer(\n          (0): L2ReconstructionLoss()\n          (1): LearnedActivationsL1Loss(l1_coefficient=0.001)\n        )\n\n    \"\"\"\n\n    _modules: dict[str, \"AbstractLoss\"]\n    \"\"\"Children loss modules.\"\"\"\n\n    def log_name(self) -&gt; str:\n        \"\"\"Log name.\n\n        Returns:\n            Name of the loss module for logging.\n        \"\"\"\n        return \"total_loss\"\n\n    def __init__(\n        self,\n        *loss_modules: AbstractLoss,\n    ):\n        \"\"\"Initialize the loss reducer.\n\n        Args:\n            *loss_modules: Loss modules to reduce.\n\n        Raises:\n            ValueError: If the loss reducer has no loss modules.\n        \"\"\"\n        super().__init__()\n\n        for idx, loss_module in enumerate(loss_modules):\n            self._modules[str(idx)] = loss_module\n\n        if len(self) == 0:\n            error_message = \"Loss reducer must have at least one loss module.\"\n            raise ValueError(error_message)\n\n    def forward(\n        self,\n        source_activations: InputOutputActivationBatch,\n        learned_activations: LearnedActivationBatch,\n        decoded_activations: InputOutputActivationBatch,\n    ) -&gt; TrainBatchStatistic:\n        \"\"\"Reduce loss.\n\n        Args:\n            source_activations: Source activations (input activations to the autoencoder from the\n                source model).\n            learned_activations: Learned activations (intermediate activations in the autoencoder).\n            decoded_activations: Decoded activations.\n\n        Returns:\n            Mean loss across the batch, summed across the loss modules.\n        \"\"\"\n        all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n            [\n                loss_module.forward(source_activations, learned_activations, decoded_activations)\n                for loss_module in self._modules.values()\n            ]\n        )\n\n        return all_modules_loss.sum(dim=0)\n\n    def __dir__(self) -&gt; list[str]:\n        \"\"\"Dir dunder method.\"\"\"\n        return list(self._modules.__dir__())\n\n    def __getitem__(self, idx: int) -&gt; AbstractLoss:\n        \"\"\"Get item dunder method.\"\"\"\n        return self._modules[str(idx)]\n\n    def __iter__(self) -&gt; Iterator[AbstractLoss]:\n        \"\"\"Iterator dunder method.\"\"\"\n        return iter(self._modules.values())\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length dunder method.\"\"\"\n        return len(self._modules)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__dir__","title":"<code>__dir__()</code>","text":"<p>Dir dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __dir__(self) -&gt; list[str]:\n    \"\"\"Dir dunder method.\"\"\"\n    return list(self._modules.__dir__())\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get item dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; AbstractLoss:\n    \"\"\"Get item dunder method.\"\"\"\n    return self._modules[str(idx)]\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__init__","title":"<code>__init__(*loss_modules)</code>","text":"<p>Initialize the loss reducer.</p> <p>Parameters:</p> Name Type Description Default <code>*loss_modules</code> <code>AbstractLoss</code> <p>Loss modules to reduce.</p> <code>()</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the loss reducer has no loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __init__(\n    self,\n    *loss_modules: AbstractLoss,\n):\n    \"\"\"Initialize the loss reducer.\n\n    Args:\n        *loss_modules: Loss modules to reduce.\n\n    Raises:\n        ValueError: If the loss reducer has no loss modules.\n    \"\"\"\n    super().__init__()\n\n    for idx, loss_module in enumerate(loss_modules):\n        self._modules[str(idx)] = loss_module\n\n    if len(self) == 0:\n        error_message = \"Loss reducer must have at least one loss module.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterator dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __iter__(self) -&gt; Iterator[AbstractLoss]:\n    \"\"\"Iterator dunder method.\"\"\"\n    return iter(self._modules.values())\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.__len__","title":"<code>__len__()</code>","text":"<p>Length dunder method.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length dunder method.\"\"\"\n    return len(self._modules)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.forward","title":"<code>forward(source_activations, learned_activations, decoded_activations)</code>","text":"<p>Reduce loss.</p> <p>Parameters:</p> Name Type Description Default <code>source_activations</code> <code>InputOutputActivationBatch</code> <p>Source activations (input activations to the autoencoder from the source model).</p> required <code>learned_activations</code> <code>LearnedActivationBatch</code> <p>Learned activations (intermediate activations in the autoencoder).</p> required <code>decoded_activations</code> <code>InputOutputActivationBatch</code> <p>Decoded activations.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>Mean loss across the batch, summed across the loss modules.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def forward(\n    self,\n    source_activations: InputOutputActivationBatch,\n    learned_activations: LearnedActivationBatch,\n    decoded_activations: InputOutputActivationBatch,\n) -&gt; TrainBatchStatistic:\n    \"\"\"Reduce loss.\n\n    Args:\n        source_activations: Source activations (input activations to the autoencoder from the\n            source model).\n        learned_activations: Learned activations (intermediate activations in the autoencoder).\n        decoded_activations: Decoded activations.\n\n    Returns:\n        Mean loss across the batch, summed across the loss modules.\n    \"\"\"\n    all_modules_loss: Float[Tensor, \"module train_batch\"] = torch.stack(\n        [\n            loss_module.forward(source_activations, learned_activations, decoded_activations)\n            for loss_module in self._modules.values()\n        ]\n    )\n\n    return all_modules_loss.sum(dim=0)\n</code></pre>"},{"location":"reference/loss/reducer/#sparse_autoencoder.loss.reducer.LossReducer.log_name","title":"<code>log_name()</code>","text":"<p>Log name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loss module for logging.</p> Source code in <code>sparse_autoencoder/loss/reducer.py</code> <pre><code>def log_name(self) -&gt; str:\n    \"\"\"Log name.\n\n    Returns:\n        Name of the loss module for logging.\n    \"\"\"\n    return \"total_loss\"\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics","text":"<p>Metrics.</p>"},{"location":"reference/metrics/metrics_container/","title":"Metrics container","text":"<p>Metrics container.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.default_metrics","title":"<code>default_metrics = MetricsContainer(train_metrics=[TrainBatchFeatureDensityMetric(), CapacityMetric(), TrainBatchLearnedActivationsL0()], resample_metrics=[NeuronActivityMetric()], validation_metrics=[ModelReconstructionScore()])</code>  <code>module-attribute</code>","text":"<p>Default metrics container.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer","title":"<code>MetricsContainer</code>  <code>dataclass</code>","text":"<p>Metrics container.</p> <p>Stores all metrics used in a pipeline.</p> Source code in <code>sparse_autoencoder/metrics/metrics_container.py</code> <pre><code>@dataclass\nclass MetricsContainer:\n    \"\"\"Metrics container.\n\n    Stores all metrics used in a pipeline.\n    \"\"\"\n\n    generate_metrics: list[AbstractGenerateMetric] = field(default_factory=list)\n    \"\"\"Metrics for the generate section.\"\"\"\n\n    resample_metrics: list[AbstractResampleMetric] = field(default_factory=list)\n    \"\"\"Metrics for the resample section.\"\"\" \"\"\n\n    train_metrics: list[AbstractTrainMetric] = field(default_factory=list)\n    \"\"\"Metrics for the train section.\"\"\"\n\n    validation_metrics: list[AbstractValidationMetric] = field(default_factory=list)\n    \"\"\"Metrics for the validation section.\"\"\"\n</code></pre>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer.generate_metrics","title":"<code>generate_metrics: list[AbstractGenerateMetric] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics for the generate section.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer.resample_metrics","title":"<code>resample_metrics: list[AbstractResampleMetric] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics for the resample section.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer.train_metrics","title":"<code>train_metrics: list[AbstractTrainMetric] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics for the train section.</p>"},{"location":"reference/metrics/metrics_container/#sparse_autoencoder.metrics.metrics_container.MetricsContainer.validation_metrics","title":"<code>validation_metrics: list[AbstractValidationMetric] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics for the validation section.</p>"},{"location":"reference/metrics/generate/","title":"Generate step metrics","text":"<p>Generate step metrics.</p>"},{"location":"reference/metrics/generate/#sparse_autoencoder.metrics.generate.AbstractGenerateMetric","title":"<code>AbstractGenerateMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract generate metric.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>class AbstractGenerateMetric(ABC):\n    \"\"\"Abstract generate metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/#sparse_autoencoder.metrics.generate.AbstractGenerateMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/abstract_generate_metric/","title":"Abstract generate metric","text":"<p>Abstract generate metric.</p>"},{"location":"reference/metrics/generate/abstract_generate_metric/#sparse_autoencoder.metrics.generate.abstract_generate_metric.AbstractGenerateMetric","title":"<code>AbstractGenerateMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract generate metric.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>class AbstractGenerateMetric(ABC):\n    \"\"\"Abstract generate metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/abstract_generate_metric/#sparse_autoencoder.metrics.generate.abstract_generate_metric.AbstractGenerateMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: GenerateMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/generate/abstract_generate_metric/#sparse_autoencoder.metrics.generate.abstract_generate_metric.GenerateMetricData","title":"<code>GenerateMetricData</code>  <code>dataclass</code>","text":"<p>Generate metric data.</p> Source code in <code>sparse_autoencoder/metrics/generate/abstract_generate_metric.py</code> <pre><code>@dataclass\nclass GenerateMetricData:\n    \"\"\"Generate metric data.\"\"\"\n\n    generated_activations: InputOutputActivationBatch\n</code></pre>"},{"location":"reference/metrics/resample/","title":"Resample metrics","text":"<p>Resample metrics.</p>"},{"location":"reference/metrics/resample/abstract_resample_metric/","title":"Abstract resample metric","text":"<p>Abstract resample metric.</p>"},{"location":"reference/metrics/resample/abstract_resample_metric/#sparse_autoencoder.metrics.resample.abstract_resample_metric.AbstractResampleMetric","title":"<code>AbstractResampleMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract resample metric.</p> Source code in <code>sparse_autoencoder/metrics/resample/abstract_resample_metric.py</code> <pre><code>class AbstractResampleMetric(ABC):\n    \"\"\"Abstract resample metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: ResampleMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\n\n        Args:\n            data: Resample metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n</code></pre>"},{"location":"reference/metrics/resample/abstract_resample_metric/#sparse_autoencoder.metrics.resample.abstract_resample_metric.AbstractResampleMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ResampleMetricData</code> <p>Resample metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/resample/abstract_resample_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: ResampleMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\n\n    Args:\n        data: Resample metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n</code></pre>"},{"location":"reference/metrics/resample/abstract_resample_metric/#sparse_autoencoder.metrics.resample.abstract_resample_metric.ResampleMetricData","title":"<code>ResampleMetricData</code>  <code>dataclass</code>","text":"<p>Resample metric data.</p> Source code in <code>sparse_autoencoder/metrics/resample/abstract_resample_metric.py</code> <pre><code>@dataclass\nclass ResampleMetricData:\n    \"\"\"Resample metric data.\"\"\"\n\n    neuron_activity: NeuronActivity\n    \"\"\"Number of times each neuron fired.\"\"\"\n</code></pre>"},{"location":"reference/metrics/resample/abstract_resample_metric/#sparse_autoencoder.metrics.resample.abstract_resample_metric.ResampleMetricData.neuron_activity","title":"<code>neuron_activity: NeuronActivity</code>  <code>instance-attribute</code>","text":"<p>Number of times each neuron fired.</p>"},{"location":"reference/metrics/resample/neuron_activity_metric/","title":"Neuron activity metric","text":"<p>Neuron activity metric.</p>"},{"location":"reference/metrics/resample/neuron_activity_metric/#sparse_autoencoder.metrics.resample.neuron_activity_metric.NeuronActivityMetric","title":"<code>NeuronActivityMetric</code>","text":"<p>             Bases: <code>AbstractResampleMetric</code></p> <p>Neuron activity metric.</p> Source code in <code>sparse_autoencoder/metrics/resample/neuron_activity_metric.py</code> <pre><code>class NeuronActivityMetric(AbstractResampleMetric):\n    \"\"\"Neuron activity metric.\"\"\"\n\n    def calculate(self, data: ResampleMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the neuron activity metrics.\n\n        Args:\n            data: Resample metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n        neuron_activity = data.neuron_activity\n\n        # Histogram of neuron activity\n        numpy_neuron_activity: NDArray[np.float_] = neuron_activity.detach().cpu().numpy()\n        bins, values = np.histogram(numpy_neuron_activity, bins=50)\n        histogram = wandb.Histogram(np_histogram=(bins, values))\n\n        return {\n            \"resample_alive_neuron_count\": (neuron_activity &gt; 0).sum().item(),\n            \"resample_dead_neuron_count\": (neuron_activity == 0).sum().item(),\n            \"resample_neuron_activity_histogram\": histogram,\n        }\n</code></pre>"},{"location":"reference/metrics/resample/neuron_activity_metric/#sparse_autoencoder.metrics.resample.neuron_activity_metric.NeuronActivityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the neuron activity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ResampleMetricData</code> <p>Resample metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/resample/neuron_activity_metric.py</code> <pre><code>def calculate(self, data: ResampleMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the neuron activity metrics.\n\n    Args:\n        data: Resample metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n    neuron_activity = data.neuron_activity\n\n    # Histogram of neuron activity\n    numpy_neuron_activity: NDArray[np.float_] = neuron_activity.detach().cpu().numpy()\n    bins, values = np.histogram(numpy_neuron_activity, bins=50)\n    histogram = wandb.Histogram(np_histogram=(bins, values))\n\n    return {\n        \"resample_alive_neuron_count\": (neuron_activity &gt; 0).sum().item(),\n        \"resample_dead_neuron_count\": (neuron_activity == 0).sum().item(),\n        \"resample_neuron_activity_histogram\": histogram,\n    }\n</code></pre>"},{"location":"reference/metrics/train/","title":"Train step metrics","text":"<p>Train step metrics.</p>"},{"location":"reference/metrics/train/#sparse_autoencoder.metrics.train.AbstractTrainMetric","title":"<code>AbstractTrainMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract train metric.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>class AbstractTrainMetric(ABC):\n    \"\"\"Abstract train metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/#sparse_autoencoder.metrics.train.AbstractTrainMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/abstract_train_metric/","title":"Abstract train metric","text":"<p>Abstract train metric.</p>"},{"location":"reference/metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric","title":"<code>AbstractTrainMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract train metric.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>class AbstractTrainMetric(ABC):\n    \"\"\"Abstract train metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary of metrics.\n        \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.AbstractTrainMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of metrics.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary of metrics.\n    \"\"\"\n</code></pre>"},{"location":"reference/metrics/train/abstract_train_metric/#sparse_autoencoder.metrics.train.abstract_train_metric.TrainMetricData","title":"<code>TrainMetricData</code>  <code>dataclass</code>","text":"<p>Train metric data.</p> Source code in <code>sparse_autoencoder/metrics/train/abstract_train_metric.py</code> <pre><code>@dataclass\nclass TrainMetricData:\n    \"\"\"Train metric data.\"\"\"\n\n    input_activations: InputOutputActivationBatch\n\n    learned_activations: LearnedActivationBatch\n\n    decoded_activations: InputOutputActivationBatch\n</code></pre>"},{"location":"reference/metrics/train/capacity/","title":"Capacity Metrics","text":"<p>Capacity Metrics.</p>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric","title":"<code>CapacityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Capacities Metrics for Learned Features.</p> <p>Measure the capacity of a set of features as defined in Polysemanticity and Capacity in Neural Networks.</p> <p>Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature. Formally it's the ratio of the squared dot product of a feature with itself to the sum of its squared dot products of all features.</p> <p>If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is 1/n.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>class CapacityMetric(AbstractTrainMetric):\n    \"\"\"Capacities Metrics for Learned Features.\n\n    Measure the capacity of a set of features as defined in [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf).\n\n    Capacity is intuitively measuring the 'proportion of a dimension' assigned to a feature.\n    Formally it's the ratio of the squared dot product of a feature with itself to the sum of its\n    squared dot products of all features.\n\n    If the features are orthogonal, the capacity is 1. If they are all the same, the capacity is\n    1/n.\n    \"\"\"\n\n    @staticmethod\n    def capacities(features: LearnedActivationBatch) -&gt; TrainBatchStatistic:\n        \"\"\"Calculate capacities.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n            &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n            &gt;&gt;&gt; orthogonal_caps\n            tensor([1., 1., 1.])\n\n        Args:\n            features: A collection of features.\n\n        Returns:\n            A 1D tensor of capacities, where each element is the capacity of the corresponding\n            feature.\n        \"\"\"\n        squared_dot_products = (\n            einops.einsum(\n                features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n            )\n            ** 2\n        )\n        sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n        return torch.diag(squared_dot_products) / sum_of_sq_dot\n\n    @staticmethod\n    def wandb_capacities_histogram(\n        capacities: TrainBatchStatistic,\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the capacities.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n        wandb_capacities_histogram(capacities)})`.\n\n        Args:\n            capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the capacities for a training batch.\"\"\"\n        train_batch_capacities = self.capacities(data.learned_activations)\n        train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n        return {\n            \"train_batch_capacities_histogram\": train_batch_capacities_histogram,\n        }\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the capacities for a training batch.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the capacities for a training batch.\"\"\"\n    train_batch_capacities = self.capacities(data.learned_activations)\n    train_batch_capacities_histogram = self.wandb_capacities_histogram(train_batch_capacities)\n\n    return {\n        \"train_batch_capacities_histogram\": train_batch_capacities_histogram,\n    }\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.capacities","title":"<code>capacities(features)</code>  <code>staticmethod</code>","text":"<p>Calculate capacities.</p> Example <p>import torch orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) orthogonal_caps = CapacityMetric.capacities(orthogonal_features) orthogonal_caps tensor([1., 1., 1.])</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>LearnedActivationBatch</code> <p>A collection of features.</p> required <p>Returns:</p> Type Description <code>TrainBatchStatistic</code> <p>A 1D tensor of capacities, where each element is the capacity of the corresponding</p> <code>TrainBatchStatistic</code> <p>feature.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef capacities(features: LearnedActivationBatch) -&gt; TrainBatchStatistic:\n    \"\"\"Calculate capacities.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; orthogonal_features = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n        &gt;&gt;&gt; orthogonal_caps = CapacityMetric.capacities(orthogonal_features)\n        &gt;&gt;&gt; orthogonal_caps\n        tensor([1., 1., 1.])\n\n    Args:\n        features: A collection of features.\n\n    Returns:\n        A 1D tensor of capacities, where each element is the capacity of the corresponding\n        feature.\n    \"\"\"\n    squared_dot_products = (\n        einops.einsum(\n            features, features, \"n_feats1 feat_dim, n_feats2 feat_dim -&gt; n_feats1 n_feats2\"\n        )\n        ** 2\n    )\n    sum_of_sq_dot = squared_dot_products.sum(dim=-1)\n    return torch.diag(squared_dot_products) / sum_of_sq_dot\n</code></pre>"},{"location":"reference/metrics/train/capacity/#sparse_autoencoder.metrics.train.capacity.CapacityMetric.wandb_capacities_histogram","title":"<code>wandb_capacities_histogram(capacities)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the capacities.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"capacities_histogram\": wandb_capacities_histogram(capacities)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>TrainBatchStatistic</code> <p>Capacity of each feature. Can be calculated using :func:<code>calc_capacities</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/capacity.py</code> <pre><code>@staticmethod\ndef wandb_capacities_histogram(\n    capacities: TrainBatchStatistic,\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the capacities.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"capacities_histogram\":\n    wandb_capacities_histogram(capacities)})`.\n\n    Args:\n        capacities: Capacity of each feature. Can be calculated using :func:`calc_capacities`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_capacities: NDArray[np.float_] = capacities.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_capacities, bins=20, range=(0, 1))\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/metrics/train/feature_density/","title":"Train batch feature density","text":"<p>Train batch feature density.</p>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric","title":"<code>TrainBatchFeatureDensityMetric</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Train batch feature density.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a training batch.</p> <p>Generally we want a small number of features to be active in each batch, so average feature density should be low. By contrast if the average feature density is high, it means that the features are not sparse enough.</p> Warning <p>This is not the same as the feature density of the entire training set. It's main use is tracking the progress of training.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>class TrainBatchFeatureDensityMetric(AbstractTrainMetric):\n    \"\"\"Train batch feature density.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\"), in a\n    training batch.\n\n    Generally we want a small number of features to be active in each batch, so average feature\n    density should be low. By contrast if the average feature density is high, it means that the\n    features are not sparse enough.\n\n    Warning:\n        This is not the same as the feature density of the entire training set. It's main use is\n        tracking the progress of training.\n    \"\"\"\n\n    threshold: float\n\n    def __init__(self, threshold: float = 0.0) -&gt; None:\n        \"\"\"Initialise the train batch feature density metric.\n\n        Args:\n            threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n                This should be close to zero.\n        \"\"\"\n        super().__init__()\n        self.threshold = threshold\n\n    def feature_density(self, activations: LearnedActivationBatch) -&gt; LearntActivationVector:\n        \"\"\"Count how many times each feature was active.\n\n        Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n            &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n            [1.0, 0.5, 0.0]\n\n        Args:\n            activations: Sample of cached activations (the Autoencoder's learned features).\n\n        Returns:\n            Number of times each feature was active in a sample.\n        \"\"\"\n        has_fired: LearnedActivationBatch = torch.gt(activations, self.threshold).to(\n            dtype=torch.float  # Move to float so it can be averaged\n        )\n\n        return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n\n    @staticmethod\n    def wandb_feature_density_histogram(\n        feature_density: LearntActivationVector,\n    ) -&gt; wandb.Histogram:\n        \"\"\"Create a W&amp;B histogram of the feature density.\n\n        This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n        wandb_feature_density_histogram(feature_density)})`.\n\n        Args:\n            feature_density: Number of times each feature was active in a sample. Can be calculated\n                using :func:`feature_activity_count`.\n\n        Returns:\n            Weights &amp; Biases histogram for logging with `wandb.log`.\n        \"\"\"\n        numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n        bins, values = histogram(numpy_feature_density, bins=50)\n        return wandb.Histogram(np_histogram=(bins, values))\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the train batch feature density metrics.\n\n        Args:\n            data: Train metric data.\n\n        Returns:\n            Dictionary with the train batch feature density metric, and a histogram of the feature\n            density.\n        \"\"\"\n        train_batch_feature_density: LearntActivationVector = self.feature_density(\n            data.learned_activations\n        )\n\n        train_batch_feature_density_histogram: wandb.Histogram = (\n            self.wandb_feature_density_histogram(train_batch_feature_density)\n        )\n\n        return {\n            \"train_batch_feature_density_histogram\": train_batch_feature_density_histogram,\n        }\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.__init__","title":"<code>__init__(threshold=0.0)</code>","text":"<p>Initialise the train batch feature density metric.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Threshold for considering a feature active (i.e. the neuron has \"fired\"). This should be close to zero.</p> <code>0.0</code> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def __init__(self, threshold: float = 0.0) -&gt; None:\n    \"\"\"Initialise the train batch feature density metric.\n\n    Args:\n        threshold: Threshold for considering a feature active (i.e. the neuron has \"fired\").\n            This should be close to zero.\n    \"\"\"\n    super().__init__()\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the train batch feature density metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TrainMetricData</code> <p>Train metric data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with the train batch feature density metric, and a histogram of the feature</p> <code>dict[str, Any]</code> <p>density.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the train batch feature density metrics.\n\n    Args:\n        data: Train metric data.\n\n    Returns:\n        Dictionary with the train batch feature density metric, and a histogram of the feature\n        density.\n    \"\"\"\n    train_batch_feature_density: LearntActivationVector = self.feature_density(\n        data.learned_activations\n    )\n\n    train_batch_feature_density_histogram: wandb.Histogram = (\n        self.wandb_feature_density_histogram(train_batch_feature_density)\n    )\n\n    return {\n        \"train_batch_feature_density_histogram\": train_batch_feature_density_histogram,\n    }\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.feature_density","title":"<code>feature_density(activations)</code>","text":"<p>Count how many times each feature was active.</p> <p>Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").</p> Example <p>import torch activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]]) TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist() [1.0, 0.5, 0.0]</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>LearnedActivationBatch</code> <p>Sample of cached activations (the Autoencoder's learned features).</p> required <p>Returns:</p> Type Description <code>LearntActivationVector</code> <p>Number of times each feature was active in a sample.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>def feature_density(self, activations: LearnedActivationBatch) -&gt; LearntActivationVector:\n    \"\"\"Count how many times each feature was active.\n\n    Percentage of samples in which each feature was active (i.e. the neuron has \"fired\").\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; activations = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.0, 0.0001]])\n        &gt;&gt;&gt; TrainBatchFeatureDensityMetric(0.001).feature_density(activations).tolist()\n        [1.0, 0.5, 0.0]\n\n    Args:\n        activations: Sample of cached activations (the Autoencoder's learned features).\n\n    Returns:\n        Number of times each feature was active in a sample.\n    \"\"\"\n    has_fired: LearnedActivationBatch = torch.gt(activations, self.threshold).to(\n        dtype=torch.float  # Move to float so it can be averaged\n    )\n\n    return einops.reduce(has_fired, \"sample activation -&gt; activation\", \"mean\")\n</code></pre>"},{"location":"reference/metrics/train/feature_density/#sparse_autoencoder.metrics.train.feature_density.TrainBatchFeatureDensityMetric.wandb_feature_density_histogram","title":"<code>wandb_feature_density_histogram(feature_density)</code>  <code>staticmethod</code>","text":"<p>Create a W&amp;B histogram of the feature density.</p> <p>This can be logged with Weights &amp; Biases using e.g. <code>wandb.log({\"feature_density_histogram\": wandb_feature_density_histogram(feature_density)})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature_density</code> <code>LearntActivationVector</code> <p>Number of times each feature was active in a sample. Can be calculated using :func:<code>feature_activity_count</code>.</p> required <p>Returns:</p> Type Description <code>Histogram</code> <p>Weights &amp; Biases histogram for logging with <code>wandb.log</code>.</p> Source code in <code>sparse_autoencoder/metrics/train/feature_density.py</code> <pre><code>@staticmethod\ndef wandb_feature_density_histogram(\n    feature_density: LearntActivationVector,\n) -&gt; wandb.Histogram:\n    \"\"\"Create a W&amp;B histogram of the feature density.\n\n    This can be logged with Weights &amp; Biases using e.g. `wandb.log({\"feature_density_histogram\":\n    wandb_feature_density_histogram(feature_density)})`.\n\n    Args:\n        feature_density: Number of times each feature was active in a sample. Can be calculated\n            using :func:`feature_activity_count`.\n\n    Returns:\n        Weights &amp; Biases histogram for logging with `wandb.log`.\n    \"\"\"\n    numpy_feature_density: NDArray[np.float_] = feature_density.detach().cpu().numpy()\n\n    bins, values = histogram(numpy_feature_density, bins=50)\n    return wandb.Histogram(np_histogram=(bins, values))\n</code></pre>"},{"location":"reference/metrics/train/l0_norm_metric/","title":"L0 norm sparsity metric","text":"<p>L0 norm sparsity metric.</p>"},{"location":"reference/metrics/train/l0_norm_metric/#sparse_autoencoder.metrics.train.l0_norm_metric.TrainBatchLearnedActivationsL0","title":"<code>TrainBatchLearnedActivationsL0</code>","text":"<p>             Bases: <code>AbstractTrainMetric</code></p> <p>Learned activations L0 norm sparsity metric.</p> <p>The L0 norm is the number of non-zero elements in a learned activation vector. We then average this over the batch.</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm_metric.py</code> <pre><code>@final\nclass TrainBatchLearnedActivationsL0(AbstractTrainMetric):\n    \"\"\"Learned activations L0 norm sparsity metric.\n\n    The L0 norm is the number of non-zero elements in a learned activation vector. We then average\n    this over the batch.\n    \"\"\"\n\n    def calculate(self, data: TrainMetricData) -&gt; dict[str, float]:\n        \"\"\"Create a log item for Weights and Biases.\"\"\"\n        batch_size = data.learned_activations.size(0)\n        n_non_zero_activations = torch.count_nonzero(data.learned_activations)\n        batch_average = n_non_zero_activations / batch_size\n        return {\"learned_activations_l0_norm\": batch_average.item()}\n</code></pre>"},{"location":"reference/metrics/train/l0_norm_metric/#sparse_autoencoder.metrics.train.l0_norm_metric.TrainBatchLearnedActivationsL0.calculate","title":"<code>calculate(data)</code>","text":"<p>Create a log item for Weights and Biases.</p> Source code in <code>sparse_autoencoder/metrics/train/l0_norm_metric.py</code> <pre><code>def calculate(self, data: TrainMetricData) -&gt; dict[str, float]:\n    \"\"\"Create a log item for Weights and Biases.\"\"\"\n    batch_size = data.learned_activations.size(0)\n    n_non_zero_activations = torch.count_nonzero(data.learned_activations)\n    batch_average = n_non_zero_activations / batch_size\n    return {\"learned_activations_l0_norm\": batch_average.item()}\n</code></pre>"},{"location":"reference/metrics/validate/","title":"Validate step metrics","text":"<p>Validate step metrics.</p>"},{"location":"reference/metrics/validate/#sparse_autoencoder.metrics.validate.AbstractValidationMetric","title":"<code>AbstractValidationMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract validation metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>class AbstractValidationMetric(ABC):\n    \"\"\"Abstract validation metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/#sparse_autoencoder.metrics.validate.AbstractValidationMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/abstract_validate_metric/","title":"Abstract metric classes","text":"<p>Abstract metric classes.</p>"},{"location":"reference/metrics/validate/abstract_validate_metric/#sparse_autoencoder.metrics.validate.abstract_validate_metric.AbstractValidationMetric","title":"<code>AbstractValidationMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract validation metric.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>class AbstractValidationMetric(ABC):\n    \"\"\"Abstract validation metric.\"\"\"\n\n    @abstractmethod\n    def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/abstract_validate_metric/#sparse_autoencoder.metrics.validate.abstract_validate_metric.AbstractValidationMetric.calculate","title":"<code>calculate(data)</code>  <code>abstractmethod</code>","text":"<p>Calculate any metrics.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>@abstractmethod\ndef calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate any metrics.\"\"\"\n</code></pre>"},{"location":"reference/metrics/validate/abstract_validate_metric/#sparse_autoencoder.metrics.validate.abstract_validate_metric.ValidationMetricData","title":"<code>ValidationMetricData</code>  <code>dataclass</code>","text":"<p>Validation metric data.</p> Source code in <code>sparse_autoencoder/metrics/validate/abstract_validate_metric.py</code> <pre><code>@dataclass\nclass ValidationMetricData:\n    \"\"\"Validation metric data.\"\"\"\n\n    source_model_loss: ValidationStatistics\n\n    source_model_loss_with_reconstruction: ValidationStatistics\n\n    source_model_loss_with_zero_ablation: ValidationStatistics\n</code></pre>"},{"location":"reference/metrics/validate/model_reconstruction_score/","title":"Model reconstruction score","text":"<p>Model reconstruction score.</p>"},{"location":"reference/metrics/validate/model_reconstruction_score/#sparse_autoencoder.metrics.validate.model_reconstruction_score.ModelReconstructionScore","title":"<code>ModelReconstructionScore</code>","text":"<p>             Bases: <code>AbstractValidationMetric</code></p> <p>Model reconstruction score.</p> <p>Creates a score that measures how well the model can reconstruct the data.</p> \\[ \\begin{align*}     v &amp;= \\text{number of validation items} \\\\     l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\     l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\     l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\     s &amp;= \\text{reconstruction score} \\\\     s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\     s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v \\end{align*} \\] Source code in <code>sparse_autoencoder/metrics/validate/model_reconstruction_score.py</code> <pre><code>class ModelReconstructionScore(AbstractValidationMetric):\n    r\"\"\"Model reconstruction score.\n\n    Creates a score that measures how well the model can reconstruct the data.\n\n    $$\n    \\begin{align*}\n        v &amp;= \\text{number of validation items} \\\\\n        l \\in{\\mathbb{R}^v} &amp;= \\text{loss with no changes to the source model} \\\\\n        l_\\text{recon} \\in{\\mathbb{R}^v} &amp;= \\text{loss with reconstruction} \\\\\n        l_\\text{zero} \\in{\\mathbb{R}^v} &amp;= \\text{loss with zero ablation} \\\\\n        s &amp;= \\text{reconstruction score} \\\\\n        s_\\text{itemwise} &amp;= \\frac{l_\\text{zero} - l_\\text{recon}}{l_\\text{zero} - l} \\\\\n        s &amp;= \\sum_{i=1}^v s_\\text{itemwise} / v\n    \\end{align*}\n    $$\n    \"\"\"\n\n    def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n        \"\"\"Calculate the model reconstruction score.\n\n        Example:\n            &gt;&gt;&gt; from sparse_autoencoder.tensor_types import ValidationStatistics\n            &gt;&gt;&gt; data = ValidationMetricData(\n            ...     source_model_loss=ValidationStatistics([2.0, 2.0, 2.0]),\n            ...     source_model_loss_with_reconstruction=ValidationStatistics([3.0, 3.0, 3.0]),\n            ...     source_model_loss_with_zero_ablation=ValidationStatistics([5.0, 5.0, 5.0])\n            ... )\n            &gt;&gt;&gt; metric = ModelReconstructionScore()\n            &gt;&gt;&gt; result = metric.calculate(data)\n            &gt;&gt;&gt; round(result['model_reconstruction_score'], 3)\n            0.667\n\n        Args:\n            data: Validation data.\n\n        Returns:\n            Model reconstruction score.\n        \"\"\"\n        # Return no statistics if the data is empty (e.g. if we're at the very end of training)\n        if data.source_model_loss.numel() == 0:\n            return {}\n\n        # Calculate the reconstruction score\n        zero_ablate_loss_minus_default_loss: ValidationStatistics = (\n            data.source_model_loss_with_zero_ablation - data.source_model_loss\n        )\n        zero_ablate_loss_minus_reconstruction_loss: ValidationStatistics = (\n            data.source_model_loss_with_zero_ablation - data.source_model_loss_with_reconstruction\n        )\n        model_reconstruction_score_itemwise: ValidationStatistics = (\n            zero_ablate_loss_minus_reconstruction_loss / zero_ablate_loss_minus_default_loss\n        )\n        model_reconstruction_score: float = model_reconstruction_score_itemwise.mean().item()\n\n        # Get the other metrics\n        validation_baseline_loss: float = data.source_model_loss.mean().item()\n        validation_loss_with_reconstruction: float = (\n            data.source_model_loss_with_reconstruction.mean().item()\n        )\n        validation_loss_with_zero_ablation: float = (\n            data.source_model_loss_with_zero_ablation.mean().item()\n        )\n\n        return {\n            \"validation_baseline_loss\": validation_baseline_loss,\n            \"validation_loss_with_reconstruction\": validation_loss_with_reconstruction,\n            \"validation_loss_with_zero_ablation\": validation_loss_with_zero_ablation,\n            \"model_reconstruction_score\": model_reconstruction_score,\n        }\n</code></pre>"},{"location":"reference/metrics/validate/model_reconstruction_score/#sparse_autoencoder.metrics.validate.model_reconstruction_score.ModelReconstructionScore.calculate","title":"<code>calculate(data)</code>","text":"<p>Calculate the model reconstruction score.</p> Example <p>from sparse_autoencoder.tensor_types import ValidationStatistics data = ValidationMetricData( ...     source_model_loss=ValidationStatistics([2.0, 2.0, 2.0]), ...     source_model_loss_with_reconstruction=ValidationStatistics([3.0, 3.0, 3.0]), ...     source_model_loss_with_zero_ablation=ValidationStatistics([5.0, 5.0, 5.0]) ... ) metric = ModelReconstructionScore() result = metric.calculate(data) round(result['model_reconstruction_score'], 3) 0.667</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ValidationMetricData</code> <p>Validation data.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Model reconstruction score.</p> Source code in <code>sparse_autoencoder/metrics/validate/model_reconstruction_score.py</code> <pre><code>def calculate(self, data: ValidationMetricData) -&gt; dict[str, Any]:\n    \"\"\"Calculate the model reconstruction score.\n\n    Example:\n        &gt;&gt;&gt; from sparse_autoencoder.tensor_types import ValidationStatistics\n        &gt;&gt;&gt; data = ValidationMetricData(\n        ...     source_model_loss=ValidationStatistics([2.0, 2.0, 2.0]),\n        ...     source_model_loss_with_reconstruction=ValidationStatistics([3.0, 3.0, 3.0]),\n        ...     source_model_loss_with_zero_ablation=ValidationStatistics([5.0, 5.0, 5.0])\n        ... )\n        &gt;&gt;&gt; metric = ModelReconstructionScore()\n        &gt;&gt;&gt; result = metric.calculate(data)\n        &gt;&gt;&gt; round(result['model_reconstruction_score'], 3)\n        0.667\n\n    Args:\n        data: Validation data.\n\n    Returns:\n        Model reconstruction score.\n    \"\"\"\n    # Return no statistics if the data is empty (e.g. if we're at the very end of training)\n    if data.source_model_loss.numel() == 0:\n        return {}\n\n    # Calculate the reconstruction score\n    zero_ablate_loss_minus_default_loss: ValidationStatistics = (\n        data.source_model_loss_with_zero_ablation - data.source_model_loss\n    )\n    zero_ablate_loss_minus_reconstruction_loss: ValidationStatistics = (\n        data.source_model_loss_with_zero_ablation - data.source_model_loss_with_reconstruction\n    )\n    model_reconstruction_score_itemwise: ValidationStatistics = (\n        zero_ablate_loss_minus_reconstruction_loss / zero_ablate_loss_minus_default_loss\n    )\n    model_reconstruction_score: float = model_reconstruction_score_itemwise.mean().item()\n\n    # Get the other metrics\n    validation_baseline_loss: float = data.source_model_loss.mean().item()\n    validation_loss_with_reconstruction: float = (\n        data.source_model_loss_with_reconstruction.mean().item()\n    )\n    validation_loss_with_zero_ablation: float = (\n        data.source_model_loss_with_zero_ablation.mean().item()\n    )\n\n    return {\n        \"validation_baseline_loss\": validation_baseline_loss,\n        \"validation_loss_with_reconstruction\": validation_loss_with_reconstruction,\n        \"validation_loss_with_zero_ablation\": validation_loss_with_zero_ablation,\n        \"model_reconstruction_score\": model_reconstruction_score,\n    }\n</code></pre>"},{"location":"reference/optimizer/","title":"Optimizers for Sparse Autoencoders","text":"<p>Optimizers for Sparse Autoencoders.</p> <p>When training a Sparse Autoencoder, it can be necessary to manually edit the model parameters (e.g. with neuron resampling to prevent dead neurons). When doing this, it's also necessary to reset the optimizer state for these parameters, as otherwise things like running averages will be incorrect (e.g. the running averages of the gradients and the squares of gradients with Adam).</p> <p>The optimizer used in the original [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning]https://www.anthropic.com/index/towards-monosemanticity-decomposing-language-models-with-dictionary-learning) paper is available here as :class:<code>AdamWithReset</code>.</p> <p>To enable creating other optimizers with reset methods, we also provide the interface :class:<code>AbstractOptimizerWithReset</code>.</p>"},{"location":"reference/optimizer/abstract_optimizer/","title":"Abstract optimizer with reset","text":"<p>Abstract optimizer with reset.</p>"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset","title":"<code>AbstractOptimizerWithReset</code>","text":"<p>             Bases: <code>Optimizer</code>, <code>ABC</code></p> <p>Abstract optimizer with reset.</p> <p>When implementing this interface, we recommend adding a <code>named_parameters</code> argument to the constructor, which can be obtained from <code>named_parameters=model.named_parameters()</code> by the end user. This is so that the optimizer can find the parameters to reset.</p> Source code in <code>sparse_autoencoder/optimizer/abstract_optimizer.py</code> <pre><code>class AbstractOptimizerWithReset(Optimizer, ABC):\n    \"\"\"Abstract optimizer with reset.\n\n    When implementing this interface, we recommend adding a `named_parameters` argument to the\n    constructor, which can be obtained from `named_parameters=model.named_parameters()` by the end\n    user. This is so that the optimizer can find the parameters to reset.\n    \"\"\"\n\n    @abstractmethod\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Resets any optimizer state (e.g. momentum). This is for use after manually editing model\n            parameters (e.g. with activation resampling).\n        \"\"\"\n\n    @abstractmethod\n    def reset_neurons_state(\n        self,\n        parameter_name: str,\n        neuron_indices: LearntNeuronIndices,\n        axis: int,\n        parameter_group: int = 0,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Args:\n            parameter_name: The name of the parameter. Examples from the standard sparse autoencoder\n                implementation  include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`,\n                `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`.\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the parameter to reset.\n            parameter_group: The index of the parameter group to reset (typically this is just zero,\n                unless you have setup multiple parameter groups for e.g. different learning rates\n                for different parameters).\n\n        Raises:\n            ValueError: If the parameter name is not found.\n        \"\"\"\n</code></pre>"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter_name, neuron_indices, axis, parameter_group=0)</code>  <code>abstractmethod</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>The name of the parameter. Examples from the standard sparse autoencoder implementation  include <code>tied_bias</code>, <code>encoder.Linear.weight</code>, <code>encoder.Linear.bias</code>, <code>decoder.Linear.weight</code>, and <code>decoder.ConstrainedUnitNormLinear.weight</code>.</p> required <code>neuron_indices</code> <code>LearntNeuronIndices</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the parameter to reset.</p> required <code>parameter_group</code> <code>int</code> <p>The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters).</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the parameter name is not found.</p> Source code in <code>sparse_autoencoder/optimizer/abstract_optimizer.py</code> <pre><code>@abstractmethod\ndef reset_neurons_state(\n    self,\n    parameter_name: str,\n    neuron_indices: LearntNeuronIndices,\n    axis: int,\n    parameter_group: int = 0,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Args:\n        parameter_name: The name of the parameter. Examples from the standard sparse autoencoder\n            implementation  include `tied_bias`, `encoder.Linear.weight`, `encoder.Linear.bias`,\n            `decoder.Linear.weight`, and `decoder.ConstrainedUnitNormLinear.weight`.\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the parameter to reset.\n        parameter_group: The index of the parameter group to reset (typically this is just zero,\n            unless you have setup multiple parameter groups for e.g. different learning rates\n            for different parameters).\n\n    Raises:\n        ValueError: If the parameter name is not found.\n    \"\"\"\n</code></pre>"},{"location":"reference/optimizer/abstract_optimizer/#sparse_autoencoder.optimizer.abstract_optimizer.AbstractOptimizerWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>  <code>abstractmethod</code>","text":"<p>Reset the state for all parameters.</p> <p>Resets any optimizer state (e.g. momentum). This is for use after manually editing model     parameters (e.g. with activation resampling).</p> Source code in <code>sparse_autoencoder/optimizer/abstract_optimizer.py</code> <pre><code>@abstractmethod\ndef reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Resets any optimizer state (e.g. momentum). This is for use after manually editing model\n        parameters (e.g. with activation resampling).\n    \"\"\"\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/","title":"Adam Optimizer with a reset method","text":"<p>Adam Optimizer with a reset method.</p> <p>This reset method is useful when resampling dead neurons during training.</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset","title":"<code>AdamWithReset</code>","text":"<p>             Bases: <code>Adam</code>, <code>AbstractOptimizerWithReset</code></p> <p>Adam Optimizer with a reset method.</p> <p>The :meth:<code>reset_state_all_parameters</code> and :meth:<code>reset_neurons_state</code> methods are useful when manually editing the model parameters during training (e.g. when resampling dead neurons). This is because Adam maintains running averages of the gradients and the squares of gradients, which will be incorrect if the parameters are changed.</p> <p>Otherwise this is the same as the standard Adam optimizer.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>@final\nclass AdamWithReset(Adam, AbstractOptimizerWithReset):\n    \"\"\"Adam Optimizer with a reset method.\n\n    The :meth:`reset_state_all_parameters` and :meth:`reset_neurons_state` methods are useful when\n    manually editing the model parameters during training (e.g. when resampling dead neurons). This\n    is because Adam maintains running averages of the gradients and the squares of gradients, which\n    will be incorrect if the parameters are changed.\n\n    Otherwise this is the same as the standard Adam optimizer.\n    \"\"\"\n\n    parameter_names: list[str]\n    \"\"\"Parameter Names.\n\n    The names of the parameters, so that we can find them later when resetting the state.\n    \"\"\"\n\n    def __init__(  # noqa: PLR0913 (extending existing implementation)\n        self,\n        params: params_t,\n        lr: float | Tensor = 1e-3,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        *,\n        amsgrad: bool = False,\n        foreach: bool | None = None,\n        maximize: bool = False,\n        capturable: bool = False,\n        differentiable: bool = False,\n        fused: bool | None = None,\n        named_parameters: Iterator[tuple[str, Parameter]],\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer.\n\n        Warning:\n            Named parameters must be with default settings (remove duplicates and not recursive).\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n        Args:\n            params: Iterable of parameters to optimize or dicts defining parameter groups.\n            lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n                float LR unless specifying fused=True or capturable=True.\n            betas: Coefficients used for computing running averages of gradient and its square.\n            eps: Term added to the denominator to improve numerical stability.\n            weight_decay: Weight decay (L2 penalty).\n            amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n                Convergence of Adam and Beyond\".\n            foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n                over the for-loop implementation on CUDA if more performant. Note that foreach uses\n                more peak memory.\n            maximize: If True, maximizes the parameters based on the objective, instead of\n                minimizing.\n            capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n                ungraphed performance.\n            differentiable: Whether autograd should occur through the optimizer step in training.\n                Setting to True can impair performance.\n            fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n                torch.float32, torch.float16, and torch.bfloat16.\n            named_parameters: An iterator over the named parameters of the model. This is used to\n                find the parameters when resetting their state. You should set this as\n                `model.named_parameters()`.\n\n        Raises:\n            ValueError: If the number of parameter names does not match the number of parameters.\n        \"\"\"\n        # Initialise the parent class (note we repeat the parameter names so that type hints work).\n        super().__init__(\n            params=params,\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n            foreach=foreach,\n            maximize=maximize,\n            capturable=capturable,\n            differentiable=differentiable,\n            fused=fused,\n        )\n\n        # Store the names of the parameters, so that we can find them later when resetting the\n        # state.\n        self.parameter_names = [name for name, _value in named_parameters]\n\n        if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n            error_message = (\n                \"The number of parameter names does not match the number of parameters. \"\n                \"If using model.named_parameters() make sure remove_duplicates is True \"\n                \"and recursive is False (the default settings).\"\n            )\n            raise ValueError(error_message)\n\n    def reset_state_all_parameters(self) -&gt; None:\n        \"\"\"Reset the state for all parameters.\n\n        Iterates over all parameters and resets both the running averages of the gradients and the\n        squares of gradients.\n        \"\"\"\n        # Iterate over every parameter\n        for group in self.param_groups:\n            for parameter in group[\"params\"]:\n                # Get the state\n                state = self.state[parameter]\n\n                # Check if state is initialized\n                if len(state) == 0:\n                    continue\n\n                # Reset running averages\n                exp_avg: Tensor = state[\"exp_avg\"]\n                exp_avg.zero_()\n                exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n                exp_avg_sq.zero_()\n\n                # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n                if \"max_exp_avg_sq\" in state:\n                    max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                    max_exp_avg_sq.zero_()\n\n    def _get_parameter_name_idx(self, parameter_name: str) -&gt; int:\n        \"\"\"Get the index of a parameter name.\n\n        Args:\n            parameter_name: The name of the parameter.\n\n        Returns:\n            int: The index of the parameter name.\n\n        Raises:\n            ValueError: If the parameter name is not found.\n        \"\"\"\n        if parameter_name not in self.parameter_names:\n            error_message = f\"Parameter name {parameter_name} not found.\"\n            raise ValueError(error_message)\n\n        return self.parameter_names.index(parameter_name)\n\n    def reset_neurons_state(\n        self,\n        parameter_name: str,\n        neuron_indices: LearntNeuronIndices,\n        axis: int,\n        parameter_group: int = 0,\n    ) -&gt; None:\n        \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n        Example:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n            &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n            &gt;&gt;&gt; optimizer = AdamWithReset(\n            ...     model.parameters(),\n            ...     named_parameters=model.named_parameters(),\n            ... )\n            &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n            &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n            &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._weight\", dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._bias\", dead_neurons_indices, axis=0)\n            &gt;&gt;&gt; optimizer.reset_neurons_state(\n            ...     \"_decoder._weight\",\n            ...     dead_neurons_indices,\n            ...     axis=1\n            ... )\n\n        Args:\n            parameter_name: The name of the parameter. Examples from the standard sparse autoencoder\n                implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n                `_decoder._weight`.\n            neuron_indices: The indices of the neurons to reset.\n            axis: The axis of the parameter to reset.\n            parameter_group: The index of the parameter group to reset (typically this is just zero,\n                unless you have setup multiple parameter groups for e.g. different learning rates\n                for different parameters).\n        \"\"\"\n        # Get the state of the parameter\n        group = self.param_groups[parameter_group]\n        parameter_name_idx = self._get_parameter_name_idx(parameter_name)\n        parameter = group[\"params\"][parameter_name_idx]\n        state = self.state[parameter]\n\n        # Check if state is initialized\n        if len(state) == 0:\n            return\n\n        # Reset running averages for the specified neurons\n        if \"exp_avg\" in state:\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.index_fill_(axis, neuron_indices, 0)\n        if \"exp_avg_sq\" in state:\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n\n        # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n        if \"max_exp_avg_sq\" in state:\n            max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n            max_exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.parameter_names","title":"<code>parameter_names: list[str] = [name for (name, _value) in named_parameters]</code>  <code>instance-attribute</code>","text":"<p>Parameter Names.</p> <p>The names of the parameters, so that we can find them later when resetting the state.</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.__init__","title":"<code>__init__(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, *, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None, named_parameters)</code>","text":"<p>Initialize the optimizer.</p> Warning <p>Named parameters must be with default settings (remove duplicates and not recursive).</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... ) optimizer.reset_state_all_parameters()</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>params_t</code> <p>Iterable of parameters to optimize or dicts defining parameter groups.</p> required <code>lr</code> <code>float | Tensor</code> <p>Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a float LR unless specifying fused=True or capturable=True.</p> <code>0.001</code> <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients used for computing running averages of gradient and its square.</p> <code>(0.9, 0.999)</code> <code>eps</code> <code>float</code> <p>Term added to the denominator to improve numerical stability.</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>Weight decay (L2 penalty).</p> <code>0</code> <code>amsgrad</code> <code>bool</code> <p>Whether to use the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Whether foreach implementation of optimizer is used. If None, foreach is used over the for-loop implementation on CUDA if more performant. Note that foreach uses more peak memory.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>If True, maximizes the parameters based on the objective, instead of minimizing.</p> <code>False</code> <code>capturable</code> <code>bool</code> <p>Whether this instance is safe to capture in a CUDA graph. True can impair ungraphed performance.</p> <code>False</code> <code>differentiable</code> <code>bool</code> <p>Whether autograd should occur through the optimizer step in training. Setting to True can impair performance.</p> <code>False</code> <code>fused</code> <code>bool | None</code> <p>Whether the fused implementation (CUDA only) is used. Supports torch.float64, torch.float32, torch.float16, and torch.bfloat16.</p> <code>None</code> <code>named_parameters</code> <code>Iterator[tuple[str, Parameter]]</code> <p>An iterator over the named parameters of the model. This is used to find the parameters when resetting their state. You should set this as <code>model.named_parameters()</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of parameter names does not match the number of parameters.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def __init__(  # noqa: PLR0913 (extending existing implementation)\n    self,\n    params: params_t,\n    lr: float | Tensor = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0,\n    *,\n    amsgrad: bool = False,\n    foreach: bool | None = None,\n    maximize: bool = False,\n    capturable: bool = False,\n    differentiable: bool = False,\n    fused: bool | None = None,\n    named_parameters: Iterator[tuple[str, Parameter]],\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Warning:\n        Named parameters must be with default settings (remove duplicates and not recursive).\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; optimizer.reset_state_all_parameters()\n\n    Args:\n        params: Iterable of parameters to optimize or dicts defining parameter groups.\n        lr: Learning rate. A Tensor LR is not yet fully supported for all implementations. Use a\n            float LR unless specifying fused=True or capturable=True.\n        betas: Coefficients used for computing running averages of gradient and its square.\n        eps: Term added to the denominator to improve numerical stability.\n        weight_decay: Weight decay (L2 penalty).\n        amsgrad: Whether to use the AMSGrad variant of this algorithm from the paper \"On the\n            Convergence of Adam and Beyond\".\n        foreach: Whether foreach implementation of optimizer is used. If None, foreach is used\n            over the for-loop implementation on CUDA if more performant. Note that foreach uses\n            more peak memory.\n        maximize: If True, maximizes the parameters based on the objective, instead of\n            minimizing.\n        capturable: Whether this instance is safe to capture in a CUDA graph. True can impair\n            ungraphed performance.\n        differentiable: Whether autograd should occur through the optimizer step in training.\n            Setting to True can impair performance.\n        fused: Whether the fused implementation (CUDA only) is used. Supports torch.float64,\n            torch.float32, torch.float16, and torch.bfloat16.\n        named_parameters: An iterator over the named parameters of the model. This is used to\n            find the parameters when resetting their state. You should set this as\n            `model.named_parameters()`.\n\n    Raises:\n        ValueError: If the number of parameter names does not match the number of parameters.\n    \"\"\"\n    # Initialise the parent class (note we repeat the parameter names so that type hints work).\n    super().__init__(\n        params=params,\n        lr=lr,\n        betas=betas,\n        eps=eps,\n        weight_decay=weight_decay,\n        amsgrad=amsgrad,\n        foreach=foreach,\n        maximize=maximize,\n        capturable=capturable,\n        differentiable=differentiable,\n        fused=fused,\n    )\n\n    # Store the names of the parameters, so that we can find them later when resetting the\n    # state.\n    self.parameter_names = [name for name, _value in named_parameters]\n\n    if len(self.parameter_names) != len(self.param_groups[0][\"params\"]):\n        error_message = (\n            \"The number of parameter names does not match the number of parameters. \"\n            \"If using model.named_parameters() make sure remove_duplicates is True \"\n            \"and recursive is False (the default settings).\"\n        )\n        raise ValueError(error_message)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state","title":"<code>reset_neurons_state(parameter_name, neuron_indices, axis, parameter_group=0)</code>","text":"<p>Reset the state for specific neurons, on a specific parameter.</p> Example <p>import torch from sparse_autoencoder.autoencoder.model import SparseAutoencoder model = SparseAutoencoder(5, 10, torch.zeros(5)) optimizer = AdamWithReset( ...     model.parameters(), ...     named_parameters=model.named_parameters(), ... )</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>The name of the parameter. Examples from the standard sparse autoencoder implementation  include <code>tied_bias</code>, <code>_encoder._weight</code>, <code>_encoder._bias</code>, <code>_decoder._weight</code>.</p> required <code>neuron_indices</code> <code>LearntNeuronIndices</code> <p>The indices of the neurons to reset.</p> required <code>axis</code> <code>int</code> <p>The axis of the parameter to reset.</p> required <code>parameter_group</code> <code>int</code> <p>The index of the parameter group to reset (typically this is just zero, unless you have setup multiple parameter groups for e.g. different learning rates for different parameters).</p> <code>0</code> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_neurons_state(\n    self,\n    parameter_name: str,\n    neuron_indices: LearntNeuronIndices,\n    axis: int,\n    parameter_group: int = 0,\n) -&gt; None:\n    \"\"\"Reset the state for specific neurons, on a specific parameter.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n        &gt;&gt;&gt; model = SparseAutoencoder(5, 10, torch.zeros(5))\n        &gt;&gt;&gt; optimizer = AdamWithReset(\n        ...     model.parameters(),\n        ...     named_parameters=model.named_parameters(),\n        ... )\n        &gt;&gt;&gt; # ... train the model and then resample some dead neurons, then do this ...\n        &gt;&gt;&gt; dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices\n        &gt;&gt;&gt; # Reset the optimizer state for parameters that have been updated\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._weight\", dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\"_encoder._bias\", dead_neurons_indices, axis=0)\n        &gt;&gt;&gt; optimizer.reset_neurons_state(\n        ...     \"_decoder._weight\",\n        ...     dead_neurons_indices,\n        ...     axis=1\n        ... )\n\n    Args:\n        parameter_name: The name of the parameter. Examples from the standard sparse autoencoder\n            implementation  include `tied_bias`, `_encoder._weight`, `_encoder._bias`,\n            `_decoder._weight`.\n        neuron_indices: The indices of the neurons to reset.\n        axis: The axis of the parameter to reset.\n        parameter_group: The index of the parameter group to reset (typically this is just zero,\n            unless you have setup multiple parameter groups for e.g. different learning rates\n            for different parameters).\n    \"\"\"\n    # Get the state of the parameter\n    group = self.param_groups[parameter_group]\n    parameter_name_idx = self._get_parameter_name_idx(parameter_name)\n    parameter = group[\"params\"][parameter_name_idx]\n    state = self.state[parameter]\n\n    # Check if state is initialized\n    if len(state) == 0:\n        return\n\n    # Reset running averages for the specified neurons\n    if \"exp_avg\" in state:\n        exp_avg: Tensor = state[\"exp_avg\"]\n        exp_avg.index_fill_(axis, neuron_indices, 0)\n    if \"exp_avg_sq\" in state:\n        exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n        exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n\n    # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n    if \"max_exp_avg_sq\" in state:\n        max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n        max_exp_avg_sq.index_fill_(axis, neuron_indices, 0)\n</code></pre>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--train-the-model-and-then-resample-some-dead-neurons-then-do-this","title":"... train the model and then resample some dead neurons, then do this ...","text":"<p>dead_neurons_indices = torch.tensor([0, 1]) # Dummy dead neuron indices</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_neurons_state--reset-the-optimizer-state-for-parameters-that-have-been-updated","title":"Reset the optimizer state for parameters that have been updated","text":"<p>optimizer.reset_neurons_state(\"_encoder._weight\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state(\"_encoder._bias\", dead_neurons_indices, axis=0) optimizer.reset_neurons_state( ...     \"_decoder._weight\", ...     dead_neurons_indices, ...     axis=1 ... )</p>"},{"location":"reference/optimizer/adam_with_reset/#sparse_autoencoder.optimizer.adam_with_reset.AdamWithReset.reset_state_all_parameters","title":"<code>reset_state_all_parameters()</code>","text":"<p>Reset the state for all parameters.</p> <p>Iterates over all parameters and resets both the running averages of the gradients and the squares of gradients.</p> Source code in <code>sparse_autoencoder/optimizer/adam_with_reset.py</code> <pre><code>def reset_state_all_parameters(self) -&gt; None:\n    \"\"\"Reset the state for all parameters.\n\n    Iterates over all parameters and resets both the running averages of the gradients and the\n    squares of gradients.\n    \"\"\"\n    # Iterate over every parameter\n    for group in self.param_groups:\n        for parameter in group[\"params\"]:\n            # Get the state\n            state = self.state[parameter]\n\n            # Check if state is initialized\n            if len(state) == 0:\n                continue\n\n            # Reset running averages\n            exp_avg: Tensor = state[\"exp_avg\"]\n            exp_avg.zero_()\n            exp_avg_sq: Tensor = state[\"exp_avg_sq\"]\n            exp_avg_sq.zero_()\n\n            # If AdamW is used (weight decay fix), also reset the max exp_avg_sq\n            if \"max_exp_avg_sq\" in state:\n                max_exp_avg_sq: Tensor = state[\"max_exp_avg_sq\"]\n                max_exp_avg_sq.zero_()\n</code></pre>"},{"location":"reference/source_data/","title":"Source Data","text":"<p>Source Data.</p>"},{"location":"reference/source_data/abstract_dataset/","title":"Abstract tokenized prompts dataset class","text":"<p>Abstract tokenized prompts dataset class.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.HuggingFaceDatasetItem","title":"<code>HuggingFaceDatasetItem = TypeVar('HuggingFaceDatasetItem', bound=Any)</code>  <code>module-attribute</code>","text":"<p>Hugging face dataset item typed dict.</p> <p>When extending :class:<code>SourceDataset</code> you should create a <code>TypedDict</code> that matches the structure of each dataset item in the underlying Hugging Face dataset.</p> Example <p>With the Uncopyrighted Pile this should be a typed dict with text and meta properties.</p> <p>class PileUncopyrightedSourceDataBatch(TypedDict): ...    text: list[str] ...    meta: list[dict[str, dict[str, str]]]</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompt","title":"<code>TokenizedPrompt = list[int]</code>  <code>module-attribute</code>","text":"<p>A tokenized prompt.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset","title":"<code>SourceDataset</code>","text":"<p>             Bases: <code>ABC</code>, <code>Generic[HuggingFaceDatasetItem]</code></p> <p>Abstract source dataset.</p> <p>Source dataset that is used to generate the activations dataset (by running forward passes of the source model with this data). It should contain prompts that have been tokenized with no padding tokens (apart from an optional single first padding token). This enables efficient generation of the activations dataset.</p> <p>Wraps an HuggingFace IterableDataset.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class SourceDataset(ABC, Generic[HuggingFaceDatasetItem]):\n    \"\"\"Abstract source dataset.\n\n    Source dataset that is used to generate the activations dataset (by running forward passes of\n    the source model with this data). It should contain prompts that have been tokenized with no\n    padding tokens (apart from an optional single first padding token). This enables efficient\n    generation of the activations dataset.\n\n    Wraps an HuggingFace IterableDataset.\n    \"\"\"\n\n    context_size: int\n    \"\"\"Number of tokens in the context window.\n\n    The paper *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n    a context size of 250.\n    \"\"\"\n\n    dataset: IterableDataset\n    \"\"\"Underlying HuggingFace IterableDataset.\n\n    Warning:\n        Hugging Face `Dataset` objects are confusingly not the same as PyTorch `Dataset` objects.\n    \"\"\"\n\n    @abstractmethod\n    def preprocess(\n        self,\n        source_batch: HuggingFaceDatasetItem,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess function.\n\n        Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string\n        prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary\n        length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.\n\n        Applied to the dataset with the [Hugging Face\n        Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)\n        `map` function.\n\n        Warning:\n            The returned tokenized prompts should not have any padding tokens (apart from an\n            optional single first padding token).\n\n        Args:\n            source_batch: A batch of source data. For example, with The Pile dataset this would be a\n                dict including the key \"text\" with a value of a list of strings (not yet tokenized).\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        dataset_path: str,\n        dataset_split: str,\n        context_size: int,\n        buffer_size: int = 1000,\n        preprocess_batch_size: int = 1000,\n    ):\n        \"\"\"Initialise the dataset.\n\n        Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the\n        underlying Hugging Face `IterableDataset`.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g. `train`).\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n                streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n                just that buffer. Note that the generated activations should also be shuffled before\n                training the sparse autoencoder, so a large buffer may not be strictly necessary\n                here. Note also that this is the number of items in the dataset (e.g. number of\n                prompts) and is typically significantly less than the number of tokenized prompts\n                once the preprocessing function has been applied.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n        \"\"\"\n        self.context_size = context_size\n\n        # Load the dataset\n        dataset: IterableDataset = load_dataset(dataset_path, streaming=True, split=dataset_split)  # type: ignore\n\n        # Setup preprocessing\n        existing_columns: list[str] = list(next(iter(dataset)).keys())\n        mapped_dataset = dataset.map(\n            self.preprocess,\n            batched=True,\n            batch_size=preprocess_batch_size,\n            fn_kwargs={\"context_size\": context_size},\n            remove_columns=existing_columns,\n        )\n\n        # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least\n        # `buffer_size` items and then shuffles just that buffer.\n        # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle\n        self.dataset = mapped_dataset.shuffle(buffer_size=buffer_size)\n\n    @final\n    def __iter__(self) -&gt; Any:  # noqa: ANN401\n        \"\"\"Iterate Dunder Method.\n\n        Enables direct access to :attr:`dataset` with e.g. `for` loops.\n        \"\"\"\n        return self.dataset.__iter__()\n\n    @final\n    def __next__(self) -&gt; Any:  # noqa: ANN401\n        \"\"\"Next Dunder Method.\n\n        Enables direct access to :attr:`dataset` with e.g. `next` calls.\n        \"\"\"\n        return next(iter(self))\n\n    @final\n    def get_dataloader(self, batch_size: int) -&gt; DataLoader[TorchTokenizedPrompts]:\n        \"\"\"Get a PyTorch DataLoader.\n\n        Args:\n            batch_size: The batch size to use.\n\n        Returns:\n            PyTorch DataLoader.\n        \"\"\"\n        torch_dataset: TorchDataset[TorchTokenizedPrompts] = self.dataset.with_format(\"torch\")  # type: ignore\n\n        return DataLoader[TorchTokenizedPrompts](\n            torch_dataset,\n            batch_size=batch_size,\n            # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not\n            # here.\n            shuffle=False,\n        )\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.context_size","title":"<code>context_size: int = context_size</code>  <code>instance-attribute</code>","text":"<p>Number of tokens in the context window.</p> <p>The paper Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.dataset","title":"<code>dataset: IterableDataset = mapped_dataset.shuffle(buffer_size=buffer_size)</code>  <code>instance-attribute</code>","text":"<p>Underlying HuggingFace IterableDataset.</p> Warning <p>Hugging Face <code>Dataset</code> objects are confusingly not the same as PyTorch <code>Dataset</code> objects.</p>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__init__","title":"<code>__init__(dataset_path, dataset_split, context_size, buffer_size=1000, preprocess_batch_size=1000)</code>  <code>abstractmethod</code>","text":"<p>Initialise the dataset.</p> <p>Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the underlying Hugging Face <code>IterableDataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> required <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> required <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> required <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>int</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    dataset_path: str,\n    dataset_split: str,\n    context_size: int,\n    buffer_size: int = 1000,\n    preprocess_batch_size: int = 1000,\n):\n    \"\"\"Initialise the dataset.\n\n    Loads the dataset with streaming from HuggingFace, dds preprocessing and shuffling to the\n    underlying Hugging Face `IterableDataset`.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g. `train`).\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n            streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n            just that buffer. Note that the generated activations should also be shuffled before\n            training the sparse autoencoder, so a large buffer may not be strictly necessary\n            here. Note also that this is the number of items in the dataset (e.g. number of\n            prompts) and is typically significantly less than the number of tokenized prompts\n            once the preprocessing function has been applied.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n    \"\"\"\n    self.context_size = context_size\n\n    # Load the dataset\n    dataset: IterableDataset = load_dataset(dataset_path, streaming=True, split=dataset_split)  # type: ignore\n\n    # Setup preprocessing\n    existing_columns: list[str] = list(next(iter(dataset)).keys())\n    mapped_dataset = dataset.map(\n        self.preprocess,\n        batched=True,\n        batch_size=preprocess_batch_size,\n        fn_kwargs={\"context_size\": context_size},\n        remove_columns=existing_columns,\n    )\n\n    # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least\n    # `buffer_size` items and then shuffles just that buffer.\n    # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle\n    self.dataset = mapped_dataset.shuffle(buffer_size=buffer_size)\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate Dunder Method.</p> <p>Enables direct access to :attr:<code>dataset</code> with e.g. <code>for</code> loops.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef __iter__(self) -&gt; Any:  # noqa: ANN401\n    \"\"\"Iterate Dunder Method.\n\n    Enables direct access to :attr:`dataset` with e.g. `for` loops.\n    \"\"\"\n    return self.dataset.__iter__()\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.__next__","title":"<code>__next__()</code>","text":"<p>Next Dunder Method.</p> <p>Enables direct access to :attr:<code>dataset</code> with e.g. <code>next</code> calls.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef __next__(self) -&gt; Any:  # noqa: ANN401\n    \"\"\"Next Dunder Method.\n\n    Enables direct access to :attr:`dataset` with e.g. `next` calls.\n    \"\"\"\n    return next(iter(self))\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.get_dataloader","title":"<code>get_dataloader(batch_size)</code>","text":"<p>Get a PyTorch DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size to use.</p> required <p>Returns:</p> Type Description <code>DataLoader[TorchTokenizedPrompts]</code> <p>PyTorch DataLoader.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@final\ndef get_dataloader(self, batch_size: int) -&gt; DataLoader[TorchTokenizedPrompts]:\n    \"\"\"Get a PyTorch DataLoader.\n\n    Args:\n        batch_size: The batch size to use.\n\n    Returns:\n        PyTorch DataLoader.\n    \"\"\"\n    torch_dataset: TorchDataset[TorchTokenizedPrompts] = self.dataset.with_format(\"torch\")  # type: ignore\n\n    return DataLoader[TorchTokenizedPrompts](\n        torch_dataset,\n        batch_size=batch_size,\n        # Shuffle is most efficiently done with the `shuffle` method on the dataset itself, not\n        # here.\n        shuffle=False,\n    )\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.SourceDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>  <code>abstractmethod</code>","text":"<p>Preprocess function.</p> <p>Takes a <code>preprocess_batch_size</code> (\\(m\\)) batch of source data (which may e.g. include string prompts), and returns a dict with a single key of <code>input_ids</code> and a value of an arbitrary length list (\\(n\\)) of tokenized prompts. Note that \\(m\\) does not have to be equal to \\(n\\).</p> <p>Applied to the dataset with the Hugging Face Dataset <code>map</code> function.</p> Warning <p>The returned tokenized prompts should not have any padding tokens (apart from an optional single first padding token).</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>HuggingFaceDatasetItem</code> <p>A batch of source data. For example, with The Pile dataset this would be a dict including the key \"text\" with a value of a list of strings (not yet tokenized).</p> required <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>@abstractmethod\ndef preprocess(\n    self,\n    source_batch: HuggingFaceDatasetItem,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess function.\n\n    Takes a `preprocess_batch_size` ($m$) batch of source data (which may e.g. include string\n    prompts), and returns a dict with a single key of `input_ids` and a value of an arbitrary\n    length list ($n$) of tokenized prompts. Note that $m$ does not have to be equal to $n$.\n\n    Applied to the dataset with the [Hugging Face\n    Dataset](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map)\n    `map` function.\n\n    Warning:\n        The returned tokenized prompts should not have any padding tokens (apart from an\n        optional single first padding token).\n\n    Args:\n        source_batch: A batch of source data. For example, with The Pile dataset this would be a\n            dict including the key \"text\" with a value of a list of strings (not yet tokenized).\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TokenizedPrompts","title":"<code>TokenizedPrompts</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class TokenizedPrompts(TypedDict):\n    \"\"\"Tokenized prompts.\"\"\"\n\n    input_ids: list[TokenizedPrompt]\n</code></pre>"},{"location":"reference/source_data/abstract_dataset/#sparse_autoencoder.source_data.abstract_dataset.TorchTokenizedPrompts","title":"<code>TorchTokenizedPrompts</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Tokenized prompts prepared for PyTorch.</p> Source code in <code>sparse_autoencoder/source_data/abstract_dataset.py</code> <pre><code>class TorchTokenizedPrompts(TypedDict):\n    \"\"\"Tokenized prompts prepared for PyTorch.\"\"\"\n\n    input_ids: BatchTokenizedPrompts\n</code></pre>"},{"location":"reference/source_data/mock_dataset/","title":"Mock dataset","text":"<p>Mock dataset.</p> <p>For use with tests and simple examples.</p>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset","title":"<code>ConsecutiveIntHuggingFaceDataset</code>","text":"<p>             Bases: <code>IterableDataset</code></p> <p>Consecutive integers Hugging Face dataset for testing.</p> <p>Creates a dataset where the first item is [0,1,2...], and the second item is [1,2,3...] and so on.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>class ConsecutiveIntHuggingFaceDataset(IterableDataset):\n    \"\"\"Consecutive integers Hugging Face dataset for testing.\n\n    Creates a dataset where the first item is [0,1,2...], and the second item is [1,2,3...] and so\n    on.\n    \"\"\"\n\n    _data: Int[Tensor, \"items context_size\"]\n    \"\"\"Generated data.\"\"\"\n\n    _length: int\n    \"\"\"Size of the dataset.\"\"\"\n\n    _format: Literal[\"torch\", \"list\"] = \"list\"\n    \"\"\"Format of the data.\"\"\"\n\n    def create_data(self, num_items: int, context_size: int) -&gt; Int[Tensor, \"items context_size\"]:\n        \"\"\"Create the data.\n\n        Args:\n            num_items: The number of items in the dataset.\n            context_size: The number of tokens in the context window.\n\n        Returns:\n            The generated data.\n        \"\"\"\n        rows = torch.arange(num_items).unsqueeze(1)\n        columns = torch.arange(context_size).unsqueeze(0)\n        return rows + columns\n\n    def __init__(\n        self, context_size: int, vocab_size: int = 50_000, num_items: int = 10_000\n    ) -&gt; None:\n        \"\"\"Initialize the mock HF dataset.\n\n        Args:\n            context_size: The number of tokens in the context window\n            vocab_size: The size of the vocabulary to use.\n            num_items: The number of items in the dataset.\n\n        Raises:\n            ValueError: If more items are requested than we can create with the vocab size (given\n                that each item is a consecutive list of integers and unique).\n        \"\"\"\n        self._length = num_items\n\n        # Check we can create the data\n        if num_items + context_size &gt; vocab_size:\n            error_message = (\n                f\"num_items ({num_items}) + context_size ({context_size}) must be less than \"\n                f\"vocab_size ({vocab_size})\"\n            )\n            raise ValueError(error_message)\n\n        # Initialise the data\n        self._data = self.create_data(num_items, context_size)\n\n    def __iter__(self) -&gt; Iterator:  # type: ignore (HF typing is incorrect)\n        \"\"\"Initialize the iterator.\n\n        Returns:\n            Iterator.\n        \"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n        \"\"\"Return the next item in the dataset.\n\n        Returns:\n            TokenizedPrompts: The next item in the dataset.\n\n        Raises:\n            StopIteration: If the end of the dataset is reached.\n        \"\"\"\n        if self._index &lt; self._length:\n            item = self[self._index]\n            self._index += 1\n            return item\n\n        raise StopIteration\n\n    def __len__(self) -&gt; int:\n        \"\"\"Len Dunder Method.\"\"\"\n        return self._length\n\n    def __getitem__(self, index: int) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n        \"\"\"Get Item.\"\"\"\n        item = self._data[index]\n\n        if self._format == \"torch\":\n            return {\"input_ids\": item}\n\n        return {\"input_ids\": item.tolist()}\n\n    def with_format(  # type: ignore (only support 2 types)\n        self,\n        type: Literal[\"torch\", \"list\"],  # noqa: A002\n    ) -&gt; \"ConsecutiveIntHuggingFaceDataset\":\n        \"\"\"With Format.\"\"\"\n        self._format = type\n        return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get Item.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n    \"\"\"Get Item.\"\"\"\n    item = self._data[index]\n\n    if self._format == \"torch\":\n        return {\"input_ids\": item}\n\n    return {\"input_ids\": item.tolist()}\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__init__","title":"<code>__init__(context_size, vocab_size=50000, num_items=10000)</code>","text":"<p>Initialize the mock HF dataset.</p> <p>Parameters:</p> Name Type Description Default <code>context_size</code> <code>int</code> <p>The number of tokens in the context window</p> required <code>vocab_size</code> <code>int</code> <p>The size of the vocabulary to use.</p> <code>50000</code> <code>num_items</code> <code>int</code> <p>The number of items in the dataset.</p> <code>10000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If more items are requested than we can create with the vocab size (given that each item is a consecutive list of integers and unique).</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __init__(\n    self, context_size: int, vocab_size: int = 50_000, num_items: int = 10_000\n) -&gt; None:\n    \"\"\"Initialize the mock HF dataset.\n\n    Args:\n        context_size: The number of tokens in the context window\n        vocab_size: The size of the vocabulary to use.\n        num_items: The number of items in the dataset.\n\n    Raises:\n        ValueError: If more items are requested than we can create with the vocab size (given\n            that each item is a consecutive list of integers and unique).\n    \"\"\"\n    self._length = num_items\n\n    # Check we can create the data\n    if num_items + context_size &gt; vocab_size:\n        error_message = (\n            f\"num_items ({num_items}) + context_size ({context_size}) must be less than \"\n            f\"vocab_size ({vocab_size})\"\n        )\n        raise ValueError(error_message)\n\n    # Initialise the data\n    self._data = self.create_data(num_items, context_size)\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Initialize the iterator.</p> <p>Returns:</p> Type Description <code>Iterator</code> <p>Iterator.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __iter__(self) -&gt; Iterator:  # type: ignore (HF typing is incorrect)\n    \"\"\"Initialize the iterator.\n\n    Returns:\n        Iterator.\n    \"\"\"\n    self._index = 0\n    return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__len__","title":"<code>__len__()</code>","text":"<p>Len Dunder Method.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Len Dunder Method.\"\"\"\n    return self._length\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item in the dataset.</p> <p>Returns:</p> Name Type Description <code>TokenizedPrompts</code> <code>TokenizedPrompts | TorchTokenizedPrompts</code> <p>The next item in the dataset.</p> <p>Raises:</p> Type Description <code>StopIteration</code> <p>If the end of the dataset is reached.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __next__(self) -&gt; TokenizedPrompts | TorchTokenizedPrompts:\n    \"\"\"Return the next item in the dataset.\n\n    Returns:\n        TokenizedPrompts: The next item in the dataset.\n\n    Raises:\n        StopIteration: If the end of the dataset is reached.\n    \"\"\"\n    if self._index &lt; self._length:\n        item = self[self._index]\n        self._index += 1\n        return item\n\n    raise StopIteration\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.create_data","title":"<code>create_data(num_items, context_size)</code>","text":"<p>Create the data.</p> <p>Parameters:</p> Name Type Description Default <code>num_items</code> <code>int</code> <p>The number of items in the dataset.</p> required <code>context_size</code> <code>int</code> <p>The number of tokens in the context window.</p> required <p>Returns:</p> Type Description <code>Int[Tensor, 'items context_size']</code> <p>The generated data.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def create_data(self, num_items: int, context_size: int) -&gt; Int[Tensor, \"items context_size\"]:\n    \"\"\"Create the data.\n\n    Args:\n        num_items: The number of items in the dataset.\n        context_size: The number of tokens in the context window.\n\n    Returns:\n        The generated data.\n    \"\"\"\n    rows = torch.arange(num_items).unsqueeze(1)\n    columns = torch.arange(context_size).unsqueeze(0)\n    return rows + columns\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.ConsecutiveIntHuggingFaceDataset.with_format","title":"<code>with_format(type)</code>","text":"<p>With Format.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def with_format(  # type: ignore (only support 2 types)\n    self,\n    type: Literal[\"torch\", \"list\"],  # noqa: A002\n) -&gt; \"ConsecutiveIntHuggingFaceDataset\":\n    \"\"\"With Format.\"\"\"\n    self._format = type\n    return self\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset","title":"<code>MockDataset</code>","text":"<p>             Bases: <code>SourceDataset[TokenizedPrompts]</code></p> <p>Mock dataset for testing.</p> <p>For use with tests and simple examples.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>@final\nclass MockDataset(SourceDataset[TokenizedPrompts]):\n    \"\"\"Mock dataset for testing.\n\n    For use with tests and simple examples.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerFast\n\n    def preprocess(\n        self,\n        source_batch: TokenizedPrompts,\n        *,\n        context_size: int,  # noqa: ARG002\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\"\"\"\n        # Nothing to do here\n        return source_batch\n\n    def __init__(\n        self,\n        context_size: int = 250,\n        buffer_size: int = 1000,  # noqa: ARG002\n        preprocess_batch_size: int = 1000,  # noqa: ARG002\n        dataset_path: str = \"dummy\",  # noqa: ARG002\n        dataset_split: str = \"train\",  # noqa: ARG002\n    ):\n        \"\"\"Initialize the Random Int Dummy dataset.\n\n        Example:\n            &gt;&gt;&gt; data = MockDataset()\n            &gt;&gt;&gt; first_item = next(iter(data))\n            &gt;&gt;&gt; len(first_item[\"input_ids\"])\n            250\n\n        Args:\n            context_size: The context size to use when returning a list of tokenized prompts.\n                *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n                a context size of 250.\n            buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n                streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n                just that buffer. Note that the generated activations should also be shuffled before\n                training the sparse autoencoder, so a large buffer may not be strictly necessary\n                here. Note also that this is the number of items in the dataset (e.g. number of\n                prompts) and is typically significantly less than the number of tokenized prompts\n                once the preprocessing function has been applied.\n            preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n                tokenizing prompts).\n            dataset_path: The path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g. `train`).\n        \"\"\"\n        self.dataset = ConsecutiveIntHuggingFaceDataset(context_size=context_size)  # type: ignore\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset.__init__","title":"<code>__init__(context_size=250, buffer_size=1000, preprocess_batch_size=1000, dataset_path='dummy', dataset_split='train')</code>","text":"<p>Initialize the Random Int Dummy dataset.</p> Example <p>data = MockDataset() first_item = next(iter(data)) len(first_item[\"input_ids\"]) 250</p> <p>Parameters:</p> Name Type Description Default <code>context_size</code> <code>int</code> <p>The context size to use when returning a list of tokenized prompts. Towards Monosemanticity: Decomposing Language Models With Dictionary Learning used a context size of 250.</p> <code>250</code> <code>buffer_size</code> <code>int</code> <p>The buffer size to use when shuffling the dataset. As the dataset is streamed, this just pre-downloads at least <code>buffer_size</code> items and then shuffles just that buffer. Note that the generated activations should also be shuffled before training the sparse autoencoder, so a large buffer may not be strictly necessary here. Note also that this is the number of items in the dataset (e.g. number of prompts) and is typically significantly less than the number of tokenized prompts once the preprocessing function has been applied.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>int</code> <p>The batch size to use just for preprocessing the dataset (e.g. tokenizing prompts).</p> <code>1000</code> <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> <code>'dummy'</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g. <code>train</code>).</p> <code>'train'</code> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def __init__(\n    self,\n    context_size: int = 250,\n    buffer_size: int = 1000,  # noqa: ARG002\n    preprocess_batch_size: int = 1000,  # noqa: ARG002\n    dataset_path: str = \"dummy\",  # noqa: ARG002\n    dataset_split: str = \"train\",  # noqa: ARG002\n):\n    \"\"\"Initialize the Random Int Dummy dataset.\n\n    Example:\n        &gt;&gt;&gt; data = MockDataset()\n        &gt;&gt;&gt; first_item = next(iter(data))\n        &gt;&gt;&gt; len(first_item[\"input_ids\"])\n        250\n\n    Args:\n        context_size: The context size to use when returning a list of tokenized prompts.\n            *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* used\n            a context size of 250.\n        buffer_size: The buffer size to use when shuffling the dataset. As the dataset is\n            streamed, this just pre-downloads at least `buffer_size` items and then shuffles\n            just that buffer. Note that the generated activations should also be shuffled before\n            training the sparse autoencoder, so a large buffer may not be strictly necessary\n            here. Note also that this is the number of items in the dataset (e.g. number of\n            prompts) and is typically significantly less than the number of tokenized prompts\n            once the preprocessing function has been applied.\n        preprocess_batch_size: The batch size to use just for preprocessing the dataset (e.g.\n            tokenizing prompts).\n        dataset_path: The path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g. `train`).\n    \"\"\"\n    self.dataset = ConsecutiveIntHuggingFaceDataset(context_size=context_size)  # type: ignore\n</code></pre>"},{"location":"reference/source_data/mock_dataset/#sparse_autoencoder.source_data.mock_dataset.MockDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> Source code in <code>sparse_autoencoder/source_data/mock_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: TokenizedPrompts,\n    *,\n    context_size: int,  # noqa: ARG002\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\"\"\"\n    # Nothing to do here\n    return source_batch\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/","title":"Pre-Tokenized Dataset from Hugging Face","text":"<p>Pre-Tokenized Dataset from Hugging Face.</p> <p>PreTokenizedDataset should work with any of the following tokenized datasets: - NeelNanda/pile-small-tokenized-2b - NeelNanda/pile-tokenized-10b - NeelNanda/openwebtext-tokenized-9b - NeelNanda/c4-tokenized-2b - NeelNanda/code-tokenized - NeelNanda/c4-code-tokenized-2b - NeelNanda/pile-old-tokenized-2b</p>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataBatch","title":"<code>PreTokenizedDataBatch</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>General Pre-Tokenized Dataset Item.</p> <p>Structure depends on the specific dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>class PreTokenizedDataBatch(TypedDict):\n    \"\"\"General Pre-Tokenized Dataset Item.\n\n    Structure depends on the specific dataset from Hugging Face.\n    \"\"\"\n\n    tokens: list[\n        list[int]\n    ]  # This assumes that the dataset structure is similar to the original Neel Nanda dataset.\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset","title":"<code>PreTokenizedDataset</code>","text":"<p>             Bases: <code>SourceDataset[PreTokenizedDataBatch]</code></p> <p>General Pre-Tokenized Dataset from Hugging Face.</p> <p>Can be used for various datasets available on Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>@final\nclass PreTokenizedDataset(SourceDataset[PreTokenizedDataBatch]):\n    \"\"\"General Pre-Tokenized Dataset from Hugging Face.\n\n    Can be used for various datasets available on Hugging Face.\n    \"\"\"\n\n    def preprocess(\n        self,\n        source_batch: PreTokenizedDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        The method splits each pre-tokenized item based on the context size.\n\n        Args:\n            source_batch: A batch of source data.\n            context_size: The context size to use for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n        # Chunk each tokenized prompt into blocks of context_size,\n        # discarding the last block if too small.\n        context_size_prompts = []\n        for encoding in tokenized_prompts:\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        dataset_path: str,\n        context_size: int = 256,\n        buffer_size: int = 1000,\n        preprocess_batch_size: int = 1000,\n        dataset_split: str = \"train\",\n    ):\n        \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n        Args:\n            dataset_path: The path to the dataset on Hugging Face.\n            context_size: The context size for tokenized prompts.\n            buffer_size: Buffer size for shuffling the dataset.\n            preprocess_batch_size: Batch size for preprocessing.\n            dataset_split: Dataset split (e.g., `train`).\n        \"\"\"\n        super().__init__(\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            context_size=context_size,\n            buffer_size=buffer_size,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.__init__","title":"<code>__init__(dataset_path, context_size=256, buffer_size=1000, preprocess_batch_size=1000, dataset_split='train')</code>","text":"<p>Initialize a pre-tokenized dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str</code> <p>The path to the dataset on Hugging Face.</p> required <code>context_size</code> <code>int</code> <p>The context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>int</code> <p>Buffer size for shuffling the dataset.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>int</code> <p>Batch size for preprocessing.</p> <code>1000</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., <code>train</code>).</p> <code>'train'</code> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_path: str,\n    context_size: int = 256,\n    buffer_size: int = 1000,\n    preprocess_batch_size: int = 1000,\n    dataset_split: str = \"train\",\n):\n    \"\"\"Initialize a pre-tokenized dataset from Hugging Face.\n\n    Args:\n        dataset_path: The path to the dataset on Hugging Face.\n        context_size: The context size for tokenized prompts.\n        buffer_size: Buffer size for shuffling the dataset.\n        preprocess_batch_size: Batch size for preprocessing.\n        dataset_split: Dataset split (e.g., `train`).\n    \"\"\"\n    super().__init__(\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        context_size=context_size,\n        buffer_size=buffer_size,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/source_data/pretokenized_dataset/#sparse_autoencoder.source_data.pretokenized_dataset.PreTokenizedDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>The method splits each pre-tokenized item based on the context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>PreTokenizedDataBatch</code> <p>A batch of source data.</p> required <code>context_size</code> <code>int</code> <p>The context size to use for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/pretokenized_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: PreTokenizedDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    The method splits each pre-tokenized item based on the context size.\n\n    Args:\n        source_batch: A batch of source data.\n        context_size: The context size to use for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    tokenized_prompts: list[list[int]] = source_batch[\"tokens\"]\n\n    # Chunk each tokenized prompt into blocks of context_size,\n    # discarding the last block if too small.\n    context_size_prompts = []\n    for encoding in tokenized_prompts:\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/source_data/text_dataset/","title":"Generic Text Dataset Module for Hugging Face Datasets","text":"<p>Generic Text Dataset Module for Hugging Face Datasets.</p> <p>GenericTextDataset should work with the following datasets: - monology/pile-uncopyrighted - the_pile_openwebtext2 - roneneldan/TinyStories-33M - roneneldan/TinyStories-8M - roneneldan/TinyStories-3M - roneneldan/TinyStories-1Layer-21M - roneneldan/TinyStories-1M - roneneldan/TinyStories-2Layers-33M - roneneldan/TinyStories-Instruct-2Layers-33M - roneneldan/TinyStories-Instruct-28M - roneneldan/TinyStories-Instruct-33M - roneneldan/TinyStories-Instruct-8M - roneneldan/TinyStories-Instruct-3M - roneneldan/TinyStories-Instruct-1M - roneneldan/TinyStories-Instuct-1Layer-21M - roneneldan/TinyStories-28M</p>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.GenericTextDataBatch","title":"<code>GenericTextDataBatch</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>Generic Text Dataset Batch.</p> <p>Assumes the dataset provides a 'text' field with a list of strings.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>class GenericTextDataBatch(TypedDict):\n    \"\"\"Generic Text Dataset Batch.\n\n    Assumes the dataset provides a 'text' field with a list of strings.\n    \"\"\"\n\n    text: list[str]\n    meta: list[dict[str, dict[str, str]]]  # Optional, depending on the dataset structure.\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset","title":"<code>TextDataset</code>","text":"<p>             Bases: <code>SourceDataset[GenericTextDataBatch]</code></p> <p>Generic Text Dataset for any text-based dataset from Hugging Face.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>@final\nclass TextDataset(SourceDataset[GenericTextDataBatch]):\n    \"\"\"Generic Text Dataset for any text-based dataset from Hugging Face.\"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n\n    def preprocess(\n        self,\n        source_batch: GenericTextDataBatch,\n        *,\n        context_size: int,\n    ) -&gt; TokenizedPrompts:\n        \"\"\"Preprocess a batch of prompts.\n\n        Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n        Args:\n            source_batch: A batch of source data, including 'text' with a list of strings.\n            context_size: Context size for tokenized prompts.\n\n        Returns:\n            Tokenized prompts.\n        \"\"\"\n        prompts: list[str] = source_batch[\"text\"]\n\n        tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n        # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n        context_size_prompts = []\n        for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n            chunks = [\n                encoding[i : i + context_size]\n                for i in range(0, len(encoding), context_size)\n                if len(encoding[i : i + context_size]) == context_size\n            ]\n            context_size_prompts.extend(chunks)\n\n        return {\"input_ids\": context_size_prompts}\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizerBase,\n        context_size: int = 256,\n        buffer_size: int = 1000,\n        preprocess_batch_size: int = 1000,\n        dataset_path: str = \"monology/pile-uncopyrighted\",\n        dataset_split: str = \"train\",\n    ):\n        \"\"\"Initialize a generic text dataset from Hugging Face.\n\n        Args:\n            tokenizer: Tokenizer to process text data.\n            context_size: Context size for tokenized prompts.\n            buffer_size: Buffer size for shuffling the dataset.\n            preprocess_batch_size: Batch size for preprocessing.\n            dataset_path: Path to the dataset on Hugging Face.\n            dataset_split: Dataset split (e.g., 'train').\n        \"\"\"\n        self.tokenizer = tokenizer\n\n        super().__init__(\n            dataset_path=dataset_path,\n            dataset_split=dataset_split,\n            context_size=context_size,\n            buffer_size=buffer_size,\n            preprocess_batch_size=preprocess_batch_size,\n        )\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.__init__","title":"<code>__init__(tokenizer, context_size=256, buffer_size=1000, preprocess_batch_size=1000, dataset_path='monology/pile-uncopyrighted', dataset_split='train')</code>","text":"<p>Initialize a generic text dataset from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>Tokenizer to process text data.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> <code>256</code> <code>buffer_size</code> <code>int</code> <p>Buffer size for shuffling the dataset.</p> <code>1000</code> <code>preprocess_batch_size</code> <code>int</code> <p>Batch size for preprocessing.</p> <code>1000</code> <code>dataset_path</code> <code>str</code> <p>Path to the dataset on Hugging Face.</p> <code>'monology/pile-uncopyrighted'</code> <code>dataset_split</code> <code>str</code> <p>Dataset split (e.g., 'train').</p> <code>'train'</code> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizerBase,\n    context_size: int = 256,\n    buffer_size: int = 1000,\n    preprocess_batch_size: int = 1000,\n    dataset_path: str = \"monology/pile-uncopyrighted\",\n    dataset_split: str = \"train\",\n):\n    \"\"\"Initialize a generic text dataset from Hugging Face.\n\n    Args:\n        tokenizer: Tokenizer to process text data.\n        context_size: Context size for tokenized prompts.\n        buffer_size: Buffer size for shuffling the dataset.\n        preprocess_batch_size: Batch size for preprocessing.\n        dataset_path: Path to the dataset on Hugging Face.\n        dataset_split: Dataset split (e.g., 'train').\n    \"\"\"\n    self.tokenizer = tokenizer\n\n    super().__init__(\n        dataset_path=dataset_path,\n        dataset_split=dataset_split,\n        context_size=context_size,\n        buffer_size=buffer_size,\n        preprocess_batch_size=preprocess_batch_size,\n    )\n</code></pre>"},{"location":"reference/source_data/text_dataset/#sparse_autoencoder.source_data.text_dataset.TextDataset.preprocess","title":"<code>preprocess(source_batch, *, context_size)</code>","text":"<p>Preprocess a batch of prompts.</p> <p>Tokenizes and chunks text data into lists of tokenized prompts with specified context size.</p> <p>Parameters:</p> Name Type Description Default <code>source_batch</code> <code>GenericTextDataBatch</code> <p>A batch of source data, including 'text' with a list of strings.</p> required <code>context_size</code> <code>int</code> <p>Context size for tokenized prompts.</p> required <p>Returns:</p> Type Description <code>TokenizedPrompts</code> <p>Tokenized prompts.</p> Source code in <code>sparse_autoencoder/source_data/text_dataset.py</code> <pre><code>def preprocess(\n    self,\n    source_batch: GenericTextDataBatch,\n    *,\n    context_size: int,\n) -&gt; TokenizedPrompts:\n    \"\"\"Preprocess a batch of prompts.\n\n    Tokenizes and chunks text data into lists of tokenized prompts with specified context size.\n\n    Args:\n        source_batch: A batch of source data, including 'text' with a list of strings.\n        context_size: Context size for tokenized prompts.\n\n    Returns:\n        Tokenized prompts.\n    \"\"\"\n    prompts: list[str] = source_batch[\"text\"]\n\n    tokenized_prompts = self.tokenizer(prompts, truncation=True, padding=False)\n\n    # Chunk each tokenized prompt into blocks of context_size, discarding incomplete blocks.\n    context_size_prompts = []\n    for encoding in list(tokenized_prompts[\"input_ids\"]):  # type: ignore\n        chunks = [\n            encoding[i : i + context_size]\n            for i in range(0, len(encoding), context_size)\n            if len(encoding[i : i + context_size]) == context_size\n        ]\n        context_size_prompts.extend(chunks)\n\n    return {\"input_ids\": context_size_prompts}\n</code></pre>"},{"location":"reference/source_model/","title":"Source Model","text":"<p>Source Model.</p>"},{"location":"reference/source_model/replace_activations_hook/","title":"Replace activations hook","text":"<p>Replace activations hook.</p>"},{"location":"reference/source_model/replace_activations_hook/#sparse_autoencoder.source_model.replace_activations_hook.replace_activations_hook","title":"<code>replace_activations_hook(value, hook, sparse_autoencoder)</code>","text":"<p>Replace activations hook.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The activations to replace.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required <code>sparse_autoencoder</code> <code>AbstractAutoencoder</code> <p>The sparse autoencoder. This should be pre-initialised with <code>functools.partial</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Replaced activations.</p> Source code in <code>sparse_autoencoder/source_model/replace_activations_hook.py</code> <pre><code>def replace_activations_hook(\n    value: Tensor,\n    hook: HookPoint,  # noqa: ARG001\n    sparse_autoencoder: AbstractAutoencoder,\n) -&gt; Tensor:\n    \"\"\"Replace activations hook.\n\n    Args:\n        value: The activations to replace.\n        hook: The hook point.\n        sparse_autoencoder: The sparse autoencoder. This should be pre-initialised with\n            `functools.partial`.\n\n    Returns:\n        Replaced activations.\n    \"\"\"\n    # Squash to just have a \"*items\" and a \"batch\" dimension\n    original_shape = value.shape\n    squashed_value: InputOutputActivationBatch = value.view(-1, value.size(-1))\n\n    # Get the output activations from a forward pass of the SAE\n    _learned_activations, output_activations = sparse_autoencoder.forward(squashed_value)\n\n    # Reshape to the original shape\n    return output_activations.view(*original_shape)\n</code></pre>"},{"location":"reference/source_model/store_activations_hook/","title":"TransformerLens Hook for storing activations","text":"<p>TransformerLens Hook for storing activations.</p>"},{"location":"reference/source_model/store_activations_hook/#sparse_autoencoder.source_model.store_activations_hook.store_activations_hook","title":"<code>store_activations_hook(value, hook, store)</code>","text":"<p>Store Activations Hook.</p> <p>Useful for getting just the specific activations wanted, rather than the full cache.</p> Example <p>First we'll need a source model from TransformerLens and an activation store.</p> <p>from functools import partial from transformer_lens import HookedTransformer from sparse_autoencoder.activation_store.list_store import ListActivationStore store = ListActivationStore() model = HookedTransformer.from_pretrained(\"tiny-stories-1M\") Loaded pretrained model tiny-stories-1M into HookedTransformer</p> <p>Next we can add the hook to specific neurons (in this case the first MLP neurons), and create the tokens for a forward pass.</p> <p>model.add_hook( ...     \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store) ... ) tokens = model.to_tokens(\"Hello world\") tokens.shape torch.Size([1, 3])</p> <p>Then when we run the model, we should get one activation vector for each token (as we just have one batch item). Note we also set <code>stop_at_layer=1</code> as we don't need the logits or any other activations after the hook point that we've specified (in this case the first MLP layer).</p> <p>_output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required len(store) 3</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>SourceModelActivations</code> <p>The activations to store.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required <code>store</code> <code>ActivationStore</code> <p>The activation store. This should be pre-initialised with <code>functools.partial</code>.</p> required <p>Returns:</p> Type Description <code>SourceModelActivations</code> <p>Unmodified activations.</p> Source code in <code>sparse_autoencoder/source_model/store_activations_hook.py</code> <pre><code>def store_activations_hook(\n    value: SourceModelActivations,\n    hook: HookPoint,  # noqa: ARG001\n    store: ActivationStore,\n) -&gt; SourceModelActivations:\n    \"\"\"Store Activations Hook.\n\n    Useful for getting just the specific activations wanted, rather than the full cache.\n\n    Example:\n        First we'll need a source model from TransformerLens and an activation store.\n\n        &gt;&gt;&gt; from functools import partial\n        &gt;&gt;&gt; from transformer_lens import HookedTransformer\n        &gt;&gt;&gt; from sparse_autoencoder.activation_store.list_store import ListActivationStore\n        &gt;&gt;&gt; store = ListActivationStore()\n        &gt;&gt;&gt; model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n        Loaded pretrained model tiny-stories-1M into HookedTransformer\n\n        Next we can add the hook to specific neurons (in this case the first MLP neurons), and\n        create the tokens for a forward pass.\n\n        &gt;&gt;&gt; model.add_hook(\n        ...     \"blocks.0.mlp.hook_post\", partial(store_activations_hook, store=store)\n        ... )\n        &gt;&gt;&gt; tokens = model.to_tokens(\"Hello world\")\n        &gt;&gt;&gt; tokens.shape\n        torch.Size([1, 3])\n\n        Then when we run the model, we should get one activation vector for each token (as we just\n        have one batch item). Note we also set `stop_at_layer=1` as we don't need the logits or any\n        other activations after the hook point that we've specified (in this case the first MLP\n        layer).\n\n        &gt;&gt;&gt; _output = model.forward(\"Hello world\", stop_at_layer=1) # Change this layer as required\n        &gt;&gt;&gt; len(store)\n        3\n\n    Args:\n        value: The activations to store.\n        hook: The hook point.\n        store: The activation store. This should be pre-initialised with `functools.partial`.\n\n    Returns:\n        Unmodified activations.\n    \"\"\"\n    store.extend(value)\n\n    # Return the unmodified value\n    return value\n</code></pre>"},{"location":"reference/source_model/zero_ablate_hook/","title":"Zero ablate hook","text":"<p>Zero ablate hook.</p>"},{"location":"reference/source_model/zero_ablate_hook/#sparse_autoencoder.source_model.zero_ablate_hook.zero_ablate_hook","title":"<code>zero_ablate_hook(value, hook)</code>","text":"<p>Zero ablate hook.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The activations to store.</p> required <code>hook</code> <code>HookPoint</code> <p>The hook point.</p> required Example <p>dummy_hook_point = HookPoint() value = torch.ones(2, 3) zero_ablate_hook(value, dummy_hook_point) tensor([[0., 0., 0.],         [0., 0., 0.]])</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Replaced activations.</p> Source code in <code>sparse_autoencoder/source_model/zero_ablate_hook.py</code> <pre><code>def zero_ablate_hook(\n    value: Tensor,\n    hook: HookPoint,  # noqa: ARG001\n) -&gt; Tensor:\n    \"\"\"Zero ablate hook.\n\n    Args:\n        value: The activations to store.\n        hook: The hook point.\n\n    Example:\n        &gt;&gt;&gt; dummy_hook_point = HookPoint()\n        &gt;&gt;&gt; value = torch.ones(2, 3)\n        &gt;&gt;&gt; zero_ablate_hook(value, dummy_hook_point)\n        tensor([[0., 0., 0.],\n                [0., 0., 0.]])\n\n    Returns:\n        Replaced activations.\n    \"\"\"\n    return torch.zeros_like(value)\n</code></pre>"},{"location":"reference/train/","title":"Train","text":"<p>Train.</p>"},{"location":"reference/train/pipeline/","title":"Default pipeline","text":"<p>Default pipeline.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline for training a Sparse Autoencoder on TransformerLens activations.</p> <p>Includes all the key functionality to train a sparse autoencoder, with a specific set of     hyperparameters.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline for training a Sparse Autoencoder on TransformerLens activations.\n\n    Includes all the key functionality to train a sparse autoencoder, with a specific set of\n        hyperparameters.\n    \"\"\"\n\n    activation_resampler: AbstractActivationResampler | None\n    \"\"\"Activation resampler to use.\"\"\"\n\n    autoencoder: SparseAutoencoder\n    \"\"\"Sparse autoencoder to train.\"\"\"\n\n    cache_name: str\n    \"\"\"Name of the cache to use in the source model (hook point).\"\"\"\n\n    layer: int\n    \"\"\"Layer to get activations from with the source model.\"\"\"\n\n    log_frequency: int\n    \"\"\"Frequency at which to log metrics (in steps).\"\"\"\n\n    loss: AbstractLoss\n    \"\"\"Loss function to use.\"\"\"\n\n    metrics: MetricsContainer\n    \"\"\"Metrics to use.\"\"\"\n\n    optimizer: AbstractOptimizerWithReset\n    \"\"\"Optimizer to use.\"\"\"\n\n    progress_bar: tqdm | None\n    \"\"\"Progress bar for the pipeline.\"\"\"\n\n    source_data: Iterable[TorchTokenizedPrompts]\n    \"\"\"Iterable over the source data.\"\"\"\n\n    source_dataset: SourceDataset\n    \"\"\"Source dataset to generate activation data from (tokenized prompts).\"\"\"\n\n    source_model: HookedTransformer\n    \"\"\"Source model to get activations from.\"\"\"\n\n    total_training_steps: int = 1\n    \"\"\"Total number of training steps state.\"\"\"\n\n    @final\n    def __init__(  # noqa: PLR0913\n        self,\n        activation_resampler: AbstractActivationResampler | None,\n        autoencoder: SparseAutoencoder,\n        cache_name: str,\n        layer: int,\n        loss: AbstractLoss,\n        optimizer: AbstractOptimizerWithReset,\n        source_dataset: SourceDataset,\n        source_model: HookedTransformer,\n        checkpoint_directory: Path | None = None,\n        log_frequency: int = 100,\n        metrics: MetricsContainer = default_metrics,\n        source_data_batch_size: int = 12,\n    ) -&gt; None:\n        \"\"\"Initialize the pipeline.\n\n        Args:\n            activation_resampler: Activation resampler to use.\n            autoencoder: Sparse autoencoder to train.\n            cache_name: Name of the cache to use in the source model (hook point).\n            layer: Layer to get activations from with the source model.\n            loss: Loss function to use.\n            optimizer: Optimizer to use.\n            source_dataset: Source dataset to get data from.\n            source_model: Source model to get activations from.\n            checkpoint_directory: Directory to save checkpoints to.\n            log_frequency: Frequency at which to log metrics (in steps)\n            metrics: Metrics to use.\n            source_data_batch_size: Batch size for the source data.\n        \"\"\"\n        self.activation_resampler = activation_resampler\n        self.autoencoder = autoencoder\n        self.cache_name = cache_name\n        self.checkpoint_directory = checkpoint_directory\n        self.layer = layer\n        self.log_frequency = log_frequency\n        self.loss = loss\n        self.metrics = metrics\n        self.optimizer = optimizer\n        self.source_data_batch_size = source_data_batch_size\n        self.source_dataset = source_dataset\n        self.source_model = source_model\n\n        source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n        self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n\n    def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n        \"\"\"Generate activations.\n\n        Args:\n            store_size: Number of activations to generate.\n\n        Returns:\n            Activation store for the train section.\n\n        Raises:\n            ValueError: If the store size is not positive or is not divisible by the batch size.\n        \"\"\"\n        # Check the store size is positive and divisible by the batch size\n        if store_size &lt;= 0:\n            error_message = f\"Store size must be positive, got {store_size}\"\n            raise ValueError(error_message)\n        if store_size % self.source_data_batch_size != 0:\n            error_message = (\n                f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n                f\"got {store_size}\"\n            )\n            raise ValueError(error_message)\n\n        # Setup the store\n        num_neurons: int = self.autoencoder.n_input_features\n        source_model_device: torch.device = get_model_device(self.source_model)\n        store = TensorActivationStore(store_size, num_neurons)\n\n        # Add the hook to the model (will automatically store the activations every time the model\n        # runs)\n        self.source_model.remove_all_hook_fns()\n        hook = partial(store_activations_hook, store=store)\n        self.source_model.add_hook(self.cache_name, hook)\n\n        # Loop through the dataloader until the store reaches the desired size\n        with torch.no_grad():\n            for batch in self.source_data:\n                input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n                self.source_model.forward(\n                    input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n                )  # type: ignore (TLens is typed incorrectly)\n\n                if len(store) &gt;= store_size:\n                    break\n\n        self.source_model.remove_all_hook_fns()\n        store.shuffle()\n\n        return store\n\n    def train_autoencoder(\n        self, activation_store: TensorActivationStore, train_batch_size: int\n    ) -&gt; NeuronActivity:\n        \"\"\"Train the sparse autoencoder.\n\n        Args:\n            activation_store: Activation store from the generate section.\n            train_batch_size: Train batch size.\n\n        Returns:\n            Number of times each neuron fired.\n        \"\"\"\n        autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n        activations_dataloader = DataLoader(\n            activation_store,\n            batch_size=train_batch_size,\n        )\n\n        learned_activations_fired_count: NeuronActivity = torch.zeros(\n            self.autoencoder.n_learned_features, dtype=torch.int32, device=autoencoder_device\n        )\n\n        for store_batch in activations_dataloader:\n            # Zero the gradients\n            self.optimizer.zero_grad()\n\n            # Move the batch to the device (in place)\n            batch = store_batch.detach().to(autoencoder_device)\n\n            # Forward pass\n            learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n            # Get loss &amp; metrics\n            metrics = {}\n            total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n                batch, learned_activations, reconstructed_activations\n            )\n            metrics.update(loss_metrics)\n\n            with torch.no_grad():\n                for metric in self.metrics.train_metrics:\n                    calculated = metric.calculate(\n                        TrainMetricData(batch, learned_activations, reconstructed_activations)\n                    )\n                    metrics.update(calculated)\n\n            # Store count of how many neurons have fired\n            with torch.no_grad():\n                fired = learned_activations &gt; 0\n                learned_activations_fired_count.add_(fired.sum(dim=0))\n\n            # Backwards pass\n            total_loss.backward()\n            self.optimizer.step()\n            self.autoencoder.decoder.constrain_weights_unit_norm()\n\n            # Log\n            if wandb.run is not None and self.total_training_steps % self.log_frequency == 0:\n                wandb.log(\n                    data={**metrics, **loss_metrics}, step=self.total_training_steps, commit=True\n                )\n            self.total_training_steps += 1\n\n        return learned_activations_fired_count\n\n    def resample_neurons(\n        self,\n        activation_store: TensorActivationStore,\n        neuron_activity_sample_size: int,\n        neuron_activity: NeuronActivity,\n        train_batch_size: int,\n    ) -&gt; None:\n        \"\"\"Resample dead neurons.\n\n        Args:\n            activation_store: Activation store.\n            neuron_activity_sample_size: Sample size for resampling.\n            neuron_activity: Number of times each neuron fired.\n            train_batch_size: Train batch size (also used for resampling).\n        \"\"\"\n        if self.activation_resampler is not None:\n            # Get the updates\n            parameter_updates = self.activation_resampler.resample_dead_neurons(\n                activation_store=activation_store,\n                autoencoder=self.autoencoder,\n                loss_fn=self.loss,\n                neuron_activity=neuron_activity,\n                train_batch_size=train_batch_size,\n                neuron_activity_sample_size=neuron_activity_sample_size,\n            )\n\n            # Update the weights and biases\n            self.autoencoder.encoder.update_dictionary_vectors(\n                parameter_updates.dead_neuron_indices,\n                parameter_updates.dead_encoder_weight_updates,\n            )\n            self.autoencoder.encoder.update_bias(\n                parameter_updates.dead_neuron_indices,\n                parameter_updates.dead_encoder_bias_updates,\n            )\n            self.autoencoder.decoder.update_dictionary_vectors(\n                parameter_updates.dead_neuron_indices,\n                parameter_updates.dead_decoder_weight_updates,\n            )\n\n            # Log any metrics\n            with torch.no_grad():\n                metrics = {}\n                if wandb.run is not None:\n                    for metric in self.metrics.resample_metrics:\n                        calculated = metric.calculate(\n                            ResampleMetricData(\n                                neuron_activity=neuron_activity,\n                            )\n                        )\n                        metrics.update(calculated)\n                    wandb.log(metrics, commit=False)\n\n            # Reset the optimizer (TODO: Consider resetting just the relevant parameters)\n            self.optimizer.reset_state_all_parameters()\n\n    def validate_sae(self, validation_number_activations: int) -&gt; None:\n        \"\"\"Get validation metrics.\n\n        Args:\n            validation_number_activations: Number of activations to use for validation.\n        \"\"\"\n        losses: list[float] = []\n        losses_with_reconstruction: list[float] = []\n        losses_with_zero_ablation: list[float] = []\n        source_model_device: torch.device = get_model_device(self.source_model)\n\n        for batch in self.source_data:\n            input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n\n            # Run a forward pass with and without the replaced activations\n            self.source_model.remove_all_hook_fns()\n            replacement_hook = partial(\n                replace_activations_hook, sparse_autoencoder=self.autoencoder\n            )\n\n            loss = self.source_model.forward(input_ids, return_type=\"loss\")\n            loss_with_reconstruction = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[\n                    (\n                        self.cache_name,\n                        replacement_hook,\n                    )\n                ],\n            )\n            loss_with_zero_ablation = self.source_model.run_with_hooks(\n                input_ids,\n                return_type=\"loss\",\n                fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n            )\n\n            losses.append(loss.sum().item())\n            losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n            losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n            if len(losses) &gt;= validation_number_activations:\n                break\n\n        # Log\n        validation_data = ValidationMetricData(\n            source_model_loss=torch.tensor(losses),\n            source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n            source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n        )\n        for metric in self.metrics.validation_metrics:\n            calculated = metric.calculate(validation_data)\n            wandb.log(data=calculated, step=self.total_training_steps, commit=False)\n\n    @final\n    def save_checkpoint(self) -&gt; None:\n        \"\"\"Save the model as a checkpoint.\"\"\"\n        if self.checkpoint_directory:\n            file_path: Path = (\n                self.checkpoint_directory / f\"sae_state_dict-{self.total_training_steps}.pt\"\n            )\n            torch.save(self.autoencoder.state_dict(), file_path)\n\n    def run_pipeline(\n        self,\n        train_batch_size: int,\n        max_store_size: int,\n        max_activations: int,\n        resample_frequency: int,\n        validation_number_activations: int = 1024,\n        validate_frequency: int | None = None,\n        checkpoint_frequency: int | None = None,\n    ) -&gt; None:\n        \"\"\"Run the full training pipeline.\n\n        Args:\n            train_batch_size: Train batch size.\n            max_store_size: Maximum size of the activation store.\n            max_activations: Maximum total number of activations to train on (the original paper\n                used 8bn, although others have had success with 100m+).\n            resample_frequency: Frequency at which to resample dead neurons (the original paper used\n                every 200m).\n            validation_number_activations: Number of activations to use for validation.\n            validate_frequency: Frequency at which to get validation metrics.\n            checkpoint_frequency: Frequency at which to save a checkpoint.\n        \"\"\"\n        last_resampled: int = 0\n        neuron_activity_sample_size: int = 0\n        last_validated: int = 0\n        last_checkpoint: int = 0\n        total_activations: int = 0\n        neuron_activity: NeuronActivity = torch.zeros(self.autoencoder.n_learned_features)\n\n        self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n        # Get the store size\n        store_size: int = max_store_size - max_store_size % (\n            self.source_data_batch_size * self.source_dataset.context_size\n        )\n\n        with tqdm(\n            desc=\"Activations trained on\",\n            total=max_activations,\n        ) as progress_bar:\n            for _ in range(0, max_activations, store_size):\n                # Generate\n                progress_bar.set_postfix({\"stage\": \"generate\"})\n                activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n                # Update the counters\n                num_activation_vectors_in_store = len(activation_store)\n                last_resampled += num_activation_vectors_in_store\n                last_validated += num_activation_vectors_in_store\n                last_checkpoint += num_activation_vectors_in_store\n                total_activations += num_activation_vectors_in_store\n                if wandb.run is not None:\n                    wandb.log({\"activations_generated\": total_activations}, commit=False)\n\n                # Train\n                progress_bar.set_postfix({\"stage\": \"train\"})\n                batch_neuron_activity: NeuronActivity = self.train_autoencoder(\n                    activation_store, train_batch_size=train_batch_size\n                )\n                detached_neuron_activity = batch_neuron_activity.detach().cpu()\n                is_second_half_resample: bool = last_resampled &gt; resample_frequency / 2\n                if is_second_half_resample:\n                    neuron_activity_sample_size += num_activation_vectors_in_store\n                    neuron_activity.add_(detached_neuron_activity)\n\n                # Resample dead neurons (if needed)\n                progress_bar.set_postfix({\"stage\": \"resample\"})\n                if last_resampled &gt;= resample_frequency and self.activation_resampler is not None:\n                    self.resample_neurons(\n                        activation_store=activation_store,\n                        neuron_activity_sample_size=neuron_activity_sample_size,\n                        neuron_activity=neuron_activity,\n                        train_batch_size=train_batch_size,\n                    )\n\n                    # Reset\n                    last_resampled = 0\n                    neuron_activity_sample_size = 0\n                    neuron_activity.zero_()\n\n                # Get validation metrics (if needed)\n                progress_bar.set_postfix({\"stage\": \"validate\"})\n                if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                    self.validate_sae(validation_number_activations)\n                    last_validated = 0\n\n                # Checkpoint (if needed)\n                progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n                if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                    last_checkpoint = 0\n                    self.save_checkpoint()\n\n                # Update the progress bar\n                progress_bar.update(store_size)\n\n    @staticmethod\n    def stateful_dataloader_iterable(\n        dataloader: DataLoader[TorchTokenizedPrompts]\n    ) -&gt; Iterable[TorchTokenizedPrompts]:\n        \"\"\"Create a stateful dataloader iterable.\n\n        Create an iterable that maintains it's position in the dataloader between loops.\n\n        Examples:\n            Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n            (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n            position from where the first loop left off.\n\n            &gt;&gt;&gt; from datasets import Dataset\n            &gt;&gt;&gt; from torch.utils.data import DataLoader\n            &gt;&gt;&gt; def gen():\n            ...     yield {\"int\": 0}\n            ...     yield {\"int\": 1}\n            &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n            &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n            (tensor([0]), tensor([0]))\n\n            By contrast if you create a stateful iterable from the dataloader, each loop will get\n            different data.\n\n            &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n            &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n            (tensor([0]), tensor([1]))\n\n        Args:\n            dataloader: PyTorch DataLoader.\n\n        Returns:\n            Stateful iterable over the data in the dataloader.\n\n        Yields:\n            Data from the dataloader.\n        \"\"\"\n        yield from dataloader\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.activation_resampler","title":"<code>activation_resampler: AbstractActivationResampler | None = activation_resampler</code>  <code>instance-attribute</code>","text":"<p>Activation resampler to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.autoencoder","title":"<code>autoencoder: SparseAutoencoder = autoencoder</code>  <code>instance-attribute</code>","text":"<p>Sparse autoencoder to train.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.cache_name","title":"<code>cache_name: str = cache_name</code>  <code>instance-attribute</code>","text":"<p>Name of the cache to use in the source model (hook point).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.layer","title":"<code>layer: int = layer</code>  <code>instance-attribute</code>","text":"<p>Layer to get activations from with the source model.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.log_frequency","title":"<code>log_frequency: int = log_frequency</code>  <code>instance-attribute</code>","text":"<p>Frequency at which to log metrics (in steps).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.loss","title":"<code>loss: AbstractLoss = loss</code>  <code>instance-attribute</code>","text":"<p>Loss function to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.metrics","title":"<code>metrics: MetricsContainer = metrics</code>  <code>instance-attribute</code>","text":"<p>Metrics to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.optimizer","title":"<code>optimizer: AbstractOptimizerWithReset = optimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer to use.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.progress_bar","title":"<code>progress_bar: tqdm | None</code>  <code>instance-attribute</code>","text":"<p>Progress bar for the pipeline.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_data","title":"<code>source_data: Iterable[TorchTokenizedPrompts] = self.stateful_dataloader_iterable(source_dataloader)</code>  <code>instance-attribute</code>","text":"<p>Iterable over the source data.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_dataset","title":"<code>source_dataset: SourceDataset = source_dataset</code>  <code>instance-attribute</code>","text":"<p>Source dataset to generate activation data from (tokenized prompts).</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.source_model","title":"<code>source_model: HookedTransformer = source_model</code>  <code>instance-attribute</code>","text":"<p>Source model to get activations from.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.total_training_steps","title":"<code>total_training_steps: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Total number of training steps state.</p>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.__init__","title":"<code>__init__(activation_resampler, autoencoder, cache_name, layer, loss, optimizer, source_dataset, source_model, checkpoint_directory=None, log_frequency=100, metrics=default_metrics, source_data_batch_size=12)</code>","text":"<p>Initialize the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>activation_resampler</code> <code>AbstractActivationResampler | None</code> <p>Activation resampler to use.</p> required <code>autoencoder</code> <code>SparseAutoencoder</code> <p>Sparse autoencoder to train.</p> required <code>cache_name</code> <code>str</code> <p>Name of the cache to use in the source model (hook point).</p> required <code>layer</code> <code>int</code> <p>Layer to get activations from with the source model.</p> required <code>loss</code> <code>AbstractLoss</code> <p>Loss function to use.</p> required <code>optimizer</code> <code>AbstractOptimizerWithReset</code> <p>Optimizer to use.</p> required <code>source_dataset</code> <code>SourceDataset</code> <p>Source dataset to get data from.</p> required <code>source_model</code> <code>HookedTransformer</code> <p>Source model to get activations from.</p> required <code>checkpoint_directory</code> <code>Path | None</code> <p>Directory to save checkpoints to.</p> <code>None</code> <code>log_frequency</code> <code>int</code> <p>Frequency at which to log metrics (in steps)</p> <code>100</code> <code>metrics</code> <code>MetricsContainer</code> <p>Metrics to use.</p> <code>default_metrics</code> <code>source_data_batch_size</code> <code>int</code> <p>Batch size for the source data.</p> <code>12</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef __init__(  # noqa: PLR0913\n    self,\n    activation_resampler: AbstractActivationResampler | None,\n    autoencoder: SparseAutoencoder,\n    cache_name: str,\n    layer: int,\n    loss: AbstractLoss,\n    optimizer: AbstractOptimizerWithReset,\n    source_dataset: SourceDataset,\n    source_model: HookedTransformer,\n    checkpoint_directory: Path | None = None,\n    log_frequency: int = 100,\n    metrics: MetricsContainer = default_metrics,\n    source_data_batch_size: int = 12,\n) -&gt; None:\n    \"\"\"Initialize the pipeline.\n\n    Args:\n        activation_resampler: Activation resampler to use.\n        autoencoder: Sparse autoencoder to train.\n        cache_name: Name of the cache to use in the source model (hook point).\n        layer: Layer to get activations from with the source model.\n        loss: Loss function to use.\n        optimizer: Optimizer to use.\n        source_dataset: Source dataset to get data from.\n        source_model: Source model to get activations from.\n        checkpoint_directory: Directory to save checkpoints to.\n        log_frequency: Frequency at which to log metrics (in steps)\n        metrics: Metrics to use.\n        source_data_batch_size: Batch size for the source data.\n    \"\"\"\n    self.activation_resampler = activation_resampler\n    self.autoencoder = autoencoder\n    self.cache_name = cache_name\n    self.checkpoint_directory = checkpoint_directory\n    self.layer = layer\n    self.log_frequency = log_frequency\n    self.loss = loss\n    self.metrics = metrics\n    self.optimizer = optimizer\n    self.source_data_batch_size = source_data_batch_size\n    self.source_dataset = source_dataset\n    self.source_model = source_model\n\n    source_dataloader = source_dataset.get_dataloader(source_data_batch_size)\n    self.source_data = self.stateful_dataloader_iterable(source_dataloader)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.generate_activations","title":"<code>generate_activations(store_size)</code>","text":"<p>Generate activations.</p> <p>Parameters:</p> Name Type Description Default <code>store_size</code> <code>int</code> <p>Number of activations to generate.</p> required <p>Returns:</p> Type Description <code>TensorActivationStore</code> <p>Activation store for the train section.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the store size is not positive or is not divisible by the batch size.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def generate_activations(self, store_size: int) -&gt; TensorActivationStore:\n    \"\"\"Generate activations.\n\n    Args:\n        store_size: Number of activations to generate.\n\n    Returns:\n        Activation store for the train section.\n\n    Raises:\n        ValueError: If the store size is not positive or is not divisible by the batch size.\n    \"\"\"\n    # Check the store size is positive and divisible by the batch size\n    if store_size &lt;= 0:\n        error_message = f\"Store size must be positive, got {store_size}\"\n        raise ValueError(error_message)\n    if store_size % self.source_data_batch_size != 0:\n        error_message = (\n            f\"Store size must be divisible by the batch size ({self.source_data_batch_size}), \"\n            f\"got {store_size}\"\n        )\n        raise ValueError(error_message)\n\n    # Setup the store\n    num_neurons: int = self.autoencoder.n_input_features\n    source_model_device: torch.device = get_model_device(self.source_model)\n    store = TensorActivationStore(store_size, num_neurons)\n\n    # Add the hook to the model (will automatically store the activations every time the model\n    # runs)\n    self.source_model.remove_all_hook_fns()\n    hook = partial(store_activations_hook, store=store)\n    self.source_model.add_hook(self.cache_name, hook)\n\n    # Loop through the dataloader until the store reaches the desired size\n    with torch.no_grad():\n        for batch in self.source_data:\n            input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n            self.source_model.forward(\n                input_ids, stop_at_layer=self.layer + 1, prepend_bos=False\n            )  # type: ignore (TLens is typed incorrectly)\n\n            if len(store) &gt;= store_size:\n                break\n\n    self.source_model.remove_all_hook_fns()\n    store.shuffle()\n\n    return store\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.resample_neurons","title":"<code>resample_neurons(activation_store, neuron_activity_sample_size, neuron_activity, train_batch_size)</code>","text":"<p>Resample dead neurons.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store.</p> required <code>neuron_activity_sample_size</code> <code>int</code> <p>Sample size for resampling.</p> required <code>neuron_activity</code> <code>NeuronActivity</code> <p>Number of times each neuron fired.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size (also used for resampling).</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def resample_neurons(\n    self,\n    activation_store: TensorActivationStore,\n    neuron_activity_sample_size: int,\n    neuron_activity: NeuronActivity,\n    train_batch_size: int,\n) -&gt; None:\n    \"\"\"Resample dead neurons.\n\n    Args:\n        activation_store: Activation store.\n        neuron_activity_sample_size: Sample size for resampling.\n        neuron_activity: Number of times each neuron fired.\n        train_batch_size: Train batch size (also used for resampling).\n    \"\"\"\n    if self.activation_resampler is not None:\n        # Get the updates\n        parameter_updates = self.activation_resampler.resample_dead_neurons(\n            activation_store=activation_store,\n            autoencoder=self.autoencoder,\n            loss_fn=self.loss,\n            neuron_activity=neuron_activity,\n            train_batch_size=train_batch_size,\n            neuron_activity_sample_size=neuron_activity_sample_size,\n        )\n\n        # Update the weights and biases\n        self.autoencoder.encoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_weight_updates,\n        )\n        self.autoencoder.encoder.update_bias(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_encoder_bias_updates,\n        )\n        self.autoencoder.decoder.update_dictionary_vectors(\n            parameter_updates.dead_neuron_indices,\n            parameter_updates.dead_decoder_weight_updates,\n        )\n\n        # Log any metrics\n        with torch.no_grad():\n            metrics = {}\n            if wandb.run is not None:\n                for metric in self.metrics.resample_metrics:\n                    calculated = metric.calculate(\n                        ResampleMetricData(\n                            neuron_activity=neuron_activity,\n                        )\n                    )\n                    metrics.update(calculated)\n                wandb.log(metrics, commit=False)\n\n        # Reset the optimizer (TODO: Consider resetting just the relevant parameters)\n        self.optimizer.reset_state_all_parameters()\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.run_pipeline","title":"<code>run_pipeline(train_batch_size, max_store_size, max_activations, resample_frequency, validation_number_activations=1024, validate_frequency=None, checkpoint_frequency=None)</code>","text":"<p>Run the full training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <code>max_store_size</code> <code>int</code> <p>Maximum size of the activation store.</p> required <code>max_activations</code> <code>int</code> <p>Maximum total number of activations to train on (the original paper used 8bn, although others have had success with 100m+).</p> required <code>resample_frequency</code> <code>int</code> <p>Frequency at which to resample dead neurons (the original paper used every 200m).</p> required <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> <code>1024</code> <code>validate_frequency</code> <code>int | None</code> <p>Frequency at which to get validation metrics.</p> <code>None</code> <code>checkpoint_frequency</code> <code>int | None</code> <p>Frequency at which to save a checkpoint.</p> <code>None</code> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def run_pipeline(\n    self,\n    train_batch_size: int,\n    max_store_size: int,\n    max_activations: int,\n    resample_frequency: int,\n    validation_number_activations: int = 1024,\n    validate_frequency: int | None = None,\n    checkpoint_frequency: int | None = None,\n) -&gt; None:\n    \"\"\"Run the full training pipeline.\n\n    Args:\n        train_batch_size: Train batch size.\n        max_store_size: Maximum size of the activation store.\n        max_activations: Maximum total number of activations to train on (the original paper\n            used 8bn, although others have had success with 100m+).\n        resample_frequency: Frequency at which to resample dead neurons (the original paper used\n            every 200m).\n        validation_number_activations: Number of activations to use for validation.\n        validate_frequency: Frequency at which to get validation metrics.\n        checkpoint_frequency: Frequency at which to save a checkpoint.\n    \"\"\"\n    last_resampled: int = 0\n    neuron_activity_sample_size: int = 0\n    last_validated: int = 0\n    last_checkpoint: int = 0\n    total_activations: int = 0\n    neuron_activity: NeuronActivity = torch.zeros(self.autoencoder.n_learned_features)\n\n    self.source_model.eval()  # Set the source model to evaluation (inference) mode\n\n    # Get the store size\n    store_size: int = max_store_size - max_store_size % (\n        self.source_data_batch_size * self.source_dataset.context_size\n    )\n\n    with tqdm(\n        desc=\"Activations trained on\",\n        total=max_activations,\n    ) as progress_bar:\n        for _ in range(0, max_activations, store_size):\n            # Generate\n            progress_bar.set_postfix({\"stage\": \"generate\"})\n            activation_store: TensorActivationStore = self.generate_activations(store_size)\n\n            # Update the counters\n            num_activation_vectors_in_store = len(activation_store)\n            last_resampled += num_activation_vectors_in_store\n            last_validated += num_activation_vectors_in_store\n            last_checkpoint += num_activation_vectors_in_store\n            total_activations += num_activation_vectors_in_store\n            if wandb.run is not None:\n                wandb.log({\"activations_generated\": total_activations}, commit=False)\n\n            # Train\n            progress_bar.set_postfix({\"stage\": \"train\"})\n            batch_neuron_activity: NeuronActivity = self.train_autoencoder(\n                activation_store, train_batch_size=train_batch_size\n            )\n            detached_neuron_activity = batch_neuron_activity.detach().cpu()\n            is_second_half_resample: bool = last_resampled &gt; resample_frequency / 2\n            if is_second_half_resample:\n                neuron_activity_sample_size += num_activation_vectors_in_store\n                neuron_activity.add_(detached_neuron_activity)\n\n            # Resample dead neurons (if needed)\n            progress_bar.set_postfix({\"stage\": \"resample\"})\n            if last_resampled &gt;= resample_frequency and self.activation_resampler is not None:\n                self.resample_neurons(\n                    activation_store=activation_store,\n                    neuron_activity_sample_size=neuron_activity_sample_size,\n                    neuron_activity=neuron_activity,\n                    train_batch_size=train_batch_size,\n                )\n\n                # Reset\n                last_resampled = 0\n                neuron_activity_sample_size = 0\n                neuron_activity.zero_()\n\n            # Get validation metrics (if needed)\n            progress_bar.set_postfix({\"stage\": \"validate\"})\n            if validate_frequency is not None and last_validated &gt;= validate_frequency:\n                self.validate_sae(validation_number_activations)\n                last_validated = 0\n\n            # Checkpoint (if needed)\n            progress_bar.set_postfix({\"stage\": \"checkpoint\"})\n            if checkpoint_frequency is not None and last_checkpoint &gt;= checkpoint_frequency:\n                last_checkpoint = 0\n                self.save_checkpoint()\n\n            # Update the progress bar\n            progress_bar.update(store_size)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.save_checkpoint","title":"<code>save_checkpoint()</code>","text":"<p>Save the model as a checkpoint.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@final\ndef save_checkpoint(self) -&gt; None:\n    \"\"\"Save the model as a checkpoint.\"\"\"\n    if self.checkpoint_directory:\n        file_path: Path = (\n            self.checkpoint_directory / f\"sae_state_dict-{self.total_training_steps}.pt\"\n        )\n        torch.save(self.autoencoder.state_dict(), file_path)\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.stateful_dataloader_iterable","title":"<code>stateful_dataloader_iterable(dataloader)</code>  <code>staticmethod</code>","text":"<p>Create a stateful dataloader iterable.</p> <p>Create an iterable that maintains it's position in the dataloader between loops.</p> <p>Examples:</p> <p>Without this, when iterating over a DataLoader with 2 loops, each loop get the same data (assuming shuffle is turned off). That is to say, the second loop won't maintain the position from where the first loop left off.</p> <pre><code>&gt;&gt;&gt; from datasets import Dataset\n&gt;&gt;&gt; from torch.utils.data import DataLoader\n&gt;&gt;&gt; def gen():\n...     yield {\"int\": 0}\n...     yield {\"int\": 1}\n&gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n&gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n(tensor([0]), tensor([0]))\n</code></pre> <p>By contrast if you create a stateful iterable from the dataloader, each loop will get different data.</p> <pre><code>&gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n&gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n(tensor([0]), tensor([1]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader[TorchTokenizedPrompts]</code> <p>PyTorch DataLoader.</p> required <p>Returns:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Stateful iterable over the data in the dataloader.</p> <p>Yields:</p> Type Description <code>Iterable[TorchTokenizedPrompts]</code> <p>Data from the dataloader.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>@staticmethod\ndef stateful_dataloader_iterable(\n    dataloader: DataLoader[TorchTokenizedPrompts]\n) -&gt; Iterable[TorchTokenizedPrompts]:\n    \"\"\"Create a stateful dataloader iterable.\n\n    Create an iterable that maintains it's position in the dataloader between loops.\n\n    Examples:\n        Without this, when iterating over a DataLoader with 2 loops, each loop get the same data\n        (assuming shuffle is turned off). That is to say, the second loop won't maintain the\n        position from where the first loop left off.\n\n        &gt;&gt;&gt; from datasets import Dataset\n        &gt;&gt;&gt; from torch.utils.data import DataLoader\n        &gt;&gt;&gt; def gen():\n        ...     yield {\"int\": 0}\n        ...     yield {\"int\": 1}\n        &gt;&gt;&gt; data = DataLoader(Dataset.from_generator(gen))\n        &gt;&gt;&gt; next(iter(data))[\"int\"], next(iter(data))[\"int\"]\n        (tensor([0]), tensor([0]))\n\n        By contrast if you create a stateful iterable from the dataloader, each loop will get\n        different data.\n\n        &gt;&gt;&gt; iterator = Pipeline.stateful_dataloader_iterable(data)\n        &gt;&gt;&gt; next(iterator)[\"int\"], next(iterator)[\"int\"]\n        (tensor([0]), tensor([1]))\n\n    Args:\n        dataloader: PyTorch DataLoader.\n\n    Returns:\n        Stateful iterable over the data in the dataloader.\n\n    Yields:\n        Data from the dataloader.\n    \"\"\"\n    yield from dataloader\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.train_autoencoder","title":"<code>train_autoencoder(activation_store, train_batch_size)</code>","text":"<p>Train the sparse autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>activation_store</code> <code>TensorActivationStore</code> <p>Activation store from the generate section.</p> required <code>train_batch_size</code> <code>int</code> <p>Train batch size.</p> required <p>Returns:</p> Type Description <code>NeuronActivity</code> <p>Number of times each neuron fired.</p> Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def train_autoencoder(\n    self, activation_store: TensorActivationStore, train_batch_size: int\n) -&gt; NeuronActivity:\n    \"\"\"Train the sparse autoencoder.\n\n    Args:\n        activation_store: Activation store from the generate section.\n        train_batch_size: Train batch size.\n\n    Returns:\n        Number of times each neuron fired.\n    \"\"\"\n    autoencoder_device: torch.device = get_model_device(self.autoencoder)\n\n    activations_dataloader = DataLoader(\n        activation_store,\n        batch_size=train_batch_size,\n    )\n\n    learned_activations_fired_count: NeuronActivity = torch.zeros(\n        self.autoencoder.n_learned_features, dtype=torch.int32, device=autoencoder_device\n    )\n\n    for store_batch in activations_dataloader:\n        # Zero the gradients\n        self.optimizer.zero_grad()\n\n        # Move the batch to the device (in place)\n        batch = store_batch.detach().to(autoencoder_device)\n\n        # Forward pass\n        learned_activations, reconstructed_activations = self.autoencoder(batch)\n\n        # Get loss &amp; metrics\n        metrics = {}\n        total_loss, loss_metrics = self.loss.batch_scalar_loss_with_log(\n            batch, learned_activations, reconstructed_activations\n        )\n        metrics.update(loss_metrics)\n\n        with torch.no_grad():\n            for metric in self.metrics.train_metrics:\n                calculated = metric.calculate(\n                    TrainMetricData(batch, learned_activations, reconstructed_activations)\n                )\n                metrics.update(calculated)\n\n        # Store count of how many neurons have fired\n        with torch.no_grad():\n            fired = learned_activations &gt; 0\n            learned_activations_fired_count.add_(fired.sum(dim=0))\n\n        # Backwards pass\n        total_loss.backward()\n        self.optimizer.step()\n        self.autoencoder.decoder.constrain_weights_unit_norm()\n\n        # Log\n        if wandb.run is not None and self.total_training_steps % self.log_frequency == 0:\n            wandb.log(\n                data={**metrics, **loss_metrics}, step=self.total_training_steps, commit=True\n            )\n        self.total_training_steps += 1\n\n    return learned_activations_fired_count\n</code></pre>"},{"location":"reference/train/pipeline/#sparse_autoencoder.train.pipeline.Pipeline.validate_sae","title":"<code>validate_sae(validation_number_activations)</code>","text":"<p>Get validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>validation_number_activations</code> <code>int</code> <p>Number of activations to use for validation.</p> required Source code in <code>sparse_autoencoder/train/pipeline.py</code> <pre><code>def validate_sae(self, validation_number_activations: int) -&gt; None:\n    \"\"\"Get validation metrics.\n\n    Args:\n        validation_number_activations: Number of activations to use for validation.\n    \"\"\"\n    losses: list[float] = []\n    losses_with_reconstruction: list[float] = []\n    losses_with_zero_ablation: list[float] = []\n    source_model_device: torch.device = get_model_device(self.source_model)\n\n    for batch in self.source_data:\n        input_ids: BatchTokenizedPrompts = batch[\"input_ids\"].to(source_model_device)\n\n        # Run a forward pass with and without the replaced activations\n        self.source_model.remove_all_hook_fns()\n        replacement_hook = partial(\n            replace_activations_hook, sparse_autoencoder=self.autoencoder\n        )\n\n        loss = self.source_model.forward(input_ids, return_type=\"loss\")\n        loss_with_reconstruction = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[\n                (\n                    self.cache_name,\n                    replacement_hook,\n                )\n            ],\n        )\n        loss_with_zero_ablation = self.source_model.run_with_hooks(\n            input_ids,\n            return_type=\"loss\",\n            fwd_hooks=[(self.cache_name, zero_ablate_hook)],\n        )\n\n        losses.append(loss.sum().item())\n        losses_with_reconstruction.append(loss_with_reconstruction.sum().item())\n        losses_with_zero_ablation.append(loss_with_zero_ablation.sum().item())\n\n        if len(losses) &gt;= validation_number_activations:\n            break\n\n    # Log\n    validation_data = ValidationMetricData(\n        source_model_loss=torch.tensor(losses),\n        source_model_loss_with_reconstruction=torch.tensor(losses_with_reconstruction),\n        source_model_loss_with_zero_ablation=torch.tensor(losses_with_zero_ablation),\n    )\n    for metric in self.metrics.validation_metrics:\n        calculated = metric.calculate(validation_data)\n        wandb.log(data=calculated, step=self.total_training_steps, commit=False)\n</code></pre>"},{"location":"reference/train/sweep_config/","title":"Sweep Config","text":"<p>Sweep Config.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepConfig","title":"<code>SweepConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>WandbSweepConfig</code></p> <p>Sweep Config.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SweepConfig(WandbSweepConfig):\n    \"\"\"Sweep Config.\"\"\"\n\n    parameters: SweepParameterConfig\n\n    method: Method = Method.grid\n\n    metric: Metric = field(default_factory=lambda: Metric(name=\"loss\"))\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\"\"\"\n        dict_representation = asdict(self)\n\n        # Convert StrEnums to strings\n        dict_representation[\"method\"] = dict_representation[\"method\"].value\n\n        return dict_representation\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\"\"\"\n    dict_representation = asdict(self)\n\n    # Convert StrEnums to strings\n    dict_representation[\"method\"] = dict_representation[\"method\"].value\n\n    return dict_representation\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig","title":"<code>SweepParameterConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Parameters</code></p> <p>Sweep Parameter Config.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SweepParameterConfig(Parameters):\n    \"\"\"Sweep Parameter Config.\"\"\"\n\n    lr: Parameter[float] | None\n    \"\"\"Adam Learning Rate.\"\"\"\n\n    adam_beta_1: Parameter[float] | None\n    \"\"\"Adam Beta 1.\n\n    The exponential decay rate for the first moment estimates (mean) of the gradient.\n    \"\"\"\n\n    adam_beta_2: Parameter[float] | None\n    \"\"\"Adam Beta 2.\n\n    The exponential decay rate for the second moment estimates (variance) of the gradient.\n    \"\"\"\n\n    adam_epsilon: Parameter[float] | None\n    \"\"\"Adam Epsilon.\n\n    A small constant for numerical stability.\n    \"\"\"\n\n    adam_weight_decay: Parameter[float] | None\n    \"\"\"Adam Weight Decay.\n\n    Weight decay (L2 penalty).\n    \"\"\"\n\n    l1_coefficient: Parameter[float] | None\n    \"\"\"L1 Penalty Coefficient.\n\n    The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant.\n    The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by\n    using more features, or using a lower L1 coefficient.\n\n    Default values from the [original\n    paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html).\n    \"\"\"\n\n    batch_size: Parameter[int] | None\n    \"\"\"Batch size.\n\n    Used in SAE Forward Pass.\"\"\"\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_beta_1","title":"<code>adam_beta_1: Parameter[float] | None</code>  <code>instance-attribute</code>","text":"<p>Adam Beta 1.</p> <p>The exponential decay rate for the first moment estimates (mean) of the gradient.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_beta_2","title":"<code>adam_beta_2: Parameter[float] | None</code>  <code>instance-attribute</code>","text":"<p>Adam Beta 2.</p> <p>The exponential decay rate for the second moment estimates (variance) of the gradient.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_epsilon","title":"<code>adam_epsilon: Parameter[float] | None</code>  <code>instance-attribute</code>","text":"<p>Adam Epsilon.</p> <p>A small constant for numerical stability.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.adam_weight_decay","title":"<code>adam_weight_decay: Parameter[float] | None</code>  <code>instance-attribute</code>","text":"<p>Adam Weight Decay.</p> <p>Weight decay (L2 penalty).</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.batch_size","title":"<code>batch_size: Parameter[int] | None</code>  <code>instance-attribute</code>","text":"<p>Batch size.</p> <p>Used in SAE Forward Pass.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.l1_coefficient","title":"<code>l1_coefficient: Parameter[float] | None</code>  <code>instance-attribute</code>","text":"<p>L1 Penalty Coefficient.</p> <p>The L1 penalty is the absolute sum of learned (hidden) activations, multiplied by this constant. The penalty encourages sparsity in the learned activations. This loss penalty can be reduced by using more features, or using a lower L1 coefficient.</p> <p>Default values from the original paper.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParameterConfig.lr","title":"<code>lr: Parameter[float] | None</code>  <code>instance-attribute</code>","text":"<p>Adam Learning Rate.</p>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParametersRuntime","title":"<code>SweepParametersRuntime</code>  <code>dataclass</code>","text":"<p>             Bases: <code>dict[str, Any]</code></p> <p>Sweep parameter runtime values.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>@dataclass(frozen=True)\nclass SweepParametersRuntime(dict[str, Any]):\n    \"\"\"Sweep parameter runtime values.\"\"\"\n\n    lr: float = 0.001\n\n    adam_beta_1: float = 0.9\n\n    adam_beta_2: float = 0.999\n\n    adam_epsilon: float = 1e-8\n\n    adam_weight_decay: float = 0.0\n\n    l1_coefficient: float = 0.001\n\n    batch_size: int = 4096\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Return dict representation of this object.\"\"\"\n        return asdict(self)\n</code></pre>"},{"location":"reference/train/sweep_config/#sparse_autoencoder.train.sweep_config.SweepParametersRuntime.to_dict","title":"<code>to_dict()</code>","text":"<p>Return dict representation of this object.</p> Source code in <code>sparse_autoencoder/train/sweep_config.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return dict representation of this object.\"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"reference/train/utils/","title":"Train Utils","text":"<p>Train Utils.</p>"},{"location":"reference/train/utils/get_model_device/","title":"Get the device that the model is on","text":"<p>Get the device that the model is on.</p>"},{"location":"reference/train/utils/get_model_device/#sparse_autoencoder.train.utils.get_model_device.get_model_device","title":"<code>get_model_device(model)</code>","text":"<p>Get the device on which a PyTorch model is on.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The device ('cuda' or 'cpu') where the model is located.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has no parameters.</p> Source code in <code>sparse_autoencoder/train/utils/get_model_device.py</code> <pre><code>def get_model_device(model: Module) -&gt; torch.device:\n    \"\"\"Get the device on which a PyTorch model is on.\n\n    Args:\n        model: The PyTorch model.\n\n    Returns:\n        The device ('cuda' or 'cpu') where the model is located.\n\n    Raises:\n        ValueError: If the model has no parameters.\n    \"\"\"\n    # Check if the model has parameters\n    if len(list(model.parameters())) == 0:\n        exception_message = \"The model has no parameters.\"\n        raise ValueError(exception_message)\n\n    # Return the device of the first parameter\n    return next(model.parameters()).device\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/","title":"Wandb Sweep Config Dataclasses","text":"<p>Wandb Sweep Config Dataclasses.</p> <p>Weights &amp; Biases just provide a JSON Schema, so we've converted here to dataclasses.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Controller","title":"<code>Controller</code>  <code>dataclass</code>","text":"<p>Controller.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Controller:\n    \"\"\"Controller.\"\"\"\n\n    type: ControllerType  # noqa: A003\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ControllerType","title":"<code>ControllerType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Controller Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ControllerType(Enum):\n    \"\"\"Controller Type.\"\"\"\n\n    cloud = \"cloud\"\n    local = \"local\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Distribution","title":"<code>Distribution</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Sweep Distribution.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Distribution(Enum):\n    \"\"\"Sweep Distribution.\"\"\"\n\n    beta = \"beta\"\n    categorical = \"categorical\"\n    categoricalwprobabilities = \"categorical_w_probabilities\"\n    constant = \"constant\"\n    intuniform = \"int_uniform\"\n    invloguniform = \"inv_log_uniform\"\n    invloguniformvalues = \"inv_log_uniform_values\"\n    lognormal = \"log_normal\"\n    loguniform = \"log_uniform\"\n    loguniformvalues = \"log_uniform_values\"\n    normal = \"normal\"\n    qbeta = \"q_beta\"\n    qlognormal = \"q_log_normal\"\n    qloguniform = \"q_log_uniform\"\n    qloguniformvalues = \"q_log_uniform_values\"\n    qnormal = \"q_normal\"\n    quniform = \"q_uniform\"\n    uniform = \"uniform\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Goal","title":"<code>Goal</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Goal.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Goal(Enum):\n    \"\"\"Goal.\"\"\"\n\n    maximize = \"maximize\"\n    minimize = \"minimize\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping","title":"<code>HyperbandStopping</code>  <code>dataclass</code>","text":"<p>Hyperband Stopping Config.</p> <p>Speed up hyperparameter search by killing off runs that appear to have lower performance than successful training runs.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass HyperbandStopping:\n    \"\"\"Hyperband Stopping Config.\n\n    Speed up hyperparameter search by killing off runs that appear to have lower performance\n    than successful training runs.\n    \"\"\"\n\n    type: HyperbandStoppingType  # noqa: A003\n\n    eta: float | None = None\n    \"\"\"ETA.\n\n    At every eta^n steps, hyperband continues running the top 1/eta runs and stops all other\n    runs.\n    \"\"\"\n\n    maxiter: int | None = None\n    \"\"\"Max Iterations.\n\n    Set the last epoch to finish trimming runs, and hyperband will automatically calculate\n    the prior epochs to trim runs.\n    \"\"\"\n\n    miniter: int | None = None\n    \"\"\"Min Iterations.\n\n    Set the first epoch to start trimming runs, and hyperband will automatically calculate\n    the subsequent epochs to trim runs.\n    \"\"\"\n\n    s: float | None = None\n    \"\"\"Set the number of steps you trim runs at, working backwards from the max_iter.\"\"\"\n\n    strict: bool | None = None\n    \"\"\"Use a more aggressive condition for termination, stops more runs.\"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.eta","title":"<code>eta: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ETA.</p> <p>At every eta^n steps, hyperband continues running the top 1/eta runs and stops all other runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.maxiter","title":"<code>maxiter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Max Iterations.</p> <p>Set the last epoch to finish trimming runs, and hyperband will automatically calculate the prior epochs to trim runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.miniter","title":"<code>miniter: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Min Iterations.</p> <p>Set the first epoch to start trimming runs, and hyperband will automatically calculate the subsequent epochs to trim runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.s","title":"<code>s: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Set the number of steps you trim runs at, working backwards from the max_iter.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStopping.strict","title":"<code>strict: bool | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use a more aggressive condition for termination, stops more runs.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.HyperbandStoppingType","title":"<code>HyperbandStoppingType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Hyperband Stopping Type.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class HyperbandStoppingType(Enum):\n    \"\"\"Hyperband Stopping Type.\"\"\"\n\n    hyperband = \"hyperband\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Impute","title":"<code>Impute</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Metric value to use in bayes search for runs that fail, crash, or are killed.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Impute(Enum):\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed.\"\"\"\n\n    best = \"best\"\n    latest = \"latest\"\n    worst = \"worst\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.ImputeWhileRunning","title":"<code>ImputeWhileRunning</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Appends a calculated metric even when epochs are in a running state.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class ImputeWhileRunning(Enum):\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    best = \"best\"\n    false = \"false\"\n    latest = \"latest\"\n    worst = \"worst\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Kind","title":"<code>Kind</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Kind.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Kind(Enum):\n    \"\"\"Kind.\"\"\"\n\n    sweep = \"sweep\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Method","title":"<code>Method</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Method.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>class Method(Enum):\n    \"\"\"Method.\"\"\"\n\n    bayes = \"bayes\"\n    custom = \"custom\"\n    grid = \"grid\"\n    random = \"random\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>Metric to optimize.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Metric:\n    \"\"\"Metric to optimize.\"\"\"\n\n    name: str\n    \"\"\"Name of metric.\"\"\"\n\n    goal: Goal | None = None\n\n    impute: Impute | None = None\n    \"\"\"Metric value to use in bayes search for runs that fail, crash, or are killed\"\"\"\n\n    imputewhilerunning: ImputeWhileRunning | None = None\n    \"\"\"Appends a calculated metric even when epochs are in a running state.\"\"\"\n\n    target: float | None = None\n    \"\"\"The sweep will finish once any run achieves this value.\"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.impute","title":"<code>impute: Impute | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metric value to use in bayes search for runs that fail, crash, or are killed</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.imputewhilerunning","title":"<code>imputewhilerunning: ImputeWhileRunning | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Appends a calculated metric even when epochs are in a running state.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Name of metric.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Metric.target","title":"<code>target: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sweep will finish once any run achieves this value.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter","title":"<code>Parameter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Generic[ParamType]</code></p> <p>Sweep Parameter.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass Parameter(Generic[ParamType]):\n    \"\"\"Sweep Parameter.\"\"\"\n\n    value: ParamType | list[ParamType]\n\n    max: ParamType | None = None  # noqa: A003\n\n    min: ParamType | None = None  # noqa: A003\n\n    a: float | None = None\n\n    b: float | None = None\n\n    distribution: Distribution | None = None\n\n    q: float | None = None\n    \"\"\"Quantization parameter for quantized distributions\"\"\"\n\n    values: list[ParamType] | None = None\n    \"\"\"Discrete values\"\"\"\n\n    probabilities: list[float] | None = None\n    \"\"\"Probability of each value\"\"\"\n\n    mu: float | None = None\n    \"\"\"Mean for normal or lognormal distributions\"\"\"\n\n    sigma: float | None = None\n    \"\"\"Std Dev for normal or lognormal distributions\"\"\"\n\n    parameters: dict[str, \"Parameter[ParamType]\"] | None = None\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.mu","title":"<code>mu: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean for normal or lognormal distributions</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.probabilities","title":"<code>probabilities: list[float] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Probability of each value</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.q","title":"<code>q: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantization parameter for quantized distributions</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.sigma","title":"<code>sigma: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Std Dev for normal or lognormal distributions</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.Parameter.values","title":"<code>values: list[ParamType] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Discrete values</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig","title":"<code>WandbSweepConfig</code>  <code>dataclass</code>","text":"<p>Weights &amp; Biases Sweep Configuration.</p> Source code in <code>sparse_autoencoder/train/utils/wandb_sweep_types.py</code> <pre><code>@dataclass(frozen=True)\nclass WandbSweepConfig:\n    \"\"\"Weights &amp; Biases Sweep Configuration.\"\"\"\n\n    parameters: Parameters | Any\n\n    method: Method\n\n    metric: Metric\n    \"\"\"Metric to optimize\"\"\"\n\n    apiVersion: str | None = None\n\n    command: list[Any] | None = None\n    \"\"\"Command used to launch the training script\"\"\"\n\n    controller: Controller | None = None\n\n    description: str | None = None\n    \"\"\"Short package description\"\"\"\n\n    earlyterminate: HyperbandStopping | None = None\n\n    entity: str | None = None\n    \"\"\"The entity for this sweep\"\"\"\n\n    imageuri: str | None = None\n    \"\"\"Sweeps on Launch will use this uri instead of a job.\"\"\"\n\n    job: str | None = None\n    \"\"\"Launch Job to run.\"\"\"\n\n    kind: Kind | None = None\n\n    name: str | None = None\n    \"\"\"The name of the sweep, displayed in the W&amp;B UI.\"\"\"\n\n    program: str | None = None\n    \"\"\"Training script to run.\"\"\"\n\n    project: str | None = None\n    \"\"\"The project for this sweep.\"\"\"\n\n    runcap: int | None = None\n    \"\"\"Run Cap.\n\n    Sweep will run no more than this number of runs, across any number of agents.\n    \"\"\"\n</code></pre>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.command","title":"<code>command: list[Any] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Command used to launch the training script</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.description","title":"<code>description: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Short package description</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.entity","title":"<code>entity: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The entity for this sweep</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.imageuri","title":"<code>imageuri: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sweeps on Launch will use this uri instead of a job.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.job","title":"<code>job: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Launch Job to run.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.metric","title":"<code>metric: Metric</code>  <code>instance-attribute</code>","text":"<p>Metric to optimize</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.name","title":"<code>name: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the sweep, displayed in the W&amp;B UI.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.program","title":"<code>program: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training script to run.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.project","title":"<code>project: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The project for this sweep.</p>"},{"location":"reference/train/utils/wandb_sweep_types/#sparse_autoencoder.train.utils.wandb_sweep_types.WandbSweepConfig.runcap","title":"<code>runcap: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Run Cap.</p> <p>Sweep will run no more than this number of runs, across any number of agents.</p>"}]}